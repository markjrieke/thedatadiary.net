---
title: "Um, Factually"
date: '2024-09-15'
categories: [stan, dropout]
description: "A power ranking for the title of most pedantic nerd on Dropout's *Um, Actually*"
image: header.png
filters:
  - add-code-files
---

```{r, setup}
# libraries
library(tidyverse)
library(riekelib)
library(cmdstanr)

# import um actually episode-level data
actually <- 
  jsonlite::fromJSON("https://raw.githubusercontent.com/tekkamanendless/umactually/master/data.json") %>%
  map_if(is.data.frame, list) %>%
  as_tibble()

# individual contestants
people <- 
  actually %>%
  unnest(people) %>%
  select(id, name) %>%
  rowid_to_column("pid")

# pre-season 9 episodes
episodes <-
  actually %>%
  select(episodes) %>%
  unnest(episodes) %>%
  select(eid = dropouttv_productid,
         season = season_number,
         episode = number,
         players,
         questions) %>%
  filter(season <= 8)
```


> Um, Actually: A game show of fandom minutiae one-upmanship, where nerds do what nerds do best: flaunt encyclopedic nerd knowledge at Millennium Falcon nerd-speed.

## Introduction

*Um, Actually* is a trivia game show found on [Dropout](https://signup.dropout.tv/), wherein contestants are read false statements about their favorite pieces of nerdy pop culture and earn points by figuring out what's wrong.^[But they only get the point if they precede their correction with the phrase "um, actually..."] After 8 seasons, longtime host [Mike Trapp](https://x.com/MikeWTrapp) and his omnipresent fact-checker [Michael Salzman](https://x.com/justaddsaltz) have relinquished their hosting and fact-checking duties. [Ify Nwadiwe](https://x.com/IfyNwadiwe) and [Brian David Gilbert](https://x.com/briamgilbert) take up the mantle as host and voluntary-live-in-fact-checker in season 9.

Ify's ascendancy to host comes in the wake of an impressive run as a contestant. Ify currently holds the title of *winningest contestant*, with a whopping 9 total wins over the course of the first 8 seasons.

```{r, win-tally}
episodes %>% 
  unnest(players) %>%
  group_by(season, episode) %>%
  filter(score == max(score)) %>%
  ungroup() %>%
  count(id) %>%
  arrange(desc(n)) %>%
  left_join(people) %>%
  slice_head(n = 10) %>%
  mutate(name = fct_reorder(name, n)) %>%
  ggplot(aes(x = name,
             y = n)) + 
  geom_col(fill = "royalblue",
           alpha = 0.85) + 
  geom_text(aes(label = n),
            nudge_y = -0.3,
            family = "IBM Plex Sans",
            fontface = "bold",
            color = "white",
            size = 5) + 
  scale_y_continuous(breaks = c(0, 5, 10),
                     minor_breaks = 0:10) + 
  coord_flip() +
  theme_rieke() + 
  labs(title = "**Um, Actually leaderboard**",
       subtitle = "Total wins per contestant in seasons 1-8",
       x = NULL,
       y = NULL,
       caption = "Excludes team games. First place ties<br>count as a win for both contestants") + 
  expand_limits(y = c(0, 10))
```

But does *winningnest contestant* automatically confer the title of *most skilled player?* As Ify is oft lauded as the best Um, Actually player, there's an implicit assumption that win count is the best metric for measuring player skill. But by other metrics, you might conclude that other players are better. [Jared Logan](https://x.com/LoganJared), for example, has a perfect win record across three appearances on the show; [Brennan Lee Mulligan](https://x.com/BrennanLM) has the highest proportion of points-earned to questions-asked; and Jeremy Puckett^[A fan contestant on Season 1 Episode 32] holds the record for most points in a single game (9).^[Ify was a contestant on this episode and received only one point.]

Overall, using win count to measure the best Um, Actually player brushes away a few important factors:

* contestants who appear on the show more often have more opportunities to rack up wins;
* winning by many points is more impressive than eeking out a 1-point win;
* a contestant's point total is not independent of other contestants' skill.

In the pedantic spirit of the game, I propose here a better way of measuring player skill that addresses the shortcomings introduced by simply counting total wins. By using a hierarchical Bayesian model of latent player skill to estimate the point totals of each contestant in each game, I uncover who, statistically, is the best Um, Actually player.

::: {.callout-note}

If you're just here to see the results and power ranking of each contestant, you can [skip to the end] LINK HERE DAWG. Otherwise, strap in for the cacophony of math and code used to develop the rankings.

:::

## The rules of the game

Before diving headfirst into the results or the code to generate them, it's probably helpful to explain in detail how the game works. In each episode, three contestants vie to earn points by identifying the incorrect piece of information in a statement read by the host. Contestants buzz in to propose their corrections, and must begin their statement with the phrase "um, actually...". If their correction is, paradoxically, incorrect, or if they forget to say "um, actually," the other contestants can buzz in to try to scoop the point. If no one is able to correct the host's statement, the host reveals what was wrong and the point is lost to the ether.

![(Left to right) Brennan Lee Mulligan, Kirk Damato, and Marisha Ray as contestants --- Season 2, Episode 1](actually_set.jpg)

Players can also scoop points by being *more correct* than other contestants. For example, say a player identifies the incorrect portion of the host's statement but their correction is wrong. The host may give the other contestants a chance to scoop by correcting the correction. If the other players aren't able to correct the correction, the first player keeps the point.

Finally, peppered throughout each episode are *Shiny Questions*. Shiny Questions, just like Shiny Pok√©mon, are worth the same amount of points, they're just slightly different and a little rarer. Shiny Questions vary in format --- sometimes contestants are tasked with identifying books based on cover alone, other times contestants must find the "fake" alien out of a group of "real" fictional aliens, and sometimes contestants try to draw [cryptids](https://en.wikipedia.org/wiki/List_of_cryptids) accurately based on name only.

Ultimately, skilled players are those who are good at all aspects of the game. The best players not only have a deep well of niche nerd trivia knowledge, but are also quick on the buzzer, able to scoop points from other players, proficient in a wide array of mini-games in the form of Shiny Questions, and, most importantly, remember to say "um, actually."

## Breaking the rules

In most episodes, most questions follow the format described above: one of the three contestants earns a point or the point goes to no one. There are, however, a few edge cases that will need to be accounted for to accurately measure individual skill.

#### Multiple points awarded per question

About ~4% of the time in three-player games, multiple points are awarded on a single question. Most of these cases involve Shiny Questions in which players can potentially tie. The model should therefore estimate how likely it is that points are awarded to multiple contestants as well as derive the probability of each contestant being awarded a point when multiple points are awarded.

#### Team games

Three episodes^[Season 3, episode 2, season 5, episodes 1 and 21.] break from the three-player format and instead pitch two teams of two players against one another. Like three-player games, multiple points per question can be awarded in team games. The model will need to estimate team skill as a combination of individual skill while also estimating the proportion of questions that award points to both teams.

#### The four-player game

At New York's Comic Con in 2019, Mike Trapp hosted a live episode^[Season 2, episode 11] of Um, Actually with a fan, Jamel Wood, as a fourth contestant. Although players *could* potentially be awarded multiple points per question, this didn't happen. Thankfully, the model doesn't need to account for the possibility of multiple players being awarded points on a single question in a four-player game.^[The math to estimate this is a clunky mess of algebra: ![](four_person_math.jpg)] It does, however, need to accommodate the four-person structure.

## Um, Actually: The Model

This is, to say the least, a fairly complex model. To throw some fancy terminology at the wall, I might refer to this as a hierarchical Bayesian model of latent player skill with a finite mixture of possible point awards. But complexity is the cost we pay to jointly model all the possible edge cases brought up by varying show formats. To quote Andrew Gelman, ["big data needs big model."](https://statmodeling.stat.columbia.edu/2014/05/22/big-data-needs-big-model/)

That being said, it's easier to think of this a set of models with a shared base rather than one big model. The different show formats and point awards are handled separately within the model but are all built on the same foundation: estimated player skill.

#### Player skill

For each player, $p$, I model their latent skill, $\beta_p$, as hierarchically distributed around the latent skill of the average player, $\alpha$. The hierarchical formulation allows the model to partially pool player skill estimates. Players who appear on the show many times will have relatively precise estimates of skill. Conversely, players with few appearances will tend to have skill estimates close to the average. To restrict the range of plausible values, I place standard normal priors over the parameters.

$$
\begin{align*}
\beta_p &= \alpha + \eta_p \sigma \\
\alpha &\sim \text{Normal}(0, 1) \\
\eta &\sim \text{Normal}(0, 1) \\
\sigma &\sim \text{Half-Normal}(0, 1)
\end{align*}
$$

#### Multiple awards in team games

In each team game, $g$, I estimate the number of questions with points awarded to both teams, $S_g$ as a draw from a binomial distribution where $K_g$ is the number of questions in each game and $\delta$ is the probability that points are awarded to both teams. This is a relatively rare occurrence, so I place an informative prior over $\text{logit}(\delta)$. 

$$
\begin{align*}
S_g &\sim \text{Binomial}(K_g, \delta) \\
\text{logit}(\delta) &\sim \text{Normal}(-2,0.5)
\end{align*}
$$

#### Multiple awards in three-player games

In a three-player game, estimating the how often points are awarded to multiple contestants is a bit more complex. Here, $S_g$ is a vector with three elements that counts the number of questions in each game, $g$, in which the point was awarded to one player (or no one), two players, or all three players. $K_g$ is the number of questions in each game and $\phi$ is a vector of probabilities corresponding to each category in $S$. 

$$
\begin{align*}
S_g &\sim \text{Multinomial}(K_g, \phi) \\
\phi &= \begin{bmatrix} p_1 \\ p_2 \\ p_3 \end{bmatrix} \\
\end{align*}
$$

Since the categories are ordered, $p$ is generated by dividing the range $[0,1]$ into three $p$ sized regions with two cutpoints, $q$.^[For a detailed introduction to modeling ordinal outcomes, see Chapter 12 Section 3 of Statistical Rethinking by Richard McElreath. I also cover in more detail [here](https://www.thedatadiary.net/posts/2022-12-30-my-2022-magnum-opus/).] Applying the logit transform to $q$ yields $\kappa$, over which I place a $\text{Normal}(0,1.5)$ prior. I enforce that $\kappa_2 > \kappa_1$ with Stan's `ordered` data type.

$$
\begin{align*}
p_1 &= q_1 \\
p_2 &= q_2 - q_1 \\
p_3 &= 1 - q_2 \\
\text{logit}(q_k) &= \kappa_k \\
\kappa &\sim \text{Normal}(0, 1.5)
\end{align*}
$$

#### Single point awarded in a three-player game

In a three-player game, $g$, the number of individually awarded points each player, $p$, wins is modeled as a draw from a poisson distribution given the expected number of points for player $p$, $\lambda_{g,p}$. $\lambda_{g,p}$ is simply the product of the total number of individually awarded points, $K_g$, and player $p$'s probability of winning each point in game $g$, $\theta_{g,p}$.^[This is an example of the poisson trick --- using a series of poisson likelihoods to [vectorize a multinomial model](https://www.thedatadiary.net/posts/2023-04-25-zoom-zoom/).]

$$
\begin{align*}
R_{g,p} &\sim \text{Poisson}(\lambda_{g,p}) \\
\lambda_{g,p} &= K_g \times \theta_{g,p}
\end{align*}
$$

Each element of $\theta_g$ falls between $[0,1]$ and must satisfy the condition that $\sum \theta_g = 1$. Applying the [softmax tranformation](https://en.wikipedia.org/wiki/Softmax_function)^[$\text{softmax}(z)_i = \frac{e^{z_i}}{\sum_j^K e^{z_j}}$] to a vector of latent player skill in game $g$, $\gamma_g$, satisfies $\theta_g$'s constraints.

$$
\begin{align*}
\theta_g &= \text{softmax}(\gamma_g) \\
\gamma_g &= \begin{bmatrix} \beta_{\text{pid}[g,1]} \\ \beta_{\text{pid}[g,2]} \\ \beta_{\text{pid[g,3]}} \\ 0 \end{bmatrix}
\end{align*}
$$

It's worth spending more time interrogating these few lines in more detail. Firstly, sometimes no player is awarded a point. This is represented mathematically by "awarding" these points to the host at position 4 in $\gamma$. To ensure [identifiability](https://mc-stan.org/docs/stan-users-guide/regression.html#identifiability) of the players' skill parameters, $\beta$, I use the "host points" as the reference condition and fix the value value to $0$.^[Note that this does *not* mean that there is a 0% chance of awarding "host points."]

Secondly, the player in each position in $\gamma$ can change from game to game. For example, [Siobhan Thompson](https://x.com/vornietom) can appear at position 1 in one game, position 3 in another, but most often doesn't appear at all! The model undertakes a bit of array-indexing insanity to ensure that the length of $\gamma$ stays the same, but the player-level elements change from game to game.

Finally, although the parameter measuring player skill is static, the probability of being awarded a point can change based on the other players in the game. For example, consider a game with three equally-matched players. Unsurprisingly, they each have an equal probability of being awarded a point.

```{r}
#| code-fold: show
beta <- c(0.5, 0.5, 0.5, 0)

beta %>%
  softmax() %>%
  round(2)
```

If, however, a more skilled contestant swaps in, the probability of the other players being awarded a point drops, despite their latent skill remaining the same.

```{r}
#| code-fold: show
beta[1] <- 1.5

beta %>%
  softmax() %>%
  round(2)
```

#### Multiple points awarded in a three-player game

$$
\begin{align*}
R_{\text{3-player},g,p,q} &\sim \text{Bernoulli}(\theta_{\text{split},g,p}) \\
\theta_{\text{split},g} &= \theta_{g,p} \left(1 + \sum_{j=1 \\ j \neq p}^{J=P} \frac{\theta_{g,p[j]}}{\sum \theta_{g,p[-j]}} \right) \\
\theta_g &= \text{softmax}(\gamma_g) \\
\gamma_g &= \begin{bmatrix} \beta_{\text{pid}[g,1]} \\ \beta_{\text{pid}[g,2]} \\ \beta_{\text{pid}[g,3]} \end{bmatrix}
\end{align*}
$$

#### Team games

$$
\begin{align*}
R_{\text{team},g,t} &\sim \text{Poisson}(\lambda_{g,p}) \\
\lambda_{g,p} &= K_{\text{team},g} \times \theta_{g,p} \\
\theta_g &= \text{softmax}(\gamma_g) \\
\gamma_g &= \begin{bmatrix} \beta_{\text{pid}[g,1]} + \beta_{\text{pid}[g,2]} \\ \beta_{\text{pid}[g,3]} + \beta_{\text{pid}[g,4]} \\ 0 \end{bmatrix}
\end{align*}
$$

#### The four-player game

$$
\begin{align*}
\gamma_g &= \begin{bmatrix} \beta_{\text{pid}[g,1]} \\ \beta_{\text{pid}[g,2]} \\ \beta_{\text{pid}[g,3]} \\ \beta_{\text{pid}[g,4]} \\ 0 \end{bmatrix}
\end{align*}
$$

#### 

* callout - if you wanna just see who the best is, you can skip to the table at the end of the article
* Introduction
  * soft intro to um actually
  * ify as the winningest through 8 seasons
  * hmmm but should we question this?
* The (general) rules of the game
  * 3 players
  * make statements - something in each statement is wrong
  * must precede the correction with "um, actually"
* The weird edge cases
* Statistical model
* Data wrangling & fitting
* Results
  * Brennan, Ify, Ally
* Conclusions





