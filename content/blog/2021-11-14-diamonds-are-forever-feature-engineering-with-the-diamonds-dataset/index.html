---
title: 'Diamonds are Forever: Feature Engineering with the Diamonds Dataset'
author: ''
date: '2021-11-14'
slug: []
categories: [tidymodels]
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2021-11-14T14:50:06-06:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>Are y’all ready for some charts?? This week, I did a bit of machine learning practice with the <a href="https://ggplot2.tidyverse.org/reference/diamonds.html"><code>diamonds dataset</code></a>. This dataset is interesting and good for practice for a few reasons:</p>
<ul>
<li>there are lots of observations (50,000+);</li>
<li>it includes a mix of numeric and categorical variables;</li>
<li>there are some data oddities to deal with (log scales, interactions, non-linear relations)</li>
</ul>
<p>I’ll be doing a bit of feature engineering prior to fitting an tuning a linear model that predicts the each diamond’s <code>price</code> with the <a href="https://glmnet.stanford.edu/index.html"><code>glmnet</code></a> package. This will give a good end-to-end glimpse into the data exploration and model fitting process! Before we get into that, let’s load some packages and get a preview of the dataset.</p>
<pre class="r"><code>library(tidyverse)
library(tidymodels)
library(vip)

theme_set(theme_minimal())

diamonds %&gt;%
  slice_head(n = 10)</code></pre>
<pre><code>## # A tibble: 10 x 10
##    carat cut       color clarity depth table price     x     y     z
##    &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43
##  2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31
##  3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31
##  4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63
##  5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75
##  6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48
##  7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47
##  8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53
##  9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49
## 10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39</code></pre>
<p>Since we’re predicting price, let’s look at its distribution first.</p>
<pre class="r"><code>diamonds %&gt;%
  ggplot(aes(x = price)) +
  geom_histogram()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-2-1.png" width="2400" /></p>
<p>We’re definitely gonna want to apply a transformation to the price when modeling - let’s look at the distribution on a log-10 scale.</p>
<pre class="r"><code>diamonds %&gt;%
  ggplot(aes(x = price)) +
  geom_histogram() +
  scale_x_log10()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-3-1.png" width="2400" /></p>
<p>That’s a lot more evenly distributed, if not perfect. That’s a fine starting point, so now we’ll look through the rest of the data.</p>
<pre class="r"><code>diamonds %&gt;%
  ggplot(aes(x = carat)) +
  geom_histogram()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-1.png" width="2400" /></p>
<pre class="r"><code>diamonds %&gt;%
  ggplot(aes(x = cut,
             y = price)) +
  geom_boxplot()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-2.png" width="2400" /></p>
<pre class="r"><code>diamonds %&gt;%
  count(cut) %&gt;%
  ggplot(aes(x = cut,
             y = n)) +
  geom_col()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-3.png" width="2400" /></p>
<pre class="r"><code>diamonds %&gt;%
  ggplot(aes(x = color,
             y = price)) +
  geom_boxplot()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-4.png" width="2400" /></p>
<pre class="r"><code>diamonds %&gt;%
  count(color) %&gt;%
  ggplot(aes(x = color,
             y = n)) +
  geom_col()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-5.png" width="2400" /></p>
<pre class="r"><code>diamonds %&gt;%
  ggplot(aes(x = clarity,
             y = price)) +
  geom_boxplot()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-6.png" width="2400" /></p>
<pre class="r"><code>diamonds %&gt;%
  count(clarity) %&gt;%
  ggplot(aes(x = clarity,
             y = n)) +
  geom_col()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-7.png" width="2400" /></p>
<pre class="r"><code>diamonds %&gt;%
  ggplot(aes(x = depth)) +
  geom_histogram()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-8.png" width="2400" /></p>
<pre class="r"><code>diamonds %&gt;%
  ggplot(aes(x = table)) +
  geom_histogram() </code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-9.png" width="2400" /></p>
<pre class="r"><code>diamonds %&gt;%
  ggplot(aes(x = x)) +
  geom_histogram()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-10.png" width="2400" /></p>
<pre class="r"><code>diamonds %&gt;%
  ggplot(aes(x = y)) +
  geom_histogram() </code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-11.png" width="2400" /></p>
<pre class="r"><code>diamonds %&gt;% 
  ggplot(aes(x = z)) +
  geom_histogram()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-12.png" width="2400" /></p>
<p>It looks like there may be a good opportunity to try out a few normalization and resampling techniques, but before we get into any of that, let’s build a baseline linear model.</p>
<pre class="r"><code># splits
diamonds_split &lt;- initial_split(diamonds)
diamonds_train &lt;- training(diamonds_split)
diamonds_test &lt;- testing(diamonds_split)

# resamples (don&#39;t want to use testing data!)
diamonds_folds &lt;- vfold_cv(diamonds_train)

# model spec
mod01 &lt;-
  linear_reg() %&gt;%
  set_engine(&quot;lm&quot;)

# recipe
rec01 &lt;-
  recipe(price ~ ., data = diamonds_train) %&gt;%
  step_dummy(all_nominal_predictors())

# controls
ctrl_preds &lt;- 
  control_resamples(save_pred = TRUE)

# create a wf
wf01 &lt;-
  workflow() %&gt;%
  add_model(mod01) %&gt;%
  add_recipe(rec01)

# parallel processing
doParallel::registerDoParallel()

# fit
rs01 &lt;- 
  fit_resamples(
    wf01,
    diamonds_folds,
    control = ctrl_preds
  )

# metrics!
collect_metrics(rs01)</code></pre>
<pre><code>## # A tibble: 2 x 6
##   .metric .estimator     mean     n  std_err .config             
##   &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               
## 1 rmse    standard   1120.       10 12.0     Preprocessor1_Model1
## 2 rsq     standard      0.921    10  0.00118 Preprocessor1_Model1</code></pre>
<p>And right off the bat, we can see a fairly high value for <code>rsq</code>! However, <code>rsq</code> doesn’t tell the whole story, so we should check our predictions and residuals plots.</p>
<pre class="r"><code>augment(rs01) %&gt;%
  ggplot(aes(x = price,
             y = .pred)) +
  geom_point(alpha = 0.01) +
  geom_abline(linetype = &quot;dashed&quot;,
              size = 0.1,
              alpha = 0.5)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-6-1.png" width="2400" /></p>
<p>This is <em>definitely</em> not what we want to see! It looks like there’s an odd curve/structure to the graph and we’re actually predicting quite a few negative values. The residuals plot doesn’t look too great either.</p>
<pre class="r"><code>augment(rs01) %&gt;%
  ggplot(aes(x = price,
             y = .resid)) +
  geom_point(alpha = 0.01) +
  geom_hline(yintercept = 0,
             linetype = &quot;dashed&quot;,
             alpha = 0.5,
             size = 0.1)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-7-1.png" width="2400" /></p>
<p>What we’d like to see is a 0-correlation plot with errors normally distributed; what we’re seeing instead, however, is a ton of structure.</p>
<p>That being said, that’s okay! we expected this first pass to be pretty rough! And the price is <em>clearly</em> on a log-10 scale. To make apples-apples comparisons with models going forward, I’ll retrain this basic linear model to predict the <code>log10(price)</code>. This’ll involve a bit of data re-manipulation!</p>
<pre class="r"><code># log transform price
diamonds_model &lt;-
  diamonds %&gt;%
  mutate(price = log10(price),
         across(cut:clarity, as.character))

# bad practice copy + paste lol

# splits
set.seed(999)
diamonds_split &lt;- initial_split(diamonds_model)
diamonds_train &lt;- training(diamonds_split)
diamonds_test &lt;- testing(diamonds_split)

# resamples (don&#39;t want to use testing data!)
set.seed(888)
diamonds_folds &lt;- vfold_cv(diamonds_train)

# model spec
mod01 &lt;-
  linear_reg() %&gt;%
  set_engine(&quot;lm&quot;)

# recipe
rec01 &lt;-
  recipe(price ~ ., data = diamonds_train) %&gt;%
  step_dummy(all_nominal_predictors())

# controls
ctrl_preds &lt;- 
  control_resamples(save_pred = TRUE)

# create a wf
wf01 &lt;-
  workflow() %&gt;%
  add_model(mod01) %&gt;%
  add_recipe(rec01)

# parallel processing
doParallel::registerDoParallel()

# fit
set.seed(777)
rs01 &lt;- 
  fit_resamples(
    wf01,
    diamonds_folds,
    control = ctrl_preds
  )

# metrics!
collect_metrics(rs01)</code></pre>
<pre><code>## # A tibble: 2 x 6
##   .metric .estimator   mean     n std_err .config             
##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
## 1 rmse    standard   0.0793    10 0.00557 Preprocessor1_Model1
## 2 rsq     standard   0.966     10 0.00494 Preprocessor1_Model1</code></pre>
<p>And wow, that <em>one</em> transformation increased our <code>rsq</code> to 0.96! Again, that’s not the whole story, and we’re going to be evaluating models based on the <code>rmse</code>. Let’s look at how our prediction map has updated:</p>
<pre class="r"><code>rs01 %&gt;%
  augment() %&gt;%
  ggplot(aes(x = price,
             y = .pred)) +
  geom_point(alpha = 0.01) +
  geom_abline(linetype = &quot;dashed&quot;,
              size = 0.1,
              alpha = 0.5) </code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-9-1.png" width="2400" /></p>
<p>Now <em>that</em> is a much better starting place to be at! Let’s look at our coefficients</p>
<pre class="r"><code>set.seed(666) # :thedevilisalive:
wf01 %&gt;%
  fit(diamonds_train) %&gt;%
  pull_workflow_fit() %&gt;%
  vip::vi() %&gt;%
  mutate(Variable = fct_reorder(Variable, Importance)) %&gt;%
  ggplot(aes(x = Variable,
             y = Importance,
             fill = Sign)) +
  geom_col() +
  coord_flip() + 
  theme(plot.title.position = &quot;plot&quot;) +
  labs(x = NULL,
       y = NULL,
       title = &quot;Diamonds are forever&quot;,
       subtitle = &quot;Variable importance plot of a basic linear regression predicting diamond price&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-10-1.png" width="2400" /></p>
<p>Another way of looking at it:</p>
<pre class="r"><code>set.seed(666)
wf01 %&gt;%
  fit(diamonds_train) %&gt;%
  pull_workflow_fit() %&gt;%
  vip::vi() %&gt;%
  mutate(Importance = if_else(Sign == &quot;NEG&quot;, Importance * -1, Importance),
         Variable = fct_reorder(Variable, Importance)) %&gt;%
  ggplot(aes(x = Variable,
             y = Importance,
             fill = Sign)) +
  geom_col() +
  coord_flip() +
  labs(title = &quot;Diamonds are forever&quot;,
       subtitle = &quot;Variable importance plot of a basic linear regression predicting diamond price&quot;,
       x = NULL,
       y = NULL) +
  theme(plot.title.position = &quot;plot&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-11-1.png" width="2400" /></p>
<p>This is a good, but definitely improvable, starting point. We can likely decrease our overall error with a bit of feature engineering and drop unimportant features by tuning a regularized model. There are some oddities in this initial model that will need to be improved upon; for one, we can definitively say that the <code>carat</code> feature ought to be <em>positively</em> associated with price</p>
<pre class="r"><code>diamonds_train %&gt;%
  ggplot(aes(x = carat,
             y = price)) +
  geom_point(alpha = 0.01) +
  labs(title = &quot;A clear positive (albeit nonlinear) relationship between `carat` and `price`&quot;) +
  theme(plot.title.position = &quot;plot&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-12-1.png" width="2400" /></p>
<p>Another few things that are interesting to note in this plot! It looks like there are clusterings of carat ratings around round-ish numbers. My hypothesis here is that carat ratings tend to get rounded up to the next size. There’s also a clear abscence of diamonds priced at $1,500 (~3.17 on the log10 scale). I suppose there is some industry-specific reason to avoid a diamond price of $,1500?</p>
<pre class="r"><code>diamonds_train %&gt;%
  ggplot(aes(x = carat,
             y = price)) +
  geom_point(alpha = 0.01) +
  labs(title = &quot;A clear positive (albeit nonlinear) relationship between `carat` and `price`&quot;) +
  theme(plot.title.position = &quot;plot&quot;) +
  geom_hline(yintercept = log10(1500),
             linetype = &quot;dashed&quot;,
             size = 0.9,
             alpha = 0.5)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-13-1.png" width="2400" /></p>
<p>How to address all these things? With some feature engineering! Firstly, let’s add some recipe steps to balance classes &amp; normalize continuous variables.</p>
<p>But before I get into <em>that</em>, I’ll save the resample metrics so that we can compare models!</p>
<pre class="r"><code>metrics &lt;- collect_metrics(rs01) %&gt;% mutate(model = &quot;model01&quot;)

metrics</code></pre>
<pre><code>## # A tibble: 2 x 7
##   .metric .estimator   mean     n std_err .config              model  
##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt;  
## 1 rmse    standard   0.0793    10 0.00557 Preprocessor1_Model1 model01
## 2 rsq     standard   0.966     10 0.00494 Preprocessor1_Model1 model01</code></pre>
<pre class="r"><code># spec will be the same as model01
mod02 &lt;- mod01

# recipe!
rec02 &lt;- 
  recipe(price ~ ., data = diamonds_train) %&gt;%
  step_other(cut, color, clarity) %&gt;% 
  step_dummy(all_nominal_predictors(), -cut) %&gt;%
  
  # use smote resampling to balance classes
  themis::step_smote(cut) %&gt;% 
    
  # normalize continuous vars
  bestNormalize::step_best_normalize(carat, depth, table, x, y, z)</code></pre>
<p>Let’s <a href="https://recipes.tidymodels.org/reference/bake.html">bake</a> our recipe to verify that everything looks up-to-snuff in the preprocessed dataset.</p>
<pre class="r"><code>baked_rec02 &lt;- 
  rec02 %&gt;%
  prep() %&gt;%
  bake(new_data = NULL)

baked_rec02</code></pre>
<pre><code>## # A tibble: 80,495 x 20
##      carat cut        depth  table       x       y      z price color_E color_F
##      &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1 -0.706  Premium    0.138 -0.760 -0.709  -0.738  -0.695  3.01       0       1
##  2  0.356  Very Good  0.570  0.835  0.342   0.251   0.344  3.63       0       0
##  3  0.214  Premium   -0.308  0.835  0.293   0.263   0.166  3.58       0       0
##  4 -1.08   other      1.04  -0.310 -1.30   -1.40   -0.995  2.70       0       0
##  5 -0.641  Ideal     -0.602 -0.760 -0.595  -0.560  -0.622  2.97       0       0
##  6 -0.0759 Premium   -0.602  0.494 -0.0349 -0.0460 -0.114  3.38       0       0
##  7 -0.149  Premium   -1.16   0.103 -0.0565 -0.0842 -0.246  3.44       1       0
##  8  0.170  Very Good -0.371  0.494  0.178   0.313   0.130  3.56       0       1
##  9 -0.736  Ideal     -0.110 -0.760 -0.709  -0.738  -0.723  3.09       0       0
## 10  0.782  Ideal     -0.602 -0.310  0.819   0.846   0.732  4.02       0       0
## # ... with 80,485 more rows, and 10 more variables: color_G &lt;dbl&gt;,
## #   color_H &lt;dbl&gt;, color_I &lt;dbl&gt;, color_J &lt;dbl&gt;, clarity_SI2 &lt;dbl&gt;,
## #   clarity_VS1 &lt;dbl&gt;, clarity_VS2 &lt;dbl&gt;, clarity_VVS1 &lt;dbl&gt;,
## #   clarity_VVS2 &lt;dbl&gt;, clarity_other &lt;dbl&gt;</code></pre>
<pre class="r"><code>baked_rec02 %&gt;%
  count(cut) %&gt;%
  ggplot(aes(x = cut,
             y = n)) +
  geom_col()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-17-1.png" width="2400" /></p>
<pre class="r"><code>baked_rec02 %&gt;%
  ggplot(aes(x = carat)) +
  geom_histogram()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-17-2.png" width="2400" /></p>
<pre class="r"><code>baked_rec02 %&gt;%
  ggplot(aes(x = depth)) +
  geom_histogram()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-17-3.png" width="2400" /></p>
<pre class="r"><code>baked_rec02 %&gt;%
  ggplot(aes(x = table)) +
  geom_histogram()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-17-4.png" width="2400" /></p>
<pre class="r"><code>baked_rec02 %&gt;%
  ggplot(aes(x = x)) +
  geom_histogram()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-17-5.png" width="2400" /></p>
<pre class="r"><code>baked_rec02 %&gt;%
  ggplot(aes(x = y)) +
  geom_histogram() </code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-17-6.png" width="2400" /></p>
<pre class="r"><code>baked_rec02 %&gt;%
  ggplot(aes(x = z)) +
  geom_histogram()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-17-7.png" width="2400" /></p>
<p>Everything looks alright with the exception of the <code>table</code> predictor. I wonder if there are a lot of repeated values in the <code>table</code> variable - that may be why we’re seeing a “chunky” histogram. Let’s check</p>
<pre class="r"><code>baked_rec02 %&gt;%
  count(table) %&gt;%
  arrange(desc(n))</code></pre>
<pre><code>## # A tibble: 10,406 x 2
##     table     n
##     &lt;dbl&gt; &lt;int&gt;
##  1  0.103 12167
##  2 -0.310 11408
##  3 -0.760 11031
##  4  0.494  9406
##  5 -1.28   6726
##  6  0.835  6165
##  7  1.15   3810
##  8 -1.85   2789
##  9  1.42   2182
## 10  1.64    972
## # ... with 10,396 more rows</code></pre>
<p>Ooh - okay yeah that’s definitely the issue! I’m not <em>quite</em> sure how to deal with it, so we’re just going to ignore for now! Let’s add a new model &amp; see how it compares against the baseline transformed model.</p>
<pre class="r"><code>wf02 &lt;-
  workflow() %&gt;%
  add_model(mod02) %&gt;%
  add_recipe(rec02)

# stop parallel to avoid error!
# need to replace with PSOCK clusters
# see github issue here: https://github.com/tidymodels/recipes/issues/847
foreach::registerDoSEQ()

set.seed(666) # spoopy
rs02 &lt;-
  fit_resamples(
    wf02,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs02)</code></pre>
<pre><code>## # A tibble: 2 x 6
##   .metric .estimator  mean     n std_err .config             
##   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
## 1 rmse    standard   0.115    10 0.00141 Preprocessor1_Model1
## 2 rsq     standard   0.932    10 0.00158 Preprocessor1_Model1</code></pre>
<p>Oof - that’s actually slightly worse than our baseline model!</p>
<pre class="r"><code>rs02 %&gt;%
  augment() %&gt;%
  ggplot(aes(x = price,
             y = .pred)) +
  geom_point(alpha = 0.01) +
  geom_abline(linetype = &quot;dashed&quot;,
              size = 0.1,
              alpha = 0.5) </code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-20-1.png" width="2400" /></p>
<p>It looks like we’ve introduced structure into the residual plot!</p>
<pre class="r"><code>rs02 %&gt;%
  augment() %&gt;%
  ggplot(aes(x = price,
             y = .resid)) +
  geom_point(alpha = 0.01) +
  geom_hline(yintercept = 0,
             linetype = &quot;dashed&quot;,
             size = 0.1,
             alpha = 0.5)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-21-1.png" width="2400" /></p>
<p>Yeah that’s fairly wonky! I’m wondering if it’s due to the SMOTE upsampling method we introduced? To counteract, I’ll build &amp; train new models after each set of recipe steps (e.g., resampling, normalizing, interactions) to buil up a better performing model one step at a time.</p>
<pre class="r"><code>metrics &lt;- 
  metrics %&gt;%
  bind_rows(collect_metrics(rs02) %&gt;% mutate(model = &quot;model02&quot;))</code></pre>
<pre class="r"><code># same model spec
mod03 &lt;- mod02

# rebuild rec+wf &amp; retrain
rec03 &lt;- 
  recipe(price ~ ., data = diamonds_train) %&gt;%
  step_other(cut, color, clarity) %&gt;%
  step_dummy(all_nominal_predictors(), -cut) %&gt;%
  themis::step_smote(cut)

wf03 &lt;- 
  workflow() %&gt;%
  add_model(mod03) %&gt;%
  add_recipe(rec03)

# do paralllel
doParallel::registerDoParallel()

# refit!
set.seed(123)
rs03 &lt;-
  fit_resamples(
    wf03,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs03)</code></pre>
<pre><code>## # A tibble: 2 x 6
##   .metric .estimator   mean     n std_err .config             
##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
## 1 rmse    standard   0.0918    10 0.00502 Preprocessor1_Model1
## 2 rsq     standard   0.956     10 0.00502 Preprocessor1_Model1</code></pre>
<p>Interesting! Improved relative to <code>rs02</code>, but still not as good as our first model! Let’s try using <code>step_downsample()</code> to balance classes &amp; see how we fare.</p>
<pre class="r"><code># cleanup some large-ish items eating up memory
rm(mod01, mod02, rec01, rec02, wf01, wf02, rs01, rs02)

# save metrics
metrics &lt;- 
  metrics %&gt;%
  bind_rows(collect_metrics(rs03) %&gt;% mutate(model = &quot;model03&quot;))

# new mod
mod04 &lt;- mod03

# new rec
rec04 &lt;-
  recipe(price ~ ., data = diamonds_train) %&gt;%
  step_other(cut, color, clarity) %&gt;%
  step_dummy(all_nominal_predictors(), -cut) %&gt;%
  themis::step_downsample(cut)

wf04 &lt;-
  workflow() %&gt;%
  add_model(mod04) %&gt;%
  add_recipe(rec04) 

set.seed(456) 
rs04 &lt;-
  fit_resamples(
    wf04,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs04)</code></pre>
<pre><code>## # A tibble: 2 x 6
##   .metric .estimator  mean     n std_err .config             
##   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
## 1 rmse    standard   0.120    10  0.0150 Preprocessor1_Model1
## 2 rsq     standard   0.921    10  0.0194 Preprocessor1_Model1</code></pre>
<p>Wow - still a bit worse! I’ll try upsampling &amp; if there is no improvement, we’ll move on without resampling!</p>
<pre class="r"><code>metrics &lt;-
  metrics %&gt;%
  bind_rows(collect_metrics(rs04) %&gt;% mutate(model = &quot;model04&quot;))

mod05 &lt;- mod04

rec05 &lt;-
  recipe(price ~ ., data = diamonds_train) %&gt;%
  step_other(cut, color, clarity) %&gt;%
  step_dummy(all_nominal_predictors(), -cut) %&gt;%
  themis::step_upsample(cut)

wf05 &lt;- 
  workflow() %&gt;%
  add_model(mod05) %&gt;%
  add_recipe(rec05) 

set.seed(789)
rs05 &lt;-
  fit_resamples(
    wf05,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs05)</code></pre>
<pre><code>## # A tibble: 2 x 6
##   .metric .estimator  mean     n std_err .config             
##   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
## 1 rmse    standard   0.106    10 0.00618 Preprocessor1_Model1
## 2 rsq     standard   0.941    10 0.00714 Preprocessor1_Model1</code></pre>
<p>Okay - resampling gets stricken off our list of recipe steps! Let’s look at how the models compare so far</p>
<pre class="r"><code>metrics &lt;-
  metrics %&gt;%
  bind_rows(collect_metrics(rs05) %&gt;% mutate(model = &quot;model05&quot;))

metrics %&gt;%
  ggplot(aes(x = model)) +
  geom_point(aes(y = mean)) +
  geom_errorbar(aes(ymin = mean - std_err,
                    ymax = mean + std_err)) +
  facet_wrap(~.metric, scales = &quot;free_y&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-26-1.png" width="2400" /></p>
<p>The first simple linear model was the best as measured by both metrics! Let’s see if we can improve with some normalization of the continuous vars.</p>
<pre class="r"><code>rm(mod03, mod04, rec03, rec04, rs03, rs04, wf03, wf04)

mod06 &lt;- mod05

rec06 &lt;-
  recipe(price ~ ., data = diamonds_train) %&gt;%
  step_other(cut, color, clarity) %&gt;%
  bestNormalize::step_best_normalize(all_numeric_predictors()) %&gt;%
  step_dummy(all_nominal_predictors())

wf06 &lt;-
  workflow() %&gt;%
  add_model(mod06) %&gt;%
  add_recipe(rec06)

foreach::registerDoSEQ()
set.seed(101112)
rs06 &lt;-
  fit_resamples(
    wf06,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs06)</code></pre>
<pre><code>## # A tibble: 2 x 6
##   .metric .estimator  mean     n std_err .config             
##   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
## 1 rmse    standard   0.127    10 0.00115 Preprocessor1_Model1
## 2 rsq     standard   0.916    10 0.00136 Preprocessor1_Model1</code></pre>
<p>Well - that was quite a bit for no improvement! I guess that normalizing the continuous vars in this case isn’t helping. Moving on to adding some interactions - first let’s explore potential interactions a bit.</p>
<pre class="r"><code>metrics &lt;-
  metrics %&gt;% 
  bind_rows(collect_metrics(rs06) %&gt;% mutate(model = &quot;model06&quot;))

diamonds_train %&gt;%
  ggplot(aes(x = carat,
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) + 
  geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-28-1.png" width="2400" /></p>
<pre class="r"><code>library(splines)
diamonds_train %&gt;%
  ggplot(aes(x = carat,
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = lm,
              formula = y ~ ns(x, df = 5),
              se = FALSE) +
  facet_wrap(~cut, scales = &quot;free&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-29-1.png" width="2400" /></p>
<p>5 spline terms might not be sufficient here - capturing the lower bound well but <em>really</em> not doing well with the higher carat diamonds.</p>
<pre class="r"><code>diamonds_train %&gt;%
  ggplot(aes(x = carat,
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = lm,
              formula = y ~ ns(x, df = 10),
              se = FALSE) +
  facet_wrap(~cut, scales = &quot;free&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-30-1.png" width="2400" /></p>
<p>Hmmmm, 10 might be too many. It looks lie we’ll just lose a bit of confidence for the Premium &amp; Very Good diamonds at higher carats. Relative to the total number, I’m not too concerned.</p>
<pre class="r"><code>diamonds_train %&gt;%
  ggplot(aes(x = carat,
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = lm,
              formula = y ~ ns(x, df = 7),
              se = FALSE) +
  facet_wrap(~cut, scales = &quot;free&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-31-1.png" width="2400" /></p>
<p>7 terms feels like the best we’re going to do here - I think this is tuneable, but we’ll leave as is (now &amp; in the final model).</p>
<p>Next, we’ll look at creating interactions between the <code>color</code> and <code>carat</code> variables:</p>
<pre class="r"><code>diamonds_train %&gt;%
  ggplot(aes(x = carat, 
             y = price,
             color = color)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = lm, 
              formula = y ~ ns(x, df = 15),
              se = FALSE) +
  facet_wrap(~color)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-32-1.png" width="2400" /></p>
<p>Adding interactive spline terms with <code>df</code> of 15 seems to add some useful information!</p>
<p>We have three shape parameters, <code>x</code>, <code>y</code>, and <code>z</code> - I wonder if creating a stand-in for volume by multiplying them all together will provide any useful information?</p>
<pre class="r"><code>diamonds_train %&gt;%
  mutate(volume_param = x * y * z) %&gt;%
  ggplot(aes(x = volume_param,
             y = price)) +
  geom_point(alpha = 0.05)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-33-1.png" width="2400" /></p>
<p>Ooh, looks like we’re getting some good info here, but we may want to use <code>log10</code> to scale this back.</p>
<pre class="r"><code>diamonds_train %&gt;%
  mutate(volume_param = log10(x * y * z)) %&gt;%
  ggplot(aes(x = volume_param, 
             y = price)) +
  geom_point(alpha = 0.05)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-34-1.png" width="2400" /></p>
<p>Let’s see if this ought to interact with any other paramaters:</p>
<pre class="r"><code>diamonds_train %&gt;%
  mutate(volume_param = log10(x * y * z)) %&gt;%
  ggplot(aes(x = volume_param, 
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-35-1.png" width="2400" /></p>
<pre class="r"><code>diamonds_train %&gt;%
  mutate(volume_param = log10(x * y * z)) %&gt;%
  ggplot(aes(x = volume_param, 
             y = price,
             color = color)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-35-2.png" width="2400" /></p>
<pre class="r"><code>diamonds_train %&gt;%
  mutate(volume_param = log10(x * y * z)) %&gt;%
  ggplot(aes(x = volume_param, 
             y = price,
             color = clarity)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-35-3.png" width="2400" /></p>
<p>Hmm, it doesn’t really look like we’re capturing too great of interactions, so I’ll leave out for now. It looks like the <em>size</em> of the rock is more important than anything else! I could continue to dig further, but I’ll stop there. I’m likely getting diminishing returns, &amp; I’d like to get back into modeling!</p>
<pre class="r"><code>mod07 &lt;- mod06

rec07 &lt;-
  recipe(price ~ ., data = diamonds_train) %&gt;%
  step_other(cut, color, clarity) %&gt;%
  step_dummy(all_nominal_predictors()) %&gt;%
  step_interact(~carat:starts_with(&quot;cut_&quot;)) %&gt;%
  step_interact(~carat:starts_with(&quot;color_&quot;)) %&gt;%
  step_mutate_at(c(x, y, z),
                 fn = ~if_else(.x == 0, mean(.x), .x)) %&gt;%
  step_mutate(volume_param = log10(x * y * z)) %&gt;%
  step_ns(starts_with(&quot;carat_x_cut&quot;), deg_free = 7) %&gt;%
  step_ns(starts_with(&quot;carat_x_color&quot;), deg_free = 15) 

rec07</code></pre>
<pre><code>## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          9
## 
## Operations:
## 
## Collapsing factor levels for cut, color, clarity
## Dummy variables from all_nominal_predictors()
## Interactions with carat:starts_with(&quot;cut_&quot;)
## Interactions with carat:starts_with(&quot;color_&quot;)
## Variable mutation for c(x, y, z)
## Variable mutation for volume_param
## Natural Splines on starts_with(&quot;carat_x_cut&quot;)
## Natural Splines on starts_with(&quot;carat_x_color&quot;)</code></pre>
<pre class="r"><code>wf07 &lt;-
  workflow() %&gt;%
  add_model(mod07) %&gt;%
  add_recipe(rec07)

doParallel::registerDoParallel()
set.seed(9876)
rs07 &lt;-
  fit_resamples(
    wf07,
    diamonds_folds,
    control = ctrl_preds
  )</code></pre>
<p>This is definitely going to <em>way</em> overfit our data:</p>
<pre class="r"><code>rs07 %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 2 x 6
##   .metric .estimator   mean     n std_err .config             
##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
## 1 rmse    standard   0.0750    10 0.00377 Preprocessor1_Model1
## 2 rsq     standard   0.970     10 0.00342 Preprocessor1_Model1</code></pre>
<p>Well we (finally) made a modes improvement! Let’s see how the predictions/residuals plot:</p>
<pre class="r"><code>rs07 %&gt;%
  augment() %&gt;%
  ggplot(aes(x = price,
             y = .pred)) +
  geom_point(alpha = 0.05) +
  geom_abline(linetype = &quot;dashed&quot;,
              alpha = 0.5,
              size = 0.5)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-38-1.png" width="2400" /></p>
<p>That’s pretty good! We do have one value that’s <strong><em>way</em></strong> off, so let’s see if regulization can help. This will require setting a new baseline model, and we’ll tune our way to the best regularizaion parameters.</p>
<pre class="r"><code>metrics &lt;- 
  rs07 %&gt;%
  collect_metrics() %&gt;%
  mutate(model = &quot;model07&quot;) %&gt;%
  bind_rows(metrics)

# add normalization step
rec08 &lt;- 
  rec07 %&gt;% 
  step_zv(all_numeric_predictors()) %&gt;%
  step_normalize(all_numeric_predictors(),
                 -cut_Ideal, -cut_Premium, -cut_Very.Good, -cut_other,
                 -color_E, -color_F, -color_G, -color_H, -color_I, -color_J,
                 -clarity_SI2, -clarity_VS1, -clarity_VS2, -clarity_VVS1, -clarity_VVS2, -clarity_other)

rm(mod05, mod06, mod07, rec05, rec06, rec07, wf05, wf06, wf07, rs05, rs06, rs07)

mod08 &lt;-
  linear_reg(penalty = tune(), mixture = tune()) %&gt;%
  set_engine(&quot;glmnet&quot;) %&gt;%
  set_mode(&quot;regression&quot;) 

wf08 &lt;-
  workflow() %&gt;%
  add_model(mod08) %&gt;%
  add_recipe(rec08)

diamonds_grid &lt;- 
  grid_regular(penalty(), mixture(), levels = 20)

doParallel::registerDoParallel()
set.seed(5831)
rs08 &lt;-
  tune_grid(
    wf08,
    resamples = diamonds_folds,
    control = ctrl_preds,
    grid = diamonds_grid
  )</code></pre>
<p>Some notes but let’s explore our results…</p>
<pre class="r"><code>rs08 %&gt;%
  collect_metrics() %&gt;%
  ggplot(aes(x = penalty,
             y = mean,
             color = as.character(mixture))) +
  geom_point() +
  geom_line(alpha = 0.75) +
  facet_wrap(~.metric, scales = &quot;free&quot;) +
  scale_x_log10()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-40-1.png" width="2400" /></p>
<p>Looks like we were performing pretty well with the unregularized model, oddly enough! Let’s select the best and finalize our workflow.</p>
<pre class="r"><code>best_metrics &lt;- 
  rs08 %&gt;%
  select_best(&quot;rmse&quot;)

wf_final &lt;- 
  finalize_workflow(wf08, best_metrics)

rm(mod08, rec07, rec08, rs08, wf08)

set.seed(333)
final_fit &lt;- 
  wf_final %&gt;%
  fit(diamonds_train)

final_fit %&gt;%
  predict(diamonds_test) %&gt;%
  bind_cols(diamonds_test) %&gt;%
  select(price, .pred) %&gt;%
  ggplot(aes(x = price, 
             y = .pred)) +
  geom_point(alpha = 0.05) + 
  geom_abline(alpha = 0.5,
              linetype = &quot;dashed&quot;,
              size = 0.5)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-41-1.png" width="2400" /></p>
<p>What are the most important variables in this regularized model?</p>
<pre class="r"><code>final_fit %&gt;%
  pull_workflow_fit() %&gt;%
  vi(lambda = best_metrics$penalty) %&gt;%
  mutate(Variable = fct_reorder(Variable, Importance)) %&gt;%
  ggplot(aes(x = Variable,
             y = Importance, 
             fill = Sign)) +
  geom_col() +
  coord_flip()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-42-1.png" width="2400" /></p>
<p>As expected, most of our terms get regularized away, which is what we want! Our chart is a little unreadable; let’s plot just the most important variables in a few ways:</p>
<pre class="r"><code>final_fit %&gt;%
  pull_workflow_fit() %&gt;%
  vi(lambda = best_metrics$penalty) %&gt;%
  arrange(desc(Importance)) %&gt;%
  slice_head(n = 10) %&gt;%
  mutate(Variable = fct_reorder(Variable, Importance)) %&gt;%
  ggplot(aes(x = Variable,
             y = Importance,
             fill = Sign)) +
  geom_col() +
  coord_flip()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-43-1.png" width="2400" /></p>
<pre class="r"><code>final_fit %&gt;%
  pull_workflow_fit() %&gt;%
  vi(lambda = best_metrics$penalty) %&gt;%
  arrange(desc(Importance)) %&gt;% 
  slice_head(n = 10) %&gt;%
  mutate(Importance = if_else(Sign == &quot;NEG&quot;, -Importance, Importance),
         Variable = fct_reorder(Variable, Importance)) %&gt;%
  ggplot(aes(x = Variable,
             y = Importance,
             fill = Sign)) + 
  geom_col() +
  coord_flip()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-44-1.png" width="2400" /></p>
<p>And look at that! Our most important variable was one that came from feature engineering! The size of the rock had the biggest impact on price.</p>
<p>We’ve gone through a lot of steps, so it may be good to look back on what was done:</p>
<ul>
<li>Explored our dataset via some simple exploratory data analysis;</li>
<li>Fit a simple linear model to predict the log-transform of price;</li>
<li>Attempted (and failed) to improve upon the simple model with fancier normalization and resampling techniques;</li>
<li>Explored the dataset further to find meaningful interactions and potential new features;</li>
<li>Fit a new model with feature engineering;</li>
<li>Tuned regularization parameters on our model with feature engineering to arrive at the final model.</li>
</ul>
<p>Our models’ performances, ranked from best to worst, show that the final tuned model did indeed perform the best on the test dataset!</p>
<pre class="r"><code>metrics %&gt;%
  select(.metric, mean, model)</code></pre>
<pre><code>## # A tibble: 14 x 3
##    .metric   mean model  
##    &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;  
##  1 rmse    0.0750 model07
##  2 rsq     0.970  model07
##  3 rmse    0.0793 model01
##  4 rsq     0.966  model01
##  5 rmse    0.115  model02
##  6 rsq     0.932  model02
##  7 rmse    0.0918 model03
##  8 rsq     0.956  model03
##  9 rmse    0.120  model04
## 10 rsq     0.921  model04
## 11 rmse    0.106  model05
## 12 rsq     0.941  model05
## 13 rmse    0.127  model06
## 14 rsq     0.916  model06</code></pre>
<pre class="r"><code>final_preds &lt;-
  final_fit %&gt;%
  predict(diamonds_train) %&gt;%
  bind_cols(diamonds_train) %&gt;%
  select(price, .pred)

bind_rows(final_preds %&gt;% rmse(price, .pred),
          final_preds %&gt;% rsq(price, .pred)) %&gt;%
  rename(mean = .estimate) %&gt;%
  select(-.estimator) %&gt;%
  mutate(model = &quot;model_final&quot;) %&gt;%
  bind_rows(metrics %&gt;% select(.metric, mean, model)) %&gt;%
  pivot_wider(names_from = .metric,
              values_from = mean) %&gt;%
  mutate(model = fct_reorder(model, desc(rmse))) %&gt;%
  pivot_longer(rmse:rsq,
               names_to = &quot;metric&quot;,
               values_to = &quot;value&quot;) %&gt;%
  ggplot(aes(x = model,
             y = value)) +
  geom_point() +
  facet_wrap(~metric, scales = &quot;free&quot;) +
  coord_flip()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-45-1.png" width="2400" /></p>
