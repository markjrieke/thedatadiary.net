---
title: Introducing {workboots}
author: ''
date: '2022-03-14'
slug: []
categories: []
tags: []
subtitle: Generate bootstrap prediction intervals from a tidymodel workflow!
summary: ''
authors: []
lastmod: '2022-03-14T16:02:16-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>Sometimes, we want a model that generates a range of possible outcomes around each prediction and may opt for a model that can generate a prediction interval, like a linear model. Other times, we just care about point predictions and may opt to use a more powerful model like XGBoost. But what if we want the best of both worlds: getting a range of predictions while still using a powerful model? That’s where <a href="https://github.com/markjrieke/workboots"><code>{workboots}</code></a> comes to the rescue! <code>{workboots}</code> uses bootstrap resampling to train many models which can be used to generate a range of outcomes — regardless of model type.</p>
<p><img src="pics/workboots.PNG" /></p>
<div id="installation" class="section level3">
<h3>Installation</h3>
<p>Version 0.1.0 of <code>{workboots}</code> is available on <a href="https://cran.r-project.org/web/packages/workboots/index.html">CRAN</a>. Given that the package is still in early development, however, I’d recommend installing the development version from <a href="https://github.com/markjrieke/workboots">github</a>:</p>
<pre class="r"><code># install from CRAN
install.packages(&quot;workboots&quot;)

# or install the development version
devtools::install_github(&quot;markjrieke/workboots&quot;)</code></pre>
</div>
<div id="usage" class="section level3">
<h3>Usage</h3>
<p><code>{workboots}</code> builds on top of the <a href="https://www.tidymodels.org/"><code>{tidymodels}</code></a> suite of packages and is intended to be used in conjunction with a <a href="https://workflows.tidymodels.org/">tidymodel workflow</a>. Teaching how to use <code>{tidymodels}</code> is beyond the scope of this post, but some helpful resources are linked at the bottom for further exploration.</p>
<p>We’ll walk through two examples that show the benefit of the package: estimating a linear model’s prediction interval and generating a prediction interval for a boosted tree model.</p>
</div>
<div id="estimating-a-prediction-interval" class="section level3">
<h3>Estimating a prediction interval</h3>
<p>Let’s get started with a model we know can generate a prediction interval: a basic linear model. In this example, we’ll use the <a href="https://modeldata.tidymodels.org/reference/ames.html">Ames housing dataset</a> to predict a home’s price based on its square footage.</p>
<pre class="r"><code>library(tidymodels)

# setup our data
data(&quot;ames&quot;)
ames_mod &lt;- ames %&gt;% select(First_Flr_SF, Sale_Price)

# relationship between square footage and price
ames_mod %&gt;%
  ggplot(aes(x = First_Flr_SF, y = Sale_Price)) +
  geom_point(alpha = 0.25) +
  scale_y_continuous(labels = scales::dollar_format(), trans = &quot;log10&quot;) +
  scale_x_continuous(labels = scales::comma_format(), trans = &quot;log10&quot;) +
  labs(title = &quot;Relationship between Square Feet and Sale Price&quot;,
       subtitle = &quot;Linear relationship between the log transforms of square footage and price&quot;,
       x = NULL,
       y = NULL)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-3-1.png" width="4500" /></p>
<p>We can use a linear model to predict the log transform of <code>Sale_Price</code> based on the log transform of <code>First_Flr_SF</code>. In this example, we’ll train a linear model then plot our predictions against a holdout set with a prediction interval.</p>
<pre class="r"><code># log transform
ames_mod &lt;- 
  ames_mod %&gt;%
  mutate(across(everything(), log10))

# split into train/test data
set.seed(918)
ames_split &lt;- initial_split(ames_mod)
ames_train &lt;- training(ames_split)
ames_test &lt;- testing(ames_split)</code></pre>
<pre class="r"><code># train a linear model
set.seed(314)
mod &lt;- lm(Sale_Price ~ First_Flr_SF, data = ames_train)

# predict on new data with a prediction interval
ames_preds &lt;-
  mod %&gt;%
  predict(ames_test, interval = &quot;predict&quot;) %&gt;%
  as_tibble()

# plot!
ames_preds %&gt;%
  
  # re-scale predictions to match the original dataset&#39;s scale
  bind_cols(ames_test) %&gt;%
  mutate(across(everything(), ~10^.x)) %&gt;%
  
  # add geoms
  ggplot(aes(x = First_Flr_SF)) +
  geom_point(aes(y = Sale_Price),
             alpha = 0.25) +
  geom_line(aes(y = fit),
            size = 1) +
  geom_ribbon(aes(ymin = lwr,
                  ymax = upr),
              alpha = 0.25) +
  scale_y_continuous(labels = scales::dollar_format(), trans = &quot;log10&quot;) +
  scale_x_continuous(labels = scales::comma_format(), trans = &quot;log10&quot;) +
  labs(title = &quot;Linear Model of Sale Price&quot;,
       subtitle = &quot;Shaded area represents the 95% prediction interval&quot;,
       x = NULL,
       y = NULL) </code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-6-1.png" width="4500" /></p>
<p>With <code>{workboots}</code>, we can approximate the linear model’s prediction interval by passing a workflow built on a linear model to the function <code>predict_boots()</code>.</p>
<pre class="r"><code>library(tidymodels)
library(workboots)

# setup a workflow with a linear model
ames_wf &lt;-
  workflow() %&gt;%
  add_recipe(recipe(Sale_Price ~ First_Flr_SF, data = ames_train)) %&gt;%
  add_model(linear_reg())

# generate bootstrap predictions on ames_test
set.seed(713)
ames_preds_boot &lt;-
  ames_wf %&gt;%
  predict_boots(
    n = 2000,
    training_data = ames_train,
    new_data = ames_test
  )</code></pre>
<p><code>predict_boots()</code> works by creating 2000 <a href="https://rsample.tidymodels.org/reference/bootstraps.html">bootstrap resamples</a> of the training data, fitting a linear model to each resample, then generating 2000 predictions for each home’s price in the holdout set. We can then use <code>summarise_predictions()</code> to generate upper and lower intervals for each prediction.</p>
<pre class="r"><code>ames_preds_boot %&gt;%
  summarise_predictions()</code></pre>
<pre><code>## # A tibble: 733 x 5
##    rowid .preds               .pred_lower .pred .pred_upper
##    &lt;int&gt; &lt;list&gt;                     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;
##  1     1 &lt;tibble [2,000 x 2]&gt;        5.17  5.44        5.71
##  2     2 &lt;tibble [2,000 x 2]&gt;        4.98  5.27        5.55
##  3     3 &lt;tibble [2,000 x 2]&gt;        4.97  5.25        5.52
##  4     4 &lt;tibble [2,000 x 2]&gt;        5.12  5.40        5.67
##  5     5 &lt;tibble [2,000 x 2]&gt;        5.15  5.44        5.71
##  6     6 &lt;tibble [2,000 x 2]&gt;        4.93  5.21        5.49
##  7     7 &lt;tibble [2,000 x 2]&gt;        4.67  4.94        5.22
##  8     8 &lt;tibble [2,000 x 2]&gt;        4.85  5.13        5.40
##  9     9 &lt;tibble [2,000 x 2]&gt;        4.87  5.14        5.41
## 10    10 &lt;tibble [2,000 x 2]&gt;        5.14  5.41        5.69
## # ... with 723 more rows</code></pre>
<p>By overlaying the intervals on top of one another, we can see that the prediction interval generated by <code>predict_boots()</code> is a good approximation of the theoretical interval generated by <code>lm()</code>.</p>
<pre class="r"><code>ames_preds_boot %&gt;%
  summarise_predictions() %&gt;%
  bind_cols(ames_test) %&gt;%
  ggplot(aes(x = First_Flr_SF, y = .pred)) + geom_point()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-10-1.png" width="4500" /></p>
</div>
