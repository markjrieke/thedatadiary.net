---
title: Hierarchical Hospitals
author: ''
date: '2022-11-14'
slug: []
categories:
- rstats
- stan
- healthcare
tags: []
subtitle: 'Multilevel Models for Patient Satisfaction'
summary: ''
authors: []
lastmod: '2022-11-14T18:08:50-06:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
math: true
---



<div id="hierarchical-hospitals" class="section level2">
<h2>Hierarchical Hospitals</h2>
<p>If the past year of working at a large hospital system has taught me one thing, it’s that hospitals are a Russian nesting doll of structure. Within the hospital system, there are several campuses. Within each campus, there are several service areas (inpatient, outpatient, emergency, day surgery, etc.). And finally, within each service area at each campus, there can be many individual hospital units.</p>
<p>Having worked with patient satisfaction data, I know that each of these levels contains useful information that may be beneficial to include in a model. Hospital A, for example, tends to receive better reviews than Hospital B, but within Hospital A the labor &amp; delivery unit tends to receive better reviews than the intensive care unit. Including every single unit as a categorical predictor isn’t a great modeling choice, since information about each unit remains separate (<em>no pooling</em>). On the other hand, ignoring the nested structure lumps all data points together (<em>complete pooling</em>), implicitly making the assumption that the data is independent, which can generate misleading predictions!</p>
<p>This is where hierarchical models come into play! Hierarchical models offer a happy middle ground and allow for <em>partial pooling</em> of information between groups. This approach allows for information to be shared across groups while still treating each group as unique (this is a pretty simplistic summary of hierarchical models; for a more detailed introduction, see <a href="https://www.bayesrulesbook.com/chapter-15.html">Chapter 15 of Bayes Rules!</a>).</p>
<p>While there are <a href="https://github.com/lme4/lme4/">non-Bayesian approaches</a> to hierarchical models, they mesh well with a Bayesian framework, so in this post I’ll build a Bayesian model to predict satisfaction scores based on simulated hospital data.</p>
</div>
<div id="simulating-hospital-data" class="section level2">
<h2>Simulating Hospital Data</h2>
<p>I can’t share live data, so I’ll need to simulate some fake data for this example. Let’s start with five hospitals, each with different baseline levels of satisfaction.</p>
<pre class="r"><code>library(tidyverse)
library(rstanarm)
library(tidybayes)
library(ggdist)
library(tidytext)

# reproducibility!
set.seed(54321)

# manually assign hospital-level intercept
hospital_prob &lt;- 
  tibble(hospital = paste(&quot;Hospital&quot;, seq(1:5)),
         hospital_prob = seq(from = 0.6, to = 0.8, length.out = 5) ,
         hospital_sigma = rep(0.2, 5))

hospital_prob %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">hospital</th>
<th align="right">hospital_prob</th>
<th align="right">hospital_sigma</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Hospital 1</td>
<td align="right">0.60</td>
<td align="right">0.2</td>
</tr>
<tr class="even">
<td align="left">Hospital 2</td>
<td align="right">0.65</td>
<td align="right">0.2</td>
</tr>
<tr class="odd">
<td align="left">Hospital 3</td>
<td align="right">0.70</td>
<td align="right">0.2</td>
</tr>
<tr class="even">
<td align="left">Hospital 4</td>
<td align="right">0.75</td>
<td align="right">0.2</td>
</tr>
<tr class="odd">
<td align="left">Hospital 5</td>
<td align="right">0.80</td>
<td align="right">0.2</td>
</tr>
</tbody>
</table>
<p>In this example, Hospital 1 will tend to have the lowest scores while Hospital 5 will tend to have the highest scores. Within each hospital, we want individual unit-level scores to be able to vary randomly.</p>
<pre class="r"><code># simulate 5 hospitals within the system, each with 100 units (500 total)
satisfaction &lt;- 
  tibble(hospital = rep(paste(&quot;Hospital&quot;, seq(1:5)), 100)) %&gt;%
  arrange(hospital) %&gt;%
  
  # add in the units at each hospital
  bind_cols(unit = rep(paste(&quot;Unit&quot;, seq(1:100)), 5)) %&gt;%
  mutate(unit = paste(hospital, unit)) %&gt;%
  
  # add in the hospital-level intercept
  left_join(hospital_prob, by = &quot;hospital&quot;) %&gt;%
  
  # estimate a unit-level intercept 
  rowwise() %&gt;%
  mutate(unit_offset = rnorm(1, 0, 0.05),
         unit_prob = gamlss.dist::rBE(1, hospital_prob + unit_offset, hospital_sigma)) %&gt;%
  select(hospital, unit, unit_prob) %&gt;%
  
  # generate fake responses
  mutate(n = round(rlnorm(1, log(100), 1.5)),
         topbox = rbinom(1, n, unit_prob)) %&gt;%
  ungroup() %&gt;%
  select(-unit_prob)

# display example at each hospital
set.seed(333)
satisfaction %&gt;%
  group_by(hospital) %&gt;%
  slice_sample(n = 1) %&gt;%
  mutate(score = topbox/n) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">hospital</th>
<th align="left">unit</th>
<th align="right">n</th>
<th align="right">topbox</th>
<th align="right">score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Hospital 1</td>
<td align="left">Hospital 1 Unit 14</td>
<td align="right">35</td>
<td align="right">21</td>
<td align="right">0.6000000</td>
</tr>
<tr class="even">
<td align="left">Hospital 2</td>
<td align="left">Hospital 2 Unit 41</td>
<td align="right">137</td>
<td align="right">107</td>
<td align="right">0.7810219</td>
</tr>
<tr class="odd">
<td align="left">Hospital 3</td>
<td align="left">Hospital 3 Unit 55</td>
<td align="right">298</td>
<td align="right">252</td>
<td align="right">0.8456376</td>
</tr>
<tr class="even">
<td align="left">Hospital 4</td>
<td align="left">Hospital 4 Unit 66</td>
<td align="right">12</td>
<td align="right">7</td>
<td align="right">0.5833333</td>
</tr>
<tr class="odd">
<td align="left">Hospital 5</td>
<td align="left">Hospital 5 Unit 39</td>
<td align="right">211</td>
<td align="right">169</td>
<td align="right">0.8009479</td>
</tr>
</tbody>
</table>
<p>This unit level variation is important! Even though units within certain hospitals tend to perform worse than units in others, individual units at lower-rated hospitals can still outperform highly-rated hospitals! An easier way to see both the hospital-level and unit-level variation is to place all on the same plot.</p>
<pre class="r"><code>satisfaction %&gt;%
  mutate(score = topbox/n) %&gt;%
  ggplot(aes(x = hospital,
             y = score,
             size = n,
             color = hospital)) + 
  geom_boxplot() + 
  geom_jitter(alpha = 0.25) + 
  coord_flip() +
  scale_color_brewer(palette = &quot;Dark2&quot;) +
  scale_size_continuous(range = c(1, 15)) +
  scale_y_continuous(labels = scales::label_percent()) +
  theme(legend.position = &quot;none&quot;) +
  labs(title = &quot;It&#39;s in the way that you Units&quot;,
       subtitle = &quot;Satisfaction scores vary both **across** and **within** hospitals&quot;,
       x = NULL,
       y = NULL)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-2-1.png" width="4500" /></p>
<p>Here, each point represents an individual unit within a hospital — larger points indicate units with more responses. There’s clearly variation <em>across</em> hospitals, but also variation <em>within</em> each hospital. We can generally trust that the satisfaction score for units with lots of responses is accurate, but units with only few responses can be misleading — some have scores of 100%! I don’t think that these units are actually perfect, it’s likelier that they got lucky.</p>
<p>A hierarchical model will allow us to pool all this information together — when a unit has lots of responses, the model’s estimate of their true score will land pretty close to their raw score. When a unit only has a few responses, however, the model will shrink the estimate of their true score towards the hospital group-level average.</p>
</div>
<div id="fitting-a-model" class="section level2">
<h2>Fitting a Model</h2>
<p>I’ve found recently that writing out a model specification helps, so let’s write out the model and priors.</p>
<p><span class="math display">\[
\begin{gather}
y_{unit} \sim Binomial(\pi_{unit}, n_{unit}) \\
logit(\pi_{unit}) = \mu + \beta_{hospital} + \beta_{unit} \\
\beta_{hospital} \sim Normal(0, 2) \\
\beta_{unit} \sim Normal(0, 2)
\end{gather}
\]</span></p>
<p>In this case, the number of topbox responses at each unit, <span class="math inline">\(y_{unit}\)</span>, is estimated with a binomial distribution where each patient within that visits the unit has a probability <span class="math inline">\(\pi_{unit}\)</span> of selecting the topbox response. <span class="math inline">\(\pi_{unit}\)</span> is allowed to vary from the global mean, <span class="math inline">\(\mu\)</span>, both by hospital (<span class="math inline">\(\beta_{hospital}\)</span>) and by unit (<span class="math inline">\(\beta_{unit}\)</span>). This can be implemented in <a href="https://mc-stan.org/">Stan</a> via the <a href="https://mc-stan.org/rstanarm/index.html"><code>{rstanarm}</code></a> package.</p>
<pre class="r"><code># run chains on separte cores
options(mc.cores = parallel::detectCores())

# fit a bayesian model!
satisfaction_model &lt;- 
  stan_glmer(
    cbind(topbox, n - topbox) ~ (1 | hospital) + (1 | unit),
    data = satisfaction,
    family = binomial(),
    prior_intercept = normal(0, 2, autoscale = TRUE),
    prior = normal(0, 2, autoscale = TRUE),
    prior_covariance = decov(regularization = 1, concentration = 1, shape = 1, scale = 1),
    chains = 4,
    iter = 2000,
    seed = 999
  )</code></pre>
<p>This model gives us <em>exactly what we were looking for</em> — units with many responses have posterior estimations of <span class="math inline">\(\pi_{unit}\)</span> that are close to the raw score and have relatively small credible intervals, while the posterior estimate of <span class="math inline">\(\pi_{unit}\)</span> for a unit with few responses is shrunken towards the hospital average with relatively wide credible intervals.</p>
<pre class="r"><code>set.seed(88)
satisfaction %&gt;%
  
  # draw 1000 posterior predictions of pi for each unit
  tidybayes::add_epred_draws(satisfaction_model, ndraws = 1000) %&gt;%
  ungroup() %&gt;%
  
  # select a sample of 3 random units from each hospital to plot
  nest(preds = -c(hospital, unit, n, topbox)) %&gt;%
  group_by(hospital) %&gt;%
  slice_sample(n = 3) %&gt;%
  mutate(unit = str_sub(unit, start = 12),
         unit = glue::glue(&quot;{unit}\n(n = {scales::label_comma()(n)})&quot;),
         med_pred = map_dbl(preds, ~quantile(.x$.epred, probs = 0.5))) %&gt;%
  ungroup() %&gt;%
  unnest(preds) %&gt;%
  
  # plot!
  ggplot(aes(x = reorder_within(unit, med_pred, hospital),
             y = .epred,
             color = hospital)) +
  stat_pointinterval() +
  scale_x_reordered() +
  scale_y_continuous(labels = scales::label_percent()) +
  coord_flip() +
  facet_wrap(~hospital, scales = &quot;free&quot;) +
  theme(legend.position = &quot;none&quot;) +
  labs(title = &quot;\u03C0 in the sky&quot;,
       subtitle = &quot;Posterior estimations of \u03C0 for a random sampling of units&quot;, 
       x = NULL,
       y = NULL,
       caption = &quot;Pointrange indicates the&lt;br&gt;66% &amp; 95% posterior credible interval&quot;) +
  scale_color_brewer(palette = &quot;Dark2&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/explore%20fit-1.png" width="4500" /></p>
<p>With a hierarchical model, we can even make predictions for new units that didn’t appear in the original training data. If we were to introduce a new unit at each hospital, the model can still rely on the hospital-level term to estimate scores.</p>
<pre class="r"><code>set.seed(1)
tibble(hospital = paste(&quot;Hospital&quot;, seq(1:5))) %&gt;%
  mutate(unit = glue::glue(&quot;{hospital}\nNew Unit&quot;)) %&gt;%
  add_epred_draws(satisfaction_model) %&gt;%
  ggplot(aes(x = unit,
             y = .epred,
             color = unit)) + 
  stat_pointinterval() +
  scale_y_continuous(labels = scales::label_percent()) +
  coord_flip() +
  scale_color_brewer(palette = &quot;Dark2&quot;) +
  theme(legend.position = &quot;none&quot;) +
  labs(title = &quot;New unit, who this?&quot;,
       subtitle = &quot;Posterior estimations of \u03C0 for hypothetical new units at each hospital&quot;,
       x = NULL,
       y = NULL,
       caption = &quot;Pointrange indicates the&lt;br&gt;66% and 95% posterior credible interval&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/predicting%20for%20a%20new%20unit-1.png" width="4500" /></p>
</div>
<div id="some-closing-thoughts" class="section level2">
<h2>Some closing thoughts</h2>
<p>The model I used here can be referred to as a <em>random intercept</em> model. In this case, the intercept is allowed to vary by hospital and unit. Had I included a predictor term — age, for example — I could have put together a <em>random slope</em> model, which would have allowed the age term to also vary by hospital and unit. This would allow for one hospital to be modeled as having better scores for young patients while another hospital could see better scores for older patients. This sort of flexibility is useful, but in my experience, simply accounting for the hierarchical structure of the data with an intercept-only model gets you 90% of where you need to go!</p>
</div>
