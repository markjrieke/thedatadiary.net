<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academia 4.3.1">
  <meta name="generator" content="Hugo 0.88.1" />

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Mark Rieke">

  
  
  
    
  
  <meta name="description" content="Over the past few years, the hospital system I work for has transitioned from the old metric for measuring patient satisfaction, Likelihood to Recommend (LTR), to a newer metric, Net Promoter Score (NPS).">

  
  <link rel="alternate" hreflang="en-us" href="https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/">

  


  

  
  
  
  <meta name="theme-color" content="#fc6f5c">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Open+Sans|Roboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academia.min.84fde4bbd64cc9b4e40ee4ad0a0d9576.css">

  

  
  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'G-QM3KW2TS7Q', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/">

  
  
  
  
    
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@markjrieke">
  <meta property="twitter:creator" content="@markjrieke">
  
  <meta property="og:site_name" content="the data diary">
  <meta property="og:url" content="https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/">
  <meta property="og:title" content="My 2022 Magnum Opus | the data diary">
  <meta property="og:description" content="Over the past few years, the hospital system I work for has transitioned from the old metric for measuring patient satisfaction, Likelihood to Recommend (LTR), to a newer metric, Net Promoter Score (NPS)."><meta property="og:image" content="https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/featured.png">
  <meta property="twitter:image" content="https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/featured.png"><meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2022-12-30T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2022-12-30T16:07:32-06:00">
  

  


  





  <title>My 2022 Magnum Opus | the data diary</title>

</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">the data diary</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation"><span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">
      
      
      <ul class="navbar-nav ml-auto">
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>2022 Midterm Forecast</span><span class="caret"></span>
          </a>
          <ul class="dropdown-menu">
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/senate"><span>Senate</span></a>
            </li>
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/house"><span>House</span></a>
            </li>
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/governor"><span>Governor</span></a>
            </li>
            
          </ul>
        </li>

        
        

        

        
        
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/blog"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/about"><span>About</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/resume"><span>Resume</span></a>
        </li>

        
        

      

        

        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>
    </div>
  </div>
</nav>


  <article class="article py-5" itemscope itemtype="http://schema.org/Article">

  













<div class="container split-header">
  <div class="row justify-content-center">
    <div class="col-lg-8">
        <img class="img-fluid w-100" src="/blog/2022-12-30-my-2022-magnum-opus/featured_hu4ba5fbf5e3003b096be6946957781750_601436_680x500_fill_q90_lanczos_smart1_3.png" itemprop="image" alt="">
        
    </div>
    <div class="col-lg-8">
      <h1 itemprop="name">My 2022 Magnum Opus</h1>

      
      <p class="page-subtitle">Ordered categorical models for estimating Net Promoter Score: a hierarchical Bayesian implementation</p>
      

      



<meta content="2022-12-30 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2022-12-30 16:07:32 -0600 CST" itemprop="dateModified">

<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    <time>Dec 30, 2022</time>
  </span>
  

  

  

  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder"></i>
    <a href="/categories/rstats/">rstats</a>, <a href="/categories/stan/">stan</a>, <a href="/categories/healthcare/">healthcare</a></span>
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/&amp;text=My%202022%20Magnum%20Opus" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/&amp;t=My%202022%20Magnum%20Opus" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=My%202022%20Magnum%20Opus&amp;body=https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/&amp;title=My%202022%20Magnum%20Opus" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=My%202022%20Magnum%20Opus%20https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/&amp;title=My%202022%20Magnum%20Opus" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

      














    </div>
    
    </div>
  </div>
</div>

  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      


<p>Over the past few years, the hospital system I work for has transitioned from the old metric for measuring patient satisfaction, Likelihood to Recommend (LTR), to a newer metric, Net Promoter Score (NPS). Both metrics ask the same question — <em>how likely are you to recommend this hospital to a friend or relative?</em> — but they are measured very differently. The score for LTR is simply the percentage of patients who respond with the “topbox” option of <em>Very likely</em> on a scale from <em>Very likely</em> to <em>Very unlikely</em>. NPS, on the other hand, is a bit more involved. Patients are categorized based on their response on a 0-10 point scale: responses between 0 and 6 are considered <em>detractors</em>, 7 and 8s are considered <em>passives</em>, while 9 and 10s are considered <em>promoters</em>. The score for NPS is then the percentage of promoters <em>minus</em> the percentage of detractors.</p>
<p><span class="math display">\[
\begin{gather}
\text{NPS} = \text{Promoter %} - \text{Detractor %}
\end{gather}
\]</span></p>
<p>As a metric, NPS is a bit better than the alternative of LTR, since it is somewhat able to take into account the distribution of responses along the 0-10 scale. Consider the following set of responses (and let’s just pretend for sake of example that “promoter” here is equivalent to “topbox”). LTR’s topbox isn’t able to detect a difference in scores since promoters comprise 50% of responses in both sets. NPS, however, <em>can</em> detect a difference, since the second set is rewarded for have fewer detractors than the first set.</p>
<pre class="r"><code>tibble::tribble(
  ~promoters, ~passives, ~detractors, ~topbox, ~nps,
  &quot;50%&quot;, &quot;25%&quot;, &quot;25%&quot;, &quot;50%&quot;, &quot;25%&quot;,
  &quot;50%&quot;, &quot;50%&quot;, &quot;0%&quot;, &quot;50%&quot;, &quot;50%&quot;
) |&gt;
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">promoters</th>
<th align="left">passives</th>
<th align="left">detractors</th>
<th align="left">topbox</th>
<th align="left">nps</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">50%</td>
<td align="left">25%</td>
<td align="left">25%</td>
<td align="left">50%</td>
<td align="left">25%</td>
</tr>
<tr class="even">
<td align="left">50%</td>
<td align="left">50%</td>
<td align="left">0%</td>
<td align="left">50%</td>
<td align="left">50%</td>
</tr>
</tbody>
</table>
<p>With only a few sets of responses to compare, this seems like a trivial improvement — since we have all the data, why don’t we just look at the response distribution for every set? In practice, however, I’m often looking at responses for <em>hundreds</em> of individual hospital units across the system — encoding this extra bit of information into a single number allows for a more nuanced comparison <em>without</em> any costs to the viewer’s cognitive load.</p>
<p>Unfortunately, there’s no free lunch here, and the additional nuance that NPS provides comes at the cost of modeling complexity. Relative modeling binary choices like LTR’s topbox, the ecosystem for modeling the choice between three or more categories is far smaller. Additionally, the <em>order</em> of the categories matters — a promoter response is better than a passive response, which is better than a detractor response. This adds a layer of complexity over unordered categories (e.g., red, blue, or green).</p>
<p>Fortunately, I’m not the first person to run into this problem. I’ve been (slowly) working through <a href="https://xcelab.net/rm/">Richard McElreath’s</a> <a href="https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919">Statistical Rethinking</a>, which conveniently covers this topic directly (and comes with the added benefit of utilizing a Bayesian approach via <a href="https://mc-stan.org/">Stan</a>).</p>
<p>Let’s test both my understanding of the ordered categorical data-generating process and my ability to model it by doing a few things:</p>
<ol style="list-style-type: decimal">
<li>Define a data-generating process that links a linear model to an ordered categorical outcome.</li>
<li>Manually set model parameters and simulate responses.</li>
<li>See if I can recover those parameters with a model in Stan.</li>
</ol>
<div id="the-data-generating-process" class="section level2">
<h2>The data generating process</h2>
<pre class="r"><code>library(tidyverse)
library(rethinking)
library(riekelib)
library(broom.mixed)

# I&#39;ve been having ~issues~ with cmdstan, so switching default to rstan
set_ulam_cmdstan(FALSE)</code></pre>
<p>Each patient’s response <span class="math inline">\(R_i\)</span> can be described as a probability <span class="math inline">\(p_i\)</span> of selecting from each of the three available categories:</p>
<p><span class="math display">\[
\begin{gather}
R_i \sim \text{Categorical}(p_i) \\
p_i = \langle p_{\text{detractor}[i]}, \ p_{\text{passive}[i]}, \ p_{\text{promoter}[i]} \rangle \\
\end{gather}
\]</span></p>
<p>There’s a useful math trick we can use to enforce the order of the categories. Rather than working with the <em>individual probability</em> of each category directly, we can instead define the probabilities in terms of the <em>cumulative probability</em> of each category. For example, in the set below, the probability of selecting “passive” is 10%, but the <em>cumulative probability</em> of selecting a rating of passive <em>or lower</em> is 30%:</p>
<pre class="r"><code>probs_example &lt;- 
  tibble(nps_group = as_factor(c(&quot;detractor&quot;, &quot;passive&quot;, &quot;promoter&quot;)),
         prob = c(0.2, 0.1, 0.7),
         cumulative_prob = cumsum(prob))

probs_example %&gt;%
  mutate(across(ends_with(&quot;prob&quot;), ~scales::label_percent(accuracy = 1)(.x))) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">nps_group</th>
<th align="left">prob</th>
<th align="left">cumulative_prob</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">detractor</td>
<td align="left">20%</td>
<td align="left">20%</td>
</tr>
<tr class="even">
<td align="left">passive</td>
<td align="left">10%</td>
<td align="left">30%</td>
</tr>
<tr class="odd">
<td align="left">promoter</td>
<td align="left">70%</td>
<td align="left">100%</td>
</tr>
</tbody>
</table>
<p>With this in mind, any individual probability can be described as the difference between two cumulative probabilities <span class="math inline">\(q_k\)</span>.</p>
<pre class="r"><code>probs_example %&gt;%
  ggplot(aes(x = nps_group,
             xend = nps_group)) + 
  geom_hline(yintercept = c(0.2, 0.3),
             linetype = &quot;dashed&quot;,
             color = &quot;gray60&quot;) + 
  geom_segment(aes(y = 0,
                   yend = cumulative_prob)) +
  geom_point(aes(y = cumulative_prob)) +
  geom_segment(aes(y = cumulative_prob - prob,
                   yend = cumulative_prob),
               color = &quot;blue&quot;,
               position = position_nudge(x = 0.125)) + 
  geom_point(aes(y = cumulative_prob),
             color = &quot;blue&quot;,
             position = position_nudge(x = 0.125)) + 
  geom_text(x = 1 - 0.1,
            y = 0.1,
            label = &quot;q1: 20%&quot;,
            hjust = &quot;right&quot;) +
  geom_text(x = 1 + 0.125 + 0.1,
            y = 0.1,
            label = &quot;p1: 20%&quot;,
            hjust = &quot;left&quot;,
            color = &quot;blue&quot;) + 
  geom_text(x = 2 - 0.1,
            y = 0.25,
            label = &quot;q2: 30%&quot;,
            hjust = &quot;right&quot;) +
  geom_text(x = 2 + 0.125 + 0.1,
            y = 0.25,
            label = &quot;p2: 10%&quot;,
            hjust = &quot;left&quot;,
            color = &quot;blue&quot;) +
  geom_text(x = 3 + 0.125 + 0.1,
            y = 0.625,
            label = &quot;p3: 70%&quot;,
            color = &quot;blue&quot;,
            hjust = &quot;left&quot;) +
  expand_limits(y = c(0, 1)) +
  scale_y_continuous(labels = scales::label_percent()) +
  labs(title = glue::glue(&quot;**Cumulative** and 
                          **{color_text(&#39;individual&#39;, &#39;blue&#39;)}** 
                          probabilities of each response&quot;),
       x = NULL,
       y = NULL)</code></pre>
<p><img src="https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/index_files/figure-html/cumulative-prob-plot-1.png" width="4500" /></p>
<p>The probability of selecting a response less than detractor is 0% and the probability of selecting a response of promoter <em>or lower</em> is 100%, so we can rewrite the individual probabilities in terms of just two cumulative probabilities.</p>
<p><span class="math display">\[
\begin{gather}
R_i \sim \text{Categorical}(p_i) \\
p_i = \langle p_{\text{detractor}[i]}, \ p_{\text{passive}[i]}, \ p_{\text{promoter}[i]} \rangle \\
\color{blue}{p_{\text{detractor}[i]} = q_{1, i} \\
p_{\text{passive}[i]} = q_{2,i} - q_{1,i} \\
p_{\text{promoter}[i]} = 1 - q_{2,i}}
\end{gather}
\]</span></p>
<p>In the <a href="https://en.wikipedia.org/wiki/Logit">logit</a> space, these two cumulative probabilities can be represented by a linear model’s output <span class="math inline">\(\phi_i\)</span> <em>relative</em> to a set of <span class="math inline">\(k = 2\)</span> “cutpoints” <span class="math inline">\(\kappa_k\)</span>.</p>
<p><span class="math display">\[
\begin{gather}
R_i \sim \text{Categorical}(p_i) \\
p_i = \langle p_{\text{detractor}[i]}, \ p_{\text{passive}[i]}, \ p_{\text{promoter}[i]} \rangle \\
p_{\text{detractor}[i]} = q_{1, i} \\
p_{\text{passive}[i]} = q_{2,i} - q_{1,i} \\
p_{\text{promoter}[i]} = 1 - q_{2,i} \\
\color{blue}{\text{logit}(q_{k, i}) = \kappa_k - \phi_i \\
\phi_i = \text{some linear model}}
\end{gather}
\]</span></p>
<p>This is all a bit involved but I wouldn’t worry about the details too much. The important takeaway is that we now have a linear model <span class="math inline">\(\phi\)</span> that maps to a categorical outcome while preserving the order of the categories.</p>
</div>
<div id="simulating-data" class="section level2">
<h2>Simulating data</h2>
<p>In this case, let’s let <span class="math inline">\(\phi\)</span> vary by the patient’s age and the hospital unit they visit:</p>
<p><span class="math display">\[
\begin{gather}
R_i \sim \text{Categorical}(p_i) \\
p_i = \langle p_{\text{detractor}[i]}, \ p_{\text{passive}[i]}, \ p_{\text{promoter}[i]} \rangle \\
p_{\text{detractor}[i]} = q_{1, i} \\
p_{\text{passive}[i]} = q_{2,i} - q_{1,i} \\
p_{\text{promoter}[i]} = 1 - q_{2,i} \\
\text{logit}(q_{k, i}) = \kappa_k - \phi_i \\
\phi_i = \color{blue}{\beta_{\text{unit}[i]} + \beta_{\text{age}} \ \text{age}_i}
\end{gather}
\]</span></p>
<p>We’ll manually fix the <span class="math inline">\(\beta_{\text{unit}}\)</span> term for each unit. Additionally, let’s define a sampling weight so that the simulated data ends up with a wide range of response counts.</p>
<pre class="r"><code>unit_params &lt;-
  tribble(
    ~unit, ~beta, ~weight,
    &quot;A&quot;, 1.00, 2,
    &quot;B&quot;, 0.66, 3,
    &quot;C&quot;, 0.33, 2,
    &quot;D&quot;, 0.00, 4,
    &quot;E&quot;, -0.50, 1
  )

unit_params %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">unit</th>
<th align="right">beta</th>
<th align="right">weight</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">A</td>
<td align="right">1.00</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="left">B</td>
<td align="right">0.66</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="left">C</td>
<td align="right">0.33</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="left">D</td>
<td align="right">0.00</td>
<td align="right">4</td>
</tr>
<tr class="odd">
<td align="left">E</td>
<td align="right">-0.50</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
<p>Here, unit A is likely to have the best scores, while unit E is likely to have the worst. Unit D is likely to have the most returns, while unit E is likely to have the fewest. The randomization/discretization means that the simulated scores won’t match the expected scores exactly, but if we set cutpoints in the logit space, we can work backwards through the data-generating process to see the expected score for each unit.</p>
<pre class="r"><code># set cutpoints in the logit space
cutpoints &lt;- c(-0.5, -0.15)

# here&#39;s how we expect the units to score for an average aged patient
unit_params %&gt;%
  mutate(q1 = cutpoints[1] - beta,
         q2 = cutpoints[2] - beta,
         detractor = expit(q1),
         passive = expit(q2) - expit(q1),
         promoter = 1 - expit(q2)) %&gt;%
  select(unit, weight, promoter, passive, detractor) %&gt;%
  mutate(nps = promoter - detractor,
         across(c(promoter:nps), ~scales::label_percent(accuracy = 1)(.x))) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">unit</th>
<th align="right">weight</th>
<th align="left">promoter</th>
<th align="left">passive</th>
<th align="left">detractor</th>
<th align="left">nps</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">A</td>
<td align="right">2</td>
<td align="left">76%</td>
<td align="left">6%</td>
<td align="left">18%</td>
<td align="left">58%</td>
</tr>
<tr class="even">
<td align="left">B</td>
<td align="right">3</td>
<td align="left">69%</td>
<td align="left">7%</td>
<td align="left">24%</td>
<td align="left">45%</td>
</tr>
<tr class="odd">
<td align="left">C</td>
<td align="right">2</td>
<td align="left">62%</td>
<td align="left">8%</td>
<td align="left">30%</td>
<td align="left">31%</td>
</tr>
<tr class="even">
<td align="left">D</td>
<td align="right">4</td>
<td align="left">54%</td>
<td align="left">9%</td>
<td align="left">38%</td>
<td align="left">16%</td>
</tr>
<tr class="odd">
<td align="left">E</td>
<td align="right">1</td>
<td align="left">41%</td>
<td align="left">9%</td>
<td align="left">50%</td>
<td align="left">-9%</td>
</tr>
</tbody>
</table>
<p>Now let’s simulate patient visits. We’ll have 500 patients return surveys and the number of returns at each unit will be proportional to the <code>weight</code> set earlier.</p>
<pre class="r"><code>n_patients &lt;- 500

# simulate patient visits
set.seed(30)
unit_samples &lt;-
  sample(
    unit_params$unit,
    size = n_patients,
    prob = unit_params$weight,
    replace = TRUE
  )</code></pre>
<pre class="r"><code>tibble(unit = unit_samples) %&gt;%
  percent(unit, .keep_n = TRUE) %&gt;%
  mutate(pct = scales::label_percent(accuracy = 1)(pct)) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">unit</th>
<th align="right">n</th>
<th align="left">pct</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">A</td>
<td align="right">69</td>
<td align="left">14%</td>
</tr>
<tr class="even">
<td align="left">B</td>
<td align="right">111</td>
<td align="left">22%</td>
</tr>
<tr class="odd">
<td align="left">C</td>
<td align="right">78</td>
<td align="left">16%</td>
</tr>
<tr class="even">
<td align="left">D</td>
<td align="right">195</td>
<td align="left">39%</td>
</tr>
<tr class="odd">
<td align="left">E</td>
<td align="right">47</td>
<td align="left">9%</td>
</tr>
</tbody>
</table>
<p>Now let’s add in each patient’s age. In this case, we won’t include any relationship between age and unit; age will just vary randomly across all units. In reality, this often isn’t the case — you can imagine, for example, that patients visiting a Labor &amp; Delivery unit will tend to be younger than patients visiting a Geriatric unit! Ignoring this reality, in our simulated patient population the ages will vary generally between 25 and 65.</p>
<pre class="r"><code># simulate ages of patients &amp; combine with the unit visited
set.seed(31)
patients &lt;-
  tibble(
    unit = unit_samples,
    age = round(rnorm(n_patients, 45, 10))
  )</code></pre>
<pre class="r"><code>patients %&gt;%
  slice_head(n = 10) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">unit</th>
<th align="right">age</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">D</td>
<td align="right">46</td>
</tr>
<tr class="even">
<td align="left">B</td>
<td align="right">43</td>
</tr>
<tr class="odd">
<td align="left">B</td>
<td align="right">61</td>
</tr>
<tr class="even">
<td align="left">B</td>
<td align="right">55</td>
</tr>
<tr class="odd">
<td align="left">D</td>
<td align="right">60</td>
</tr>
<tr class="even">
<td align="left">D</td>
<td align="right">41</td>
</tr>
<tr class="odd">
<td align="left">A</td>
<td align="right">49</td>
</tr>
<tr class="even">
<td align="left">D</td>
<td align="right">54</td>
</tr>
<tr class="odd">
<td align="left">E</td>
<td align="right">32</td>
</tr>
<tr class="even">
<td align="left">D</td>
<td align="right">38</td>
</tr>
</tbody>
</table>
<p>Finally, we’ll set <span class="math inline">\(\beta_{\text{age}}\)</span> such that there is a modest positive relationship between age and the probability of a positive response — older patients at any unit will be likelier than younger patients to be a promoter!</p>
<p>With all that wrapped up, we can finally simulate individual responses.</p>
<pre class="r"><code>beta_age &lt;- 0.35

# simulate individual patient responses 
set.seed(32)
responses &lt;- 
  patients %&gt;%
  left_join(unit_params) %&gt;%
  mutate(phi = beta + beta_age * ((age - 45)/10),
         q1 = cutpoints[1] - phi,
         q2 = cutpoints[2] - phi,
         detractor = expit(q1),
         passive = expit(q2) - expit(q1),
         promoter = 1 - expit(q2)) %&gt;%
  select(unit, age, promoter, passive, detractor) %&gt;%
  rowwise() %&gt;%
  mutate(response = sample(c(&quot;promoter&quot;, &quot;passive&quot;, &quot;detractor&quot;),
                           size = 1, 
                           prob = c(promoter, passive, detractor))) %&gt;%
  ungroup() %&gt;%
  select(unit, age, response)</code></pre>
<p>Here’s the distribution of responses for each unit — as expected, unit A has lots of promoters while units D and E have the highest proportion of detractors, and unit D has the most responses while unit E has the fewest.</p>
<pre class="r"><code>egypt_blu &lt;- MetBrewer::MetPalettes$Egypt[[1]][2]
egypt_red &lt;- MetBrewer::MetPalettes$Egypt[[1]][1]
egypt_grn &lt;- MetBrewer::MetPalettes$Egypt[[1]][3]

responses %&gt;%
  mutate(response = fct_relevel(response, c(&quot;detractor&quot;, &quot;promoter&quot;, &quot;passive&quot;))) %&gt;%
  ggplot(aes(x = age,
             fill = response)) + 
  geom_histogram(position = &quot;identity&quot;,
                 alpha = 0.5) + 
  facet_wrap(~unit, scales = &quot;free_y&quot;) +
  MetBrewer::scale_fill_met_d(&quot;Egypt&quot;) +
  theme(legend.position = &quot;none&quot;) +
  labs(title = glue::glue(&quot;Simulated **{color_text(&#39;promoters&#39;, egypt_blu)}**, 
                          **{color_text(&#39;passives&#39;, egypt_grn)}**, and 
                          **{color_text(&#39;detractors&#39;, egypt_red)}**&quot;),
       subtitle = &quot;Distribution of patient responses by age at each unit&quot;,
       x = &quot;Patient age&quot;,
       y = NULL)</code></pre>
<p><img src="https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/index_files/figure-html/plot-response-distribution-1.png" width="4500" /></p>
<p>Here’s how each simulated unit scored for NPS:</p>
<pre class="r"><code>responses %&gt;%
  group_by(unit) %&gt;%
  count(response) %&gt;%
  pivot_wider(names_from = response,
              values_from = n,
              values_fill = 0) %&gt;%
  mutate(n = promoter + passive + detractor,
         nps = (promoter - detractor)/n,
         nps = scales::label_percent(accuracy = 1)(nps)) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">unit</th>
<th align="right">detractor</th>
<th align="right">passive</th>
<th align="right">promoter</th>
<th align="right">n</th>
<th align="left">nps</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">A</td>
<td align="right">8</td>
<td align="right">2</td>
<td align="right">59</td>
<td align="right">69</td>
<td align="left">74%</td>
</tr>
<tr class="even">
<td align="left">B</td>
<td align="right">27</td>
<td align="right">6</td>
<td align="right">78</td>
<td align="right">111</td>
<td align="left">46%</td>
</tr>
<tr class="odd">
<td align="left">C</td>
<td align="right">25</td>
<td align="right">7</td>
<td align="right">46</td>
<td align="right">78</td>
<td align="left">27%</td>
</tr>
<tr class="even">
<td align="left">D</td>
<td align="right">68</td>
<td align="right">21</td>
<td align="right">106</td>
<td align="right">195</td>
<td align="left">19%</td>
</tr>
<tr class="odd">
<td align="left">E</td>
<td align="right">23</td>
<td align="right">0</td>
<td align="right">24</td>
<td align="right">47</td>
<td align="left">2%</td>
</tr>
</tbody>
</table>
<p>Importantly, this differs from the expected outcome at each unit! For smaller sample sizes, each individual patient response has an outsized impact on NPS. Despite this, we should be able to recover the underlying parameters used to simulate the data with a model.</p>
</div>
<div id="model" class="section level2">
<h2>Model</h2>
<p>Remember the lengthy data-generating process nonsense from beforehand? As is, that’d be a bit of a mess to implement by hand. Luckily for us, however, McElreath’s <a href="https://github.com/rmcelreath/rethinking">{<code>rethinking</code>}</a> package contains a useful function, <code>dordlogit()</code>, that interfaces nicely with Stan’s <a href="https://mc-stan.org/docs/stan-users-guide/ordered-logistic.html">ordered logistic model</a>. This plunks some of the rote computational steps under the hood and leaves us with the most important bits: the linear model <span class="math inline">\(\phi\)</span> and the cutpoints <span class="math inline">\(\kappa\)</span>.</p>
<p><span class="math display">\[
\begin{gather}
R_i \sim \text{Ordered-logit}(\phi_i, \kappa_k) \\
\phi_i = \text{some linear model} \\
\text{~priors~}
\end{gather}
\]</span></p>
<p>Let’s build a series of increasingly complex models using this framework. I’m in a mood for raccoons, so the models are named appropriately:</p>
<ol style="list-style-type: decimal">
<li><code>raccoon_01</code>: a term-less model that just estimates the cutpoints.</li>
<li><code>raccoon_02</code>: a model with terms for age and unit.</li>
<li><code>raccoon_03</code>: a model with a term for age and a hierarchical term for unit.</li>
<li><code>raccoon_04</code>: a model with a term for age and a non-centered hierarchical term for unit.</li>
</ol>
<p>Before doing any of that, however, we’ll need to prep the data for Stan. Each unit and response category will get assigned a numeric ID and we’ll standardize patient ages across the population.</p>
<pre class="r"><code>responses &lt;- 
  responses %&gt;%
  left_join(tibble(unit = LETTERS[1:5],
                   unit_id = seq(1:5))) %&gt;%
  mutate(response_id = case_when(response == &quot;promoter&quot; ~ 3,
                                 response == &quot;passive&quot; ~ 2,
                                 response == &quot;detractor&quot; ~ 1),
         response_id = as.integer(response_id),
         age_std = (age - mean(age))/sd(age))

responses_stan &lt;- 
  responses %&gt;%
  select(response_id,
         unit_id,
         age_std) %&gt;%
  as.list()</code></pre>
<div id="raccoon-01" class="section level3">
<h3>Raccoon #01</h3>
<p>The first model doesn’t contain any terms and just estimates the cutpoints from the data. In McElreath’s words, this sort of model is little more than a Bayesian histogram of the data. To get started, we just need to provide a prior for the cutpoints <span class="math inline">\(\kappa_k\)</span>.</p>
<p><span class="math display">\[
\begin{gather}
R_i \sim \text{Ordered-logit}(\phi_i, \kappa_k) \\
\color{blue}{\phi_i = 0 \\
\kappa_k \sim \text{Normal}(0, 1)}
\end{gather}
\]</span></p>
<p>Modeling in Stan via <code>rethinking::ulam()</code> is essentially as basic as re-writing the mathematical model and supplying the data.</p>
<pre class="r"><code>raccoon_01 &lt;-
  ulam(
    alist(
      # model
      response_id ~ dordlogit(0, cutpoints),
      
      # priors
      cutpoints ~ dnorm(0, 1)
    ),
    
    data = responses_stan,
    chains = 4,
    cores = 4
  )</code></pre>
<p>This initial model doesn’t do a good job of recovering the manually set cutpoints:</p>
<pre class="r"><code>precis(raccoon_01, depth = 2)</code></pre>
<pre><code>##                    mean         sd       5.5%      94.5%    n_eff    Rhat4
## cutpoints[1] -0.8350663 0.09496125 -0.9825748 -0.6847435 1078.130 1.002124
## cutpoints[2] -0.5054729 0.08830633 -0.6494612 -0.3641978 1291.444 1.001888</code></pre>
<p>This is expected! This model doesn’t account for the variation by unit/age and instead lumps all the data together. As mentioned before, this really can be thought of as a Bayesian histogram — while it doesn’t recover the cutpoint parameters, <code>raccoon_01</code> matches the overall proportion of promoters, passives, and detractors really well.</p>
<pre class="r"><code>raccoon_01@stanfit %&gt;%
  
  # extract posterior draws &amp; convert to tibble
  posterior::as_draws_df() %&gt;%
  as_tibble() %&gt;%
  select(starts_with(&quot;cut&quot;)) %&gt;%
  rename_with(~str_remove_all(.x, &quot;[:punct:]&quot;)) %&gt;%
  
  # summarise each category w/50/80% quantiles
  mutate(detractor = expit(cutpoints1),
         passive = expit(cutpoints2) - expit(cutpoints1),
         promoter = 1 - expit(cutpoints2)) %&gt;%
  select(-starts_with(&quot;cut&quot;)) %&gt;%
  pivot_longer(cols = everything(),
               names_to = &quot;nps&quot;,
               values_to = &quot;estimate&quot;) %&gt;%
  group_by(nps) %&gt;%
  tidybayes::median_qi(estimate, .width = c(0.5, 0.8)) %&gt;%
  
  # plot alongside original data
  ggplot() + 
  geom_col(data = percent(responses, response),
           aes(x = response,
               y = pct),
           fill = egypt_red,
           alpha = 0.5,
           width = 0.5) +
  ggdist::geom_pointinterval(aes(x = nps,
                                 y = estimate,
                                 ymin = .lower,
                                 ymax = .upper),
                             color = egypt_blu) +
  scale_y_continuous(labels = scales::label_percent()) + 
  labs(title = &quot;**Raccoon #01** Posterior Fit&quot;,
       subtitle = glue::glue(&quot;Comparison of each category&#39;s 
                             **{color_text(&#39;true proportion&#39;, egypt_red)}** 
                             to the 
                             **{color_text(&#39;model\\&#39;s estimate&#39;, egypt_blu)}**&quot;),
       x = NULL,
       y = NULL,
       caption = &quot;Pointrange indicates 50/80% &lt;br&gt;posterior credible interval&quot;)</code></pre>
<p><img src="https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/index_files/figure-html/raccoon-01-post-fit-1.png" width="4500" /></p>
<p>This model isn’t terribly useful since we could have gotten the same inference from just plotting the data directly, but this serves as a base upon which we can build more complicated and useful models.</p>
</div>
<div id="raccoon-02" class="section level3">
<h3>Raccoon #02</h3>
<p>The second model is where things get a bit more interesting — now we’ll actually include predictors for <span class="math inline">\(\beta_{\text{unit}}\)</span> and <span class="math inline">\(\beta_{\text{age}}\)</span>.</p>
<p><span class="math display">\[
\begin{gather}
R_i \sim \text{Ordered-logit}(\phi_i, \kappa_k) \\
\phi_i = \color{blue}{\beta_{\text{unit}} + \beta_{\text{age}} \ \text{age}_i} \\
\kappa_k \sim \text{Normal}(0, 1) \\
\color{blue}{\beta_{\text{unit}} \sim \text{Normal}(0, 1) \\
\beta_{\text{age}} \sim \text{Normal}(0, 0.5)}
\end{gather}
\]</span></p>
<p>I’ve upped the number of samples for this particular model to avoid a warning from Stan.</p>
<pre class="r"><code>raccoon_02 &lt;- 
  ulam(
    alist(
      # model
      response_id ~ dordlogit(phi, cutpoints),
      phi &lt;- b[unit_id] + b_age * age_std,
      
      # priors
      cutpoints ~ dnorm(0, 1), 
      b[unit_id] ~ dnorm(0, 1),
      b_age ~ dnorm(0, 0.5)
    ),
    
    data = responses_stan,
    chains = 4,
    cores = 4,
    iter = 2000
  )</code></pre>
<p>This model does a pretty good job! Extracting the parameter estimates shows that all of the parameter values that we set manually fall within the 80% posterior credible range estimated by the model.</p>
<pre class="r"><code>raccoon_02@stanfit %&gt;%
  
  # extract parameter draws &amp; summarise with 50/80% quantiles
  posterior::as_draws_df() %&gt;%
  as_tibble() %&gt;%
  select(-c(lp__:.draw)) %&gt;%
  pivot_longer(cols = everything(),
               names_to = &quot;term&quot;,
               values_to = &quot;estimate&quot;) %&gt;%
  mutate(term = if_else(str_sub(term, 1, 2) == &quot;b[&quot;, 
                        LETTERS[as.integer(str_sub(term, 3, 3))], 
                        term)) %&gt;%
  group_by(term) %&gt;%
  tidybayes::median_qi(estimate, .width = c(0.5, 0.8)) %&gt;%
  
  # append with actual values used to simulate data
  left_join(unit_params, by = c(&quot;term&quot; = &quot;unit&quot;)) %&gt;%
  rename(true_value = beta) %&gt;%
  mutate(true_value = case_when(term == &quot;cutpoints[1]&quot; ~ cutpoints[1],
                                term == &quot;cutpoints[2]&quot; ~ cutpoints[2],
                                term == &quot;b_age&quot; ~ beta_age,
                                TRUE ~ true_value),
         term = fct_relevel(term, c(paste0(&quot;cutpoints[&quot;, 1:2, &quot;]&quot;),
                                    &quot;b_age&quot;,
                                    LETTERS[5:1]))) %&gt;%
  
  # plot!
  ggplot(aes(x = term,
             y = estimate,
             ymin = .lower,
             ymax = .upper)) + 
  ggdist::geom_pointinterval(color = egypt_blu) +
  geom_point(aes(y = true_value),
             color = egypt_red,
             size = 2.5) + 
  coord_flip() + 
  labs(title = &quot;**Raccoon #02** Posterior Fit&quot;,
       subtitle = glue::glue(&quot;Comparison of each parameter&#39;s 
                             **{color_text(&#39;true value&#39;, egypt_red)}** 
                             to the 
                             **{color_text(&#39;model\\&#39;s estimate&#39;, egypt_blu)}**&quot;),
       x = NULL,
       y = NULL,
       caption = &quot;Pointrange indicates 50/80% &lt;br&gt;posterior credible interval&quot;)</code></pre>
<p><img src="https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/index_files/figure-html/raccoon-02-params-1.png" width="4500" /></p>
<p>This model, however, could be improved. The model only uses categorical indicators for the unit, which causes two issues. Firstly, we can only make predictions for the few units that are in the dataset — this model would fail if we tried to make a prediction on a hypothetical new unit, unit F. Secondly, information about each unit is contained just to that unit. In this case, unit E has relatively few responses, and therefore can only draw inference from those responses. A <a href="https://www.thedatadiary.net/blog/2022-11-14-hierarchical-hospitals/">hierarchical model</a>, however, can help in both these areas.</p>
</div>
<div id="raccoon-03" class="section level3">
<h3>Raccoon #03</h3>
<p>To add a hierarchical term for the unit-level intercept, <span class="math inline">\(\beta_{\text{unit}}\)</span>, we don’t actually need to make any changes to the linear model, just how <span class="math inline">\(\beta_{\text{unit}}\)</span> is defined underneath. Rather than estimating each unit intercept directly, this new model will allow them to vary around a group mean, <span class="math inline">\(\overline{\beta}\)</span> with a standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p><span class="math display">\[
\begin{gather}
R_i \sim \text{Ordered-logit}(\phi_i, \kappa_k) \\
\phi_i = \beta_{\text{unit}} + \beta_{\text{age}} \ \text{age}_i \\
\kappa_k \sim \text{Normal}(0, 1) \\
\beta_{\text{unit}} \sim \text{Normal}(\color{blue}{\overline{\beta}}, \color{blue}{\sigma}) \\
\beta_{\text{age}} \sim \text{Normal}(0, 0.5) \\
\color{blue}{\overline{\beta} \sim \text{Normal}(0, 1) \\
\sigma \sim \text{Exponential}(1)}
\end{gather}
\]</span></p>
<p>This new definition means that we no longer set priors for <span class="math inline">\(\beta_{\text{unit}}\)</span> directly. Instead, our new terms are considered <em>hyper-priors</em> or <em>adaptive priors</em>.</p>
<pre class="r"><code>raccoon_03 &lt;-
  ulam(
    alist(
      # model
      response_id ~ dordlogit(phi, cutpoints),
      phi &lt;- b[unit_id] + b_age*age_std,
      
      # priors
      cutpoints ~ dnorm(0, 1),
      b[unit_id] ~ dnorm(b_bar, sigma),
      b_age ~ dnorm(0, 0.5),
      
      # hyper-priors
      b_bar ~ dnorm(0, 1),
      sigma ~ dexp(1)
    ),
    
    data = responses_stan,
    chains = 4,
    cores = 4
  )</code></pre>
<p>Similar to the previous model, all the true parameter values fall within the model’s 80% credible interval estimates. <em>And</em>, we’re now accounting for the group structure with a hierarchical model! If you look at Unit E, however, it looks like the model has gotten worse — the median parameter estimate here is further away from the true value than in the previous model. This, however, is actually what we want. Because there are so few responses for unit E, the estimates are shrunken towards the group mean. In the previous model, we were a bit too <em>over-indexed</em> on the responses we had — if this were real data, where we don’t inherently know the underlying parameter value, we’d want to be similarly cautious for a unit with few responses.</p>
<pre class="r"><code>raccoon_03@stanfit %&gt;%
  
  # extract posterior parameters and summarise with 50/80% quantiles
  posterior::as_draws_df() %&gt;%
  as_tibble() %&gt;%
  rename_with(~str_remove(.x, &quot;]&quot;)) %&gt;%
  rename_with(~str_replace(.x, &quot;\\[&quot;, &quot;_&quot;)) %&gt;%
  mutate(A = b_1,
         B = b_2,
         C = b_3,
         D = b_4,
         E = b_5) %&gt;%
  select(.draw, A, B, C, D, E, starts_with(&quot;cut&quot;), b_age) %&gt;%
  pivot_longer(cols = -.draw,
               names_to = &quot;term&quot;,
               values_to = &quot;estimate&quot;) %&gt;%
  group_by(term) %&gt;%
  tidybayes::median_qi(estimate, .width = c(0.5, 0.8)) %&gt;%
  
  # append with actual values used to simulate data
  mutate(term = if_else(str_detect(term, &quot;cutpoint&quot;), paste0(str_replace(term, &quot;_&quot;, &quot;\\[&quot;), &quot;]&quot;), term)) %&gt;%
  left_join(unit_params, by = c(&quot;term&quot; = &quot;unit&quot;)) %&gt;%
  rename(true_value = beta) %&gt;%
  mutate(true_value = case_when(term == &quot;cutpoints[1]&quot; ~ cutpoints[1],
                                term == &quot;cutpoints[2]&quot; ~ cutpoints[2],
                                term == &quot;b_age&quot; ~ beta_age,
                                TRUE ~ true_value),
         term = fct_relevel(term, c(paste0(&quot;cutpoints[&quot;, 1:2, &quot;]&quot;),
                                    &quot;b_age&quot;,
                                    LETTERS[5:1]))) %&gt;%
  
  # plot!
  ggplot(aes(x = term,
             y = estimate,
             ymin = .lower,
             ymax = .upper)) + 
  ggdist::geom_pointinterval(color = egypt_blu) +
  geom_point(aes(y = true_value),
             color = egypt_red,
             size = 2.5) + 
  coord_flip() + 
  labs(title = &quot;**Raccoon #03** Posterior Fit&quot;,
       subtitle = glue::glue(&quot;Comparison of each parameter&#39;s 
                             **{color_text(&#39;true value&#39;, egypt_red)}** 
                             to the 
                             **{color_text(&#39;model\\&#39;s estimate&#39;, egypt_blu)}**&quot;),
       x = NULL,
       y = NULL,
       caption = &quot;Pointrange indicates 50/80% &lt;br&gt;posterior credible interval&quot;)</code></pre>
<p><img src="https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/index_files/figure-html/raccoon-03-post-fit-1.png" width="4500" /></p>
<p>Despite all this hierarchical goodness, this model’s diagnostics could stand to be improved. Although Stan didn’t throw any errors, each parameter’s <a href="https://mc-stan.org/docs/reference-manual/effective-sample-size.html#effective-sample-size.section">effective sample size</a>, <code>n_eff</code>, is low relative to the number of actual samples drawn (in this case, we used the default of 500 samples per chain for a total of 2000 samples) and the <a href="https://mc-stan.org/docs/reference-manual/notation-for-samples-chains-and-draws.html#potential-scale-reduction">convergence statistic</a>, <code>Rhat4</code>, is often a hair or two above the target value of <code>1.00</code>.</p>
<pre class="r"><code>precis(raccoon_03, depth = 2)</code></pre>
<pre><code>##                     mean         sd       5.5%     94.5%     n_eff    Rhat4
## cutpoints[1] -0.41376023 0.60190950 -1.4433861 0.5471729  373.8733 1.018584
## cutpoints[2] -0.06246023 0.60423520 -1.0766518 0.8993494  389.2378 1.018645
## b[1]          1.49374559 0.69139567  0.3692997 2.6091664  479.3414 1.014100
## b[2]          0.74360352 0.62753994 -0.2911371 1.7151677  408.1696 1.017661
## b[3]          0.33785619 0.63617237 -0.7061142 1.3145339  385.0501 1.016526
## b[4]          0.20898156 0.61467500 -0.8026615 1.1898645  366.1409 1.019565
## b[5]         -0.06365551 0.66665904 -1.1210397 0.9557759  431.5194 1.017986
## b_age         0.24898479 0.08989783  0.1096759 0.3935037 1137.4021 1.001321
## b_bar         0.48423691 0.65357409 -0.5809470 1.5103410  416.2923 1.011756
## sigma         0.78583383 0.39324244  0.3537180 1.5136274 1023.1633 1.002546</code></pre>
<p>This is not-so-much an issue with the model specification, but with the computation. Stan’s sampler has a bit of difficulty estimating the shape of the posterior for each <span class="math inline">\(\beta_{\text{unit}}\)</span> because they are dependent on <span class="math inline">\(\overline{\beta}\)</span> and <span class="math inline">\(\sigma\)</span>, which are estimated separately (<a href="https://benslack19.github.io/data%20science/statistics/devilsfunnel_cnc_param/">this post</a> provides a good visual of the “Devil’s Funnel” — a difficult shape to explore that can arise from this sort of model).</p>
<p>Once again, I am fortunate to not be the first person to encounter this issue, and there is a relatively standard approach that we can take to address. We can respecify the model using a <a href="https://mc-stan.org/docs/stan-users-guide/reparameterization.html#non-centered-parameterization">non-centered parameterization</a> for the <span class="math inline">\(\beta_{\text{unit}}\)</span> terms.</p>
</div>
<div id="raccoon-04" class="section level3">
<h3>Raccoon #04</h3>
<p>A non-centered parameterization is mathematically equivalent to it’s <a href="https://mc-stan.org/docs/stan-users-guide/reparameterization.html#centered-parameterization">centered</a> counterpart (which is what was used in the previous model), but makes it easier for Stan’s sampler to explore the parameter space. To convert to a non-centered parameterization, we need to respecify the model such that each parameter is sampled directly, rather than being dependent on another parameter.</p>
<p>In our case, we want each unit’s intercept to be offset from the global mean by some amount. We can think of this offset as being <span class="math inline">\(z\)</span> standard deviations away from the mean. Because the model is additive, we can simply replace the <span class="math inline">\(\beta_{\text{unit}}\)</span> term in the linear model with the mean <span class="math inline">\(\overline{\beta}\)</span> and unit offset <span class="math inline">\(z_{\text{unit}} \ \sigma\)</span>.</p>
<p><span class="math display">\[
\begin{gather}
R_i \sim \text{Ordered-logit}(\phi_i, \kappa_k) \\
\phi_i = \color{blue}{\underbrace{\overline{\beta} + z_{\text{unit}} \ \sigma}_{\beta_{\text{unit}} \ \text{replacement}}} + \beta_{\text{age}} \ \text{age}_i \\
\kappa_k \sim \text{Normal}(0, 1) \\
\beta_{\text{age}} \sim \text{Normal}(0, 0.5) \\
\overline{\beta} \sim \text{Normal}(0, 1) \\
\color{blue}{z_{\text{unit}} \sim \text{Normal}(0, 1) \\
\sigma \sim \text{Exponential}(1)}
\end{gather}
\]</span></p>
<p>Again, this is mathematically equivalent to the previous model, but the sampler will now complain less about exploring the parameter space since each term is sampled directly.</p>
<pre class="r"><code>raccoon_04 &lt;- 
  ulam(
    alist(
      # model 
      response_id ~ dordlogit(phi, cutpoints),
      phi &lt;- b_bar + z[unit_id]*sigma + b_age*age_std,
      
      # priors
      cutpoints ~ dnorm(0, 1),
      b_age ~ dnorm(0, 0.5),
      
      # non-centered parameters
      b_bar ~ dnorm(0, 1),
      z[unit_id] ~ dnorm(0, 1),
      sigma ~ dexp(1)
    ),
    
    data = responses_stan,
    chains = 4,
    cores = 4
  )</code></pre>
<p>We have to do a bit more work to pull out the unit estimates for this model but, as expected, the parameter estimates here are practically equivalent to the previous model’s estimates.</p>
<pre class="r"><code>raccoon_04@stanfit %&gt;%
  
  # extract parameters and summarise with 50/80% quantiles
  posterior::as_draws_df() %&gt;%
  as_tibble() %&gt;%
  rename_with(~str_replace(str_remove(.x, &quot;]&quot;), &quot;\\[&quot;, &quot;_&quot;),
              .cols = starts_with(&quot;z&quot;)) %&gt;%
  mutate(A = b_bar + z_1 * sigma,
         B = b_bar + z_2 * sigma,
         C = b_bar + z_3 * sigma,
         D = b_bar + z_4 * sigma,
         E = b_bar + z_5 * sigma) %&gt;%
  select(A, B, C, D, E, b_age, starts_with(&quot;cut&quot;)) %&gt;%
  pivot_longer(cols = everything(),
               names_to = &quot;term&quot;,
               values_to = &quot;estimate&quot;) %&gt;%
  group_by(term) %&gt;%
  tidybayes::median_qi(estimate, .width = c(0.5, 0.8)) %&gt;%
  
  # append with actual values used to simulate data
  left_join(unit_params, by = c(&quot;term&quot; = &quot;unit&quot;)) %&gt;%
  rename(true_value = beta) %&gt;%
  mutate(true_value = case_when(term == &quot;cutpoints[1]&quot; ~ cutpoints[1],
                                term == &quot;cutpoints[2]&quot; ~ cutpoints[2],
                                term == &quot;b_age&quot; ~ beta_age,
                                TRUE ~ true_value),
         term = fct_relevel(term, c(paste0(&quot;cutpoints[&quot;, 1:2, &quot;]&quot;),
                                    &quot;b_age&quot;,
                                    LETTERS[5:1]))) %&gt;%
  
  # plot!
  ggplot(aes(x = term,
             y = estimate,
             ymin = .lower,
             ymax = .upper)) + 
  ggdist::geom_pointinterval(color = egypt_blu) +
  geom_point(aes(y = true_value),
             color = egypt_red,
             size = 2.5) + 
  coord_flip() +
  labs(title = &quot;**Raccoon #04** Posterior Fit&quot;,
       subtitle = glue::glue(&quot;Comparison of each parameter&#39;s 
                             **{color_text(&#39;true value&#39;, egypt_red)}** 
                             to the 
                             **{color_text(&#39;model\\&#39;s estimate&#39;, egypt_blu)}**&quot;),
       x = NULL,
       y = NULL,
       caption = &quot;Pointrange indicates 50/80% &lt;br&gt;posterior credible interval&quot;)</code></pre>
<p><img src="https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/index_files/figure-html/raccoon-04-post-fit-1.png" width="4500" /></p>
<p>With this new parameterization, however, this inference stands on a bit sturdier ground. While still not near the actual 2000 total samples, the effective sample size of each parameter has greatly improved and the convergence statistic is better across the board.</p>
<pre class="r"><code>precis(raccoon_04, depth = 2)</code></pre>
<pre><code>##                    mean         sd       5.5%     94.5%     n_eff     Rhat4
## cutpoints[1] -0.4528105 0.61734428 -1.4141411 0.5646185 1080.2276 0.9999476
## cutpoints[2] -0.1034459 0.61438529 -1.0705393 0.8963293 1085.7692 0.9997501
## b_age         0.2540631 0.09263271  0.1073939 0.4032082 1969.5320 0.9993506
## b_bar         0.4308891 0.63042797 -0.5693130 1.4471711  925.0096 1.0018934
## z[1]          1.4513916 0.65032581  0.4481160 2.5227073  862.1820 1.0065399
## z[2]          0.3795392 0.54168883 -0.4556955 1.2506734  681.4293 1.0058323
## z[3]         -0.2313011 0.56399174 -1.2076518 0.6440465  619.6718 1.0043170
## z[4]         -0.4386073 0.53846049 -1.3433651 0.3940326  592.6804 1.0029844
## z[5]         -0.7838474 0.62014640 -1.8038760 0.1606955  815.9599 1.0022972
## sigma         0.7803627 0.37862771  0.3673002 1.4990210  683.2843 1.0042488</code></pre>
</div>
</div>
<div id="exploring-the-posterior" class="section level2">
<h2>Exploring the posterior</h2>
<p>With this finalized model, we can answer interesting <a href="https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/multivariate-linear-models.html#plotting-multivariate-posteriors.">counterfactual</a> questions even if there isn’t data directly in the dataset. For example, what do we expect each unit’s score to be across <em>all ages?</em></p>
<pre class="r"><code># sequence of standardized ages for each unit
counterfactual_data &lt;- 
  crossing(unit_id = 1:5,
           age_std = seq(-2, 2, length.out = 50))

# extract a sample of 100 cutpoints from the posterior 
cutpoints &lt;- 
  extract.samples(
    raccoon_04,
    n = 100,
    pars = paste0(&quot;cutpoints[&quot;, 1:2, &quot;]&quot;)
  )

# convert to tibble &amp; add sim index
cutpoints &lt;- 
  tibble(
    sim = seq(1:100),
    cutpoint1 = cutpoints$`cutpoints[1]`,
    cutpoint2 = cutpoints$`cutpoints[2]`
  )
  
counterfactual_output &lt;- 
  
  # extract phi &amp; convert to wide tibble format
  raccoon_04 %&gt;%
  link(as.list(counterfactual_data),
       post = extract.samples(., n = 100)) %&gt;%
  t() %&gt;%
  as_tibble() %&gt;%
  
  # add unit/age data; convert to long format
  bind_cols(counterfactual_data, .) %&gt;%
  rowid_to_column() %&gt;%
  pivot_longer(starts_with(&quot;V&quot;),
               names_to = &quot;sim&quot;,
               values_to = &quot;phi&quot;) %&gt;%
  mutate(sim = as.numeric(str_remove(sim, &quot;V&quot;))) %&gt;%
  
  # estimate probabilities for each category
  left_join(cutpoints) %&gt;%
  mutate(q1 = cutpoint1 - phi,
         q2 = cutpoint2 - phi,
         detractor = expit(q1),
         passive = expit(q2) - expit(q1),
         promoter = 1 - expit(q2)) %&gt;%
  select(unit_id, age_std, sim, promoter, passive, detractor)

counterfactual_output %&gt;%
  slice_head(n = 10) %&gt;%
  mutate(across(c(promoter:detractor), ~scales::label_percent(accuracy = 1)(.x))) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">unit_id</th>
<th align="right">age_std</th>
<th align="right">sim</th>
<th align="left">promoter</th>
<th align="left">passive</th>
<th align="left">detractor</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">-2</td>
<td align="right">1</td>
<td align="left">76%</td>
<td align="left">6%</td>
<td align="left">19%</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">-2</td>
<td align="right">2</td>
<td align="left">78%</td>
<td align="left">5%</td>
<td align="left">17%</td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">-2</td>
<td align="right">3</td>
<td align="left">81%</td>
<td align="left">5%</td>
<td align="left">14%</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">-2</td>
<td align="right">4</td>
<td align="left">72%</td>
<td align="left">5%</td>
<td align="left">23%</td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">-2</td>
<td align="right">5</td>
<td align="left">80%</td>
<td align="left">4%</td>
<td align="left">15%</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">-2</td>
<td align="right">6</td>
<td align="left">74%</td>
<td align="left">6%</td>
<td align="left">20%</td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">-2</td>
<td align="right">7</td>
<td align="left">79%</td>
<td align="left">5%</td>
<td align="left">16%</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">-2</td>
<td align="right">8</td>
<td align="left">64%</td>
<td align="left">8%</td>
<td align="left">28%</td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">-2</td>
<td align="right">9</td>
<td align="left">79%</td>
<td align="left">4%</td>
<td align="left">17%</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">-2</td>
<td align="right">10</td>
<td align="left">68%</td>
<td align="left">8%</td>
<td align="left">24%</td>
</tr>
</tbody>
</table>
<p>Here, we’ve taken 100 samples from <code>raccoon_04</code> for each combination of <code>unit_id</code> and <code>age_std</code> and extracted the probability of selecting promoter, passive, or detractor. If we put together in a plot, we can see what the model expects of each unit across each age and how confident hte model is in that expectation.</p>
<pre class="r"><code>counterfactual_output %&gt;%
  pivot_longer(c(promoter, passive, detractor),
               names_to = &quot;nps_group&quot;,
               values_to = &quot;prob&quot;) %&gt;%
  mutate(nps_group = fct_relevel(nps_group, c(&quot;detractor&quot;, &quot;promoter&quot;, &quot;passive&quot;)),
         unit = paste(&quot;Unit&quot;, LETTERS[unit_id]),
         age = age_std * 10 + 45) %&gt;%
  ggplot(aes(x = age,
             y = prob,
             color = nps_group,
             group = paste0(sim, nps_group))) +
  geom_line(alpha = 0.2) +
  facet_wrap(~unit) +
  scale_y_continuous(labels = scales::label_percent()) + 
  MetBrewer::scale_color_met_d(&quot;Egypt&quot;) +
  theme(legend.position = &quot;none&quot;) + 
  labs(title = &quot;Counterfactual odds and oddities&quot;,
       subtitle = glue::glue(&quot;Probability of selecting 
                             **{color_text(&#39;promoter&#39;, egypt_blu)}**, 
                             **{color_text(&#39;passive&#39;, egypt_grn)}**, or 
                             **{color_text(&#39;detractor&#39;, egypt_red)}** 
                             as age increases&quot;),
       x = NULL,
       y = NULL,
       caption = &quot;Sample of 100 posterior draws&quot;)</code></pre>
<p><img src="https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/index_files/figure-html/plot-counterfactual-1.png" width="4500" /></p>
<p>There are quite a few items to note from this plot. Firstly, across all age ranges, the early-alphabet units are more likely to have promoter responses than the late-alphabet units. Additionally, there is a relatively small chance of selecting passive at each unit across the ages. As age increases, each unit is expected to receive more favorable scores. Finally, Unit D, which had the most responses, has the tightest posterior intervals while Unit E, which had the fewest, has the widest intervals. All of this is expected, given how the data was simulated. In combination with the true/estimated parameter plot, this serves as a good confirmation that <code>raccoon_04</code> models the process appropriately.</p>
<p>Since each sample gives the probability of selecting promoter, passive, or detractor, we can simply plug these probabilities into the formula for NPS to get the <em>expected</em> NPS for each unit across the ages.</p>
<pre class="r"><code>counterfactual_output %&gt;%
  
  # get nps for each posterior sample
  mutate(nps = promoter - detractor,
         unit = paste(&quot;Unit&quot;, LETTERS[unit_id]),
         age = age_std * 10 + 45) %&gt;%
  
  # plot!
  ggplot(aes(x = age,
             y = nps,
             group = sim)) + 
  geom_line(alpha = 0.25,
            color = RColorBrewer::brewer.pal(3, &quot;Dark2&quot;)[3]) +
  facet_wrap(~unit) + 
  scale_y_continuous(labels = scales::label_percent()) +
  labs(title = &quot;Counterfactual odds and oddities 2: NPS drift&quot;,
       subtitle = glue::glue(&quot;**{color_text(&#39;Expected NPS&#39;, RColorBrewer::brewer.pal(3, &#39;Dark2&#39;)[3])}** 
                             at each unit as age increases&quot;),
       x = NULL,
       y = NULL,
       caption = &quot;Sample of 100 posterior draws&quot;)</code></pre>
<p><img src="https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/index_files/figure-html/counterfactual-nps-1.png" width="4500" /></p>
</div>
<div id="in-summary" class="section level2">
<h2>In summary…</h2>
<p>In this post, we’ve done the following:</p>
<ul>
<li>Defined a data-generating process that links NPS to a linear model while preserving the order of the categories.</li>
<li>Manually set the parameters of the linear model and simulated patient responses.</li>
<li>Recovered the parameters with a series of models.</li>
</ul>
<p>The models that were built increased in both complexity and utility:</p>
<ul>
<li><code>raccoon_01</code> didn’t contain any terms and only estimated the cutpoints <span class="math inline">\(\kappa_k\)</span> — this effectively gave us a Bayesian histogram of the data.</li>
<li><code>raccoon_02</code> added terms for the unit each patient visited and their age. This recovered the parameters we set manually, but didn’t pool any information across units and only allowed us to draw inferences from the units in the dataset.</li>
<li><code>raccoon_03</code> converted <span class="math inline">\(\beta_{\text{unit}}\)</span> to a hierarchical term, which addressed some of the shortcomings of <code>raccoon_02</code>. However, the way the model was written resulted in a difficult parameter space for Stan’s sampler to explore, which gave less-than-desirable diagnostics.</li>
<li><code>raccoon_04</code> was a non-centered reparameterization of <code>raccoon_03</code>. The two models were mathematically equivalent, but <code>raccoon_04</code> was easier to for Stan to sample from, which gave us a larger effective sample size and smaller convergence statistic for each parameter (which are both good things!).</li>
</ul>
<p>With the fit from <code>raccoon_04</code>, we also were able to look at how the model expected responses to vary with age at each unit. This served as another visual confirmation that the model was doing what we expected based on how the data was simulated. These expected responses also allowed us to plot the expected NPS score at each unit as age varies.</p>
</div>
<div id="some-additional-closing-thoughts" class="section level2">
<h2>Some (additional) closing thoughts</h2>
<p>As mentioned in the opening section, NPS is a difficult metric to model (or, at least, it was prior to picking up McElreath’s book). In the past, I’d used some hack-ish methods to model NPS, such as:</p>
<ul>
<li>Ignoring passives and detractors by modeling promoters with a binomial.</li>
<li>Aggregating scores at the unit-level, rescaling NPS from <code>[-100, 100]</code> to <code>[0, 1]</code>, tossing out any zeroes or ones, then modeling with a beta distribution.</li>
<li>Separately modeling promoters, passives, and detractors with three binomial models.</li>
</ul>
<p>Each of these is wrong in their own way, but, fundamentally, they all ignore the data-generating process of a patient’s experience influencing an ordered response on a 0-10 scale.</p>
<p>Speaking of a 0-10 scale, this ordinal model extends to any number of categories — we could have have directly modeled the 11 response categories using <span class="math inline">\(k = 10\)</span> cutpoints. While NPS is more granular (and therefore, in my opinion, better) than LTR/topbox, modeling and evaluating the mean response on the 0-10 scale is <em>even more granular/better</em> than NPS! That being said, however, it’s unlikely that much of my work at the hospital will incorporate that more-granular view. NPS is our chosen metric, so while there are more potential categories, we really only care about the three big buckets of promoter, passive, or detractor. Additionally, from a benchmarking perspective, NPS is more widely available and allows for a quick comparison to <a href="https://www.qualtrics.com/news/leading-healthcare-organizations-choose-qualtrics-to-deliver-ease-and-empathy-to-patients-and-caregivers/">other hospital systems that are picking up the metric</a> or even <a href="https://success.qualtrics.com/rs/542-FMF-412/images/Qualtrics_CX_Industry_NPS_Benchmarks_eBook.pdf">other industries</a> where NPS is the standard satisfaction metric.</p>
</div>

    </div>

    



    
      








  





  
  
  
    
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hu566f88036a32ad6519e34d2b60700d41_471828_250x250_fill_q90_lanczos_center.jpg" itemprop="image" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="https://www.thedatadiary.net/">Mark Rieke</a></h5>
      <h6 class="card-subtitle">Senior CX Analyst</h6>
      <p class="card-text" itemprop="description">I&rsquo;m a mechanical engineer by education, data analyst by practice. I love machine learning and communicating complex topics clearly with simple and beautiful charts.</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://twitter.com/markjrieke" target="_blank" rel="noopener">
              <i class="fab fa-twitter"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/markjrieke" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://stackoverflow.com/users/16307782/mark-rieke" target="_blank" rel="noopener">
              <i class="fab fa-stack-overflow"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://www.linkedin.com/in/mark-rieke-ab4b0ab4/" target="_blank" rel="noopener">
              <i class="fab fa-linkedin"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
    

    

    


  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academia.min.e40e230d9b3dfeac86994156b6388764.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  <div class="container">
    <div class="row align-items-center">
      <div class="col-md-6 mb-4 mb-md-0">
        
        <p class="mb-0">
          © Mark Rieke 2022 &middot; 
          Powered by
          <a href="https://gethugothemes.com" target="_blank" rel="noopener">Gethugothemes</a>
        </p>
      </div>
      <div class="col-md-6">
        <ul class="list-inline network-icon text-right mb-0">
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://twitter.com/markjrieke" target="_blank" rel="noopener" title="Follow me on twitter!"><i class="fab fa-twitter" aria-hidden="true"></i></a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://github.com/markjrieke" target="_blank" rel="noopener" title="Find my work on GitHub!"><i class="fab fa-github" aria-hidden="true"></i></a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://stackoverflow.com/users/16307782/mark-rieke" target="_blank" rel="noopener" title="Help me out on Stack Overflow!"><i class="fab fa-stack-overflow" aria-hidden="true"></i></a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://www.linkedin.com/in/mark-rieke-ab4b0ab4/" target="_blank" rel="noopener" title="Connect with me on LinkedIn!"><i class="fab fa-linkedin" aria-hidden="true"></i></a>
          </li>
          
        </ul>
      </div>
    </div>
  </div>
</footer>
  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
