<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on the data diary</title>
    <link>https://www.thedatadiary.net/blog/</link>
    <description>Recent content in Blog on the data diary</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Mark Rieke {year}</copyright>
    <lastBuildDate>Sat, 20 Aug 2022 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://www.thedatadiary.net/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Finding new wedding bops with {tidyclust} and {spotifyr}</title>
      <link>https://www.thedatadiary.net/blog/2022-08-20-finding-new-wedding-bops-with-tidyclust-and-spotifyr/</link>
      <pubDate>Sat, 20 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-08-20-finding-new-wedding-bops-with-tidyclust-and-spotifyr/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-08-20-finding-new-wedding-bops-with-tidyclust-and-spotifyr/index_files/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-08-20-finding-new-wedding-bops-with-tidyclust-and-spotifyr/index_files/d3-bundle/d3-bundle.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-08-20-finding-new-wedding-bops-with-tidyclust-and-spotifyr/index_files/d3-lasso/d3-lasso.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-08-20-finding-new-wedding-bops-with-tidyclust-and-spotifyr/index_files/save-svg-as-png/save-svg-as-png.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://www.thedatadiary.net/blog/2022-08-20-finding-new-wedding-bops-with-tidyclust-and-spotifyr/index_files/ggiraphjs/ggiraphjs.min.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-08-20-finding-new-wedding-bops-with-tidyclust-and-spotifyr/index_files/ggiraphjs/ggiraphjs.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-08-20-finding-new-wedding-bops-with-tidyclust-and-spotifyr/index_files/girafe-binding/girafe.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Last November, I (finally) popped the big question and &lt;a href=&#34;https://www.instagram.com/p/Cc6ReimO2tH/?igshid=YmMyMTA2M2Y=&#34;&gt;proposed&lt;/a&gt;! Since then, my fiance and I have been diligently planning our wedding. While we have most of the big-ticket items checked off (venue, catering, photography, etc.), one area we still have more work to do is on the wedding playlist. We’ve &lt;a href=&#34;https://open.spotify.com/playlist/66saUDfW5ggYD6JDAHbxyV&#34;&gt;started putting together a playlist on spotify&lt;/a&gt;, but it feels like it’s come to a bit of a stand-still. Currently, there’s a mix of zesty bops and tame songs on the playlist (we need to accommodate both our college friends and our grandparents!), but spotify’s track recommender only wants to suggest tamer songs right now. Our goal is to have a full dance floor the entire night — to achieve this, we can use &lt;a href=&#34;https://www.rcharlie.com/spotifyr/index.html&#34;&gt;spotifyr&lt;/a&gt; and the new &lt;a href=&#34;https://emilhvitfeldt.github.io/tidyclust/index.html&#34;&gt;tidyclust&lt;/a&gt; package to pull in the current playlist, cluster the songs based on their features, and find new songs based on the bop cluster.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels)
library(tidyclust)
library(spotifyr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you’d like to follow along, I’d recommend installing the development versions of &lt;a href=&#34;https://github.com/tidymodels/parsnip&#34;&gt;parsnip&lt;/a&gt; and &lt;a href=&#34;https://github.com/tidymodels/workflows&#34;&gt;workflows&lt;/a&gt;, as some of the functionality that interacts with tidyclust &lt;a href=&#34;https://emilhvitfeldt.github.io/tidyclust/articles/k_means.html#setup&#34;&gt;isn’t yet on CRAN&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;pulling-in-the-playlist&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pulling in the playlist&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.rcharlie.com/spotifyr/index.html&#34;&gt;spotifyr&lt;/a&gt; is an R interface to spotify’s web API and gives access to a host of track features (you can follow &lt;a href=&#34;https://msmith7161.github.io/what-is-speechiness/&#34;&gt;this tutorial&lt;/a&gt; to get it setup). I’ll use the functions &lt;code&gt;get_user_playlists()&lt;/code&gt; and &lt;code&gt;get_playlist_tracks()&lt;/code&gt; to pull in songs that are currently on our wedding playlist (appropriately named “Ding dong”).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get the songs that are currently on the wedding playlist
ding_dong &amp;lt;- 
  get_user_playlists(&amp;quot;12130039175&amp;quot;) %&amp;gt;%
  filter(name == &amp;quot;Ding dong&amp;quot;) %&amp;gt;%
  pull(id) %&amp;gt;%
  get_playlist_tracks() %&amp;gt;% 
  as_tibble() %&amp;gt;%
  select(track.id, track.name, track.popularity) %&amp;gt;%
  rename_with(~stringr::str_replace(.x, &amp;quot;\\.&amp;quot;, &amp;quot;_&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;31%&#34; /&gt;
&lt;col width=&#34;45%&#34; /&gt;
&lt;col width=&#34;23%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;track_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;track_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;track_popularity&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5jkFvD4UJrmdoezzT1FRoP&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Rasputin&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;65&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1D066zixBwqFYqBhKgdPzp&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Fergalicious&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;71&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;12jjuxN1gxlm29cqL5M6MW&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;I Got You&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;65&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2grjqo0Frpf2okIBiifQKs&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;September&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;81&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2RlgNHKcydI9sayD2Df2xp&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Mr. Blue Sky&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;80&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6x4tKaOzfNJpEJHySoiJcs&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Mambo No. 5 (a Little Bit of…)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;77&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3n3Ppam7vgaVa1iaRUc9Lp&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Mr. Brightside&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;66&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;7Cp69rNBwU0gaFT8zxExlE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Ymca&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3Gf5nttwcX9aaSQXRWidEZ&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Ride Wit Me&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;76&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3wMUvT6eIw2L5cZFG1yH9j&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Country Grammar (Hot Shit)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;70&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Spotify estimates quite a few &lt;a href=&#34;https://developer.spotify.com/documentation/web-api/reference/#/operations/get-several-audio-features&#34;&gt;features&lt;/a&gt; for each song in their catalog: speechiness (the presence of words on a track), acousticness (whether or not a song includes acoustic instruments), liveness (estimates whether or not the track is live or studio-recorded), etc. We can use &lt;code&gt;get_track_audio_features()&lt;/code&gt; to get the features for each song based on its &lt;code&gt;track_id&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# pull in track features of songs on the playlist
track_features &amp;lt;- 
  ding_dong %&amp;gt;%
  pull(track_id) %&amp;gt;%
  get_track_audio_features()

# join together
ding_dong &amp;lt;- 
  ding_dong %&amp;gt;%
  left_join(track_features, by = c(&amp;quot;track_id&amp;quot; = &amp;quot;id&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In my case, I’m interested in the energy and valence (positivity) of each song, so I’ll select these variables to use in the cluster analysis.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;track_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;valence&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;energy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Rasputin&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.966&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.893&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fergalicious&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.829&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.583&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;I Got You&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.544&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.399&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;September&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.979&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.832&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mr. Blue Sky&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.478&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.338&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mambo No. 5 (a Little Bit of…)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.892&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.807&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mr. Brightside&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.240&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.918&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Ymca&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.671&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.951&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Ride Wit Me&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.722&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.700&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Country Grammar (Hot Shit)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.565&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.664&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;clustering-with-tidyclust&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Clustering with &lt;code&gt;tidyclust&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Currently, the playlist covers a wide spectrum of of songs. For new songs on the playlist, I’m really just interested in songs similar to others in the top right corner of the below chart with high energy and valence.&lt;/p&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:4500px;height:3000px;&#34; class=&#34;girafe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;html&#34;:&#34;&lt;?xml version=\&#34;1.0\&#34; encoding=\&#34;UTF-8\&#34;?&gt;\n&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; xmlns:xlink=&#39;http://www.w3.org/1999/xlink&#39; id=&#39;svg_0c0f112f-51ff-4c6d-a75a-6f0602a30fcd&#39; viewBox=&#39;0 0 432 360&#39;&gt;\n &lt;defs&gt;\n  &lt;clipPath id=&#39;svg_0c0f112f-51ff-4c6d-a75a-6f0602a30fcd_c1&#39;&gt;\n   &lt;rect x=&#39;0&#39; y=&#39;0&#39; width=&#39;432&#39; height=&#39;360&#39;/&gt;\n  &lt;\/clipPath&gt;\n  &lt;clipPath id=&#39;svg_0c0f112f-51ff-4c6d-a75a-6f0602a30fcd_c2&#39;&gt;\n   &lt;rect x=&#39;55.03&#39; y=&#39;49.4&#39; width=&#39;370&#39; height=&#39;270.57&#39;/&gt;\n  &lt;\/clipPath&gt;\n &lt;\/defs&gt;\n &lt;g&gt;\n  &lt;g clip-path=&#39;url(#svg_0c0f112f-51ff-4c6d-a75a-6f0602a30fcd_c1)&#39;&gt;\n   &lt;rect x=&#39;0&#39; y=&#39;0&#39; width=&#39;432&#39; height=&#39;360&#39; fill=&#39;#FFFFFF&#39; stroke=&#39;#FFFFFF&#39; stroke-width=&#39;0.75&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39;/&gt;\n   &lt;rect x=&#39;0&#39; y=&#39;0&#39; width=&#39;432&#39; height=&#39;360&#39; fill=&#39;#FFFFFF&#39; stroke=&#39;#FFFFFF&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39;/&gt;\n  &lt;\/g&gt;\n  &lt;g clip-path=&#39;url(#svg_0c0f112f-51ff-4c6d-a75a-6f0602a30fcd_c2)&#39;&gt;\n   &lt;polyline points=&#39;55.03,245.02 425.03,245.02&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,167.67 425.03,167.67&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,90.32 425.03,90.32&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;155.20,319.97 155.20,49.40&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;259.92,319.97 259.92,49.40&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;364.64,319.97 364.64,49.40&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,283.69 425.03,283.69&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,206.34 425.03,206.34&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,128.99 425.03,128.99&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,51.64 425.03,51.64&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;102.84,319.97 102.84,49.40&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;207.56,319.97 207.56,49.40&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;312.28,319.97 312.28,49.40&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;417.00,319.97 417.00,49.40&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;circle cx=&#39;402.76&#39; cy=&#39;93.02&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Rasputin&#39;/&gt;\n   &lt;circle cx=&#39;345.38&#39; cy=&#39;212.92&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Fergalicious&#39;/&gt;\n   &lt;circle cx=&#39;225.99&#39; cy=&#39;284.08&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;I Got You&#39;/&gt;\n   &lt;circle cx=&#39;408.21&#39; cy=&#39;116.62&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;September&#39;/&gt;\n   &lt;circle cx=&#39;198.35&#39; cy=&#39;307.67&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Mr. Blue Sky&#39;/&gt;\n   &lt;circle cx=&#39;371.77&#39; cy=&#39;126.28&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Mambo No. 5 (a Little Bit of...)&#39;/&gt;\n   &lt;circle cx=&#39;98.65&#39; cy=&#39;83.35&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Mr. Brightside&#39;/&gt;\n   &lt;circle cx=&#39;279.19&#39; cy=&#39;70.59&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Ymca&#39;/&gt;\n   &lt;circle cx=&#39;300.56&#39; cy=&#39;167.67&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Ride Wit Me&#39;/&gt;\n   &lt;circle cx=&#39;234.79&#39; cy=&#39;181.59&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Country Grammar (Hot Shit)&#39;/&gt;\n   &lt;circle cx=&#39;172.38&#39; cy=&#39;103.47&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Shout, Pts. 1 &amp;amp;amp; 2&#39;/&gt;\n   &lt;circle cx=&#39;125.46&#39; cy=&#39;202.86&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Low (feat. T-Pain)&#39;/&gt;\n   &lt;circle cx=&#39;325.27&#39; cy=&#39;183.91&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Come On Eileen&#39;/&gt;\n   &lt;circle cx=&#39;259.92&#39; cy=&#39;178.5&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Gold Digger&#39;/&gt;\n   &lt;circle cx=&#39;330.71&#39; cy=&#39;76.01&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Fireball (feat. John Ryan)&#39;/&gt;\n   &lt;circle cx=&#39;312.7&#39; cy=&#39;134.41&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Bang Bang&#39;/&gt;\n   &lt;circle cx=&#39;352.92&#39; cy=&#39;81.81&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Can&amp;amp;#39;t Hold Us (feat. Ray Dalton)&#39;/&gt;\n   &lt;circle cx=&#39;249.87&#39; cy=&#39;103.85&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Don&amp;amp;#39;t Stop Me Now - Remastered 2011&#39;/&gt;\n   &lt;circle cx=&#39;365.06&#39; cy=&#39;89.54&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Take on Me&#39;/&gt;\n   &lt;circle cx=&#39;350.82&#39; cy=&#39;143.69&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Uncertain Smile&#39;/&gt;\n   &lt;circle cx=&#39;205.47&#39; cy=&#39;61.7&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Everytime We Touch&#39;/&gt;\n   &lt;circle cx=&#39;400.67&#39; cy=&#39;146.78&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;It&amp;amp;#39;s Tricky&#39;/&gt;\n   &lt;circle cx=&#39;272.49&#39; cy=&#39;211.76&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Brass Monkey&#39;/&gt;\n   &lt;circle cx=&#39;263.69&#39; cy=&#39;282.53&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Wagon Wheel&#39;/&gt;\n   &lt;circle cx=&#39;270.81&#39; cy=&#39;148.72&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Love On Top&#39;/&gt;\n   &lt;circle cx=&#39;391.87&#39; cy=&#39;158.77&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;I Think We&amp;amp;#39;re Alone Now&#39;/&gt;\n   &lt;circle cx=&#39;361.29&#39; cy=&#39;119.71&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;I Wanna Dance with Somebody (Who Loves Me)&#39;/&gt;\n   &lt;circle cx=&#39;386.84&#39; cy=&#39;202.86&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Uptown Funk (feat. Bruno Mars)&#39;/&gt;\n   &lt;circle cx=&#39;112.06&#39; cy=&#39;212.53&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Single Ladies (Put a Ring on It)&#39;/&gt;\n   &lt;circle cx=&#39;392.71&#39; cy=&#39;128.99&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Shake It Off&#39;/&gt;\n   &lt;circle cx=&#39;340.77&#39; cy=&#39;163.8&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Jump Around&#39;/&gt;\n   &lt;circle cx=&#39;315.63&#39; cy=&#39;119.71&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Hips Don&amp;amp;#39;t Lie (feat. Wyclef Jean)&#39;/&gt;\n   &lt;circle cx=&#39;241.91&#39; cy=&#39;142.91&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Body (feat. Brando)&#39;/&gt;\n   &lt;circle cx=&#39;285.89&#39; cy=&#39;115.84&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Jackie Chan&#39;/&gt;\n   &lt;circle cx=&#39;151.85&#39; cy=&#39;214.85&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Wake Up in the Sky&#39;/&gt;\n   &lt;circle cx=&#39;187.46&#39; cy=&#39;228&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Summer, Highland Falls - Live at the Bayou, Washington, D.C. - July 1980&#39;/&gt;\n   &lt;circle cx=&#39;297.2&#39; cy=&#39;108.11&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Magic Dance&#39;/&gt;\n   &lt;circle cx=&#39;315.22&#39; cy=&#39;95.73&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Dancing with Myself - 2001 Remaster&#39;/&gt;\n   &lt;circle cx=&#39;271.65&#39; cy=&#39;71.37&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Waterloo&#39;/&gt;\n   &lt;circle cx=&#39;231.02&#39; cy=&#39;90.7&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Potential Breakup Song&#39;/&gt;\n   &lt;circle cx=&#39;381.4&#39; cy=&#39;119.32&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Levitating (feat. DaBaby)&#39;/&gt;\n   &lt;circle cx=&#39;184.52&#39; cy=&#39;121.64&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Forever&#39;/&gt;\n   &lt;circle cx=&#39;344.54&#39; cy=&#39;135.95&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;White Walls (feat. ScHoolboy Q &amp;amp;amp; Hollis)&#39;/&gt;\n   &lt;circle cx=&#39;253.22&#39; cy=&#39;85.68&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Rose Tattoo&#39;/&gt;\n   &lt;circle cx=&#39;197.09&#39; cy=&#39;85.68&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Kiss Me I&amp;amp;#39;m #!@&amp;amp;#39;faced&#39;/&gt;\n   &lt;circle cx=&#39;162.32&#39; cy=&#39;137.89&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;All Night (feat. Knox Fortune)&#39;/&gt;\n   &lt;circle cx=&#39;282.54&#39; cy=&#39;131.7&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Don&amp;amp;#39;t Start Now&#39;/&gt;\n   &lt;circle cx=&#39;380.56&#39; cy=&#39;106.56&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;The Legend of Chavo Guerrero&#39;/&gt;\n   &lt;circle cx=&#39;398.57&#39; cy=&#39;168.83&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Bam Bam (feat. Ed Sheeran)&#39;/&gt;\n   &lt;circle cx=&#39;228.51&#39; cy=&#39;142.14&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;The Spins&#39;/&gt;\n   &lt;circle cx=&#39;167.77&#39; cy=&#39;188.16&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Hung Up&#39;/&gt;\n   &lt;circle cx=&#39;355.43&#39; cy=&#39;95.34&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;BREAK MY SOUL&#39;/&gt;\n   &lt;circle cx=&#39;270.81&#39; cy=&#39;190.1&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Post Malone (feat. RANI)&#39;/&gt;\n   &lt;circle cx=&#39;218.87&#39; cy=&#39;181.98&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Take Me Out&#39;/&gt;\n   &lt;circle cx=&#39;200.86&#39; cy=&#39;90.32&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Sk8er Boi&#39;/&gt;\n   &lt;circle cx=&#39;328.2&#39; cy=&#39;65.95&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Timber (feat. Ke$ha)&#39;/&gt;\n   &lt;circle cx=&#39;301.39&#39; cy=&#39;128.22&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Time of Our Lives&#39;/&gt;\n   &lt;circle cx=&#39;192.48&#39; cy=&#39;94.57&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Levels - Radio Edit&#39;/&gt;\n   &lt;circle cx=&#39;285.89&#39; cy=&#39;157.61&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Body On My&#39;/&gt;\n   &lt;circle cx=&#39;402.34&#39; cy=&#39;61.7&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Hey Ya!&#39;/&gt;\n   &lt;circle cx=&#39;297.2&#39; cy=&#39;114.68&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;TiK ToK&#39;/&gt;\n   &lt;circle cx=&#39;401.92&#39; cy=&#39;212.92&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;SexyBack (feat. Timbaland)&#39;/&gt;\n   &lt;circle cx=&#39;315.63&#39; cy=&#39;86.84&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Nice For What&#39;/&gt;\n   &lt;circle cx=&#39;341.19&#39; cy=&#39;130.93&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Up&#39;/&gt;\n   &lt;circle cx=&#39;71.84&#39; cy=&#39;136.34&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Clarity&#39;/&gt;\n   &lt;circle cx=&#39;286.31&#39; cy=&#39;181.59&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;good 4 u&#39;/&gt;\n   &lt;circle cx=&#39;373.44&#39; cy=&#39;147.17&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Cake By The Ocean&#39;/&gt;\n   &lt;circle cx=&#39;124.21&#39; cy=&#39;134.02&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Titanium (feat. Sia)&#39;/&gt;\n   &lt;circle cx=&#39;229.34&#39; cy=&#39;99.6&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Where Them Girls At (feat. Nicki Minaj &amp;amp;amp; Flo Rida)&#39;/&gt;\n   &lt;circle cx=&#39;330.3&#39; cy=&#39;155.68&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;I Know You Want Me (Calle Ocho)&#39;/&gt;\n   &lt;circle cx=&#39;196.25&#39; cy=&#39;102.31&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Club Can&amp;amp;#39;t Handle Me (feat. David Guetta)&#39;/&gt;\n   &lt;circle cx=&#39;343.28&#39; cy=&#39;88&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;I Love It (feat. Charli XCX)&#39;/&gt;\n   &lt;circle cx=&#39;390.61&#39; cy=&#39;170.76&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Treasure&#39;/&gt;\n   &lt;circle cx=&#39;357.52&#39; cy=&#39;70.98&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;She Bangs - English Version&#39;/&gt;\n   &lt;circle cx=&#39;327.78&#39; cy=&#39;77.94&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Magic (feat. Rivers Cuomo)&#39;/&gt;\n   &lt;circle cx=&#39;385.17&#39; cy=&#39;114.29&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Toxic&#39;/&gt;\n   &lt;circle cx=&#39;220.13&#39; cy=&#39;75.23&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Give Me Everything (feat. Ne-Yo, Afrojack &amp;amp;amp; Nayer)&#39;/&gt;\n   &lt;circle cx=&#39;342.44&#39; cy=&#39;206.34&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Temperature&#39;/&gt;\n  &lt;\/g&gt;\n  &lt;g clip-path=&#39;url(#svg_0c0f112f-51ff-4c6d-a75a-6f0602a30fcd_c1)&#39;&gt;\n   &lt;text x=&#39;27.91&#39; y=&#39;287.67&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;40%&lt;\/text&gt;\n   &lt;text x=&#39;28.14&#39; y=&#39;210.32&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;60%&lt;\/text&gt;\n   &lt;text x=&#39;28.22&#39; y=&#39;132.97&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;80%&lt;\/text&gt;\n   &lt;text x=&#39;23.41&#39; y=&#39;55.62&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;100%&lt;\/text&gt;\n   &lt;text x=&#39;92.81&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;25%&lt;\/text&gt;\n   &lt;text x=&#39;197.43&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;50%&lt;\/text&gt;\n   &lt;text x=&#39;302.26&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;75%&lt;\/text&gt;\n   &lt;text x=&#39;404.33&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;100%&lt;\/text&gt;\n   &lt;text x=&#39;214.33&#39; y=&#39;350.04&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;valence&lt;\/text&gt;\n   &lt;text transform=&#39;translate(16.93,207.45) rotate(-90.00)&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;energy&lt;\/text&gt;\n   &lt;text x=&#39;6.97&#39; y=&#39;39.44&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;Hover&lt;\/text&gt;\n   &lt;text x=&#39;50.24&#39; y=&#39;39.44&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;over&lt;\/text&gt;\n   &lt;text x=&#39;82.72&#39; y=&#39;39.44&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;each&lt;\/text&gt;\n   &lt;text x=&#39;117.45&#39; y=&#39;39.44&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;point&lt;\/text&gt;\n   &lt;text x=&#39;155.42&#39; y=&#39;39.44&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;to&lt;\/text&gt;\n   &lt;text x=&#39;171.57&#39; y=&#39;39.44&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;see&lt;\/text&gt;\n   &lt;text x=&#39;196.5&#39; y=&#39;39.44&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;the&lt;\/text&gt;\n   &lt;text x=&#39;221.19&#39; y=&#39;39.44&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;song&#39;s&lt;\/text&gt;\n   &lt;text x=&#39;266.55&#39; y=&#39;39.44&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;name!&lt;\/text&gt;\n   &lt;text x=&#39;6.97&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;The&lt;\/text&gt;\n   &lt;text x=&#39;42.15&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;current&lt;\/text&gt;\n   &lt;text x=&#39;104.99&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;wedding&lt;\/text&gt;\n   &lt;text x=&#39;178.64&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;playlist&lt;\/text&gt;\n  &lt;\/g&gt;\n &lt;\/g&gt;\n&lt;\/svg&gt;&#34;,&#34;js&#34;:null,&#34;uid&#34;:&#34;svg_0c0f112f-51ff-4c6d-a75a-6f0602a30fcd&#34;,&#34;ratio&#34;:1.2,&#34;settings&#34;:{&#34;tooltip&#34;:{&#34;css&#34;:&#34;.tooltip_SVGID_ { background-color:gray;color:white;padding:2px;border-radius:2px;font-family:Roboto Slab; ; position:absolute;pointer-events:none;z-index:999;}&#34;,&#34;placement&#34;:&#34;doc&#34;,&#34;offx&#34;:10,&#34;offy&#34;:0,&#34;use_cursor_pos&#34;:true,&#34;opacity&#34;:0.8,&#34;usefill&#34;:false,&#34;usestroke&#34;:false,&#34;delay&#34;:{&#34;over&#34;:200,&#34;out&#34;:500}},&#34;hover&#34;:{&#34;css&#34;:&#34;.hover_SVGID_ { fill:#1279BF;stroke:#1279BF;cursor:pointer; }&#34;,&#34;reactive&#34;:false},&#34;hoverkey&#34;:{&#34;css&#34;:&#34;.hover_key_SVGID_ { stroke:red; }&#34;,&#34;reactive&#34;:false},&#34;hovertheme&#34;:{&#34;css&#34;:&#34;.hover_theme_SVGID_ { fill:green; }&#34;,&#34;reactive&#34;:false},&#34;hoverinv&#34;:{&#34;css&#34;:&#34;&#34;},&#34;zoom&#34;:{&#34;min&#34;:1,&#34;max&#34;:1},&#34;capture&#34;:{&#34;css&#34;:&#34;.selected_SVGID_ { fill:red;stroke:gray; }&#34;,&#34;type&#34;:&#34;multiple&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;capturekey&#34;:{&#34;css&#34;:&#34;.selected_key_SVGID_ { stroke:gray; }&#34;,&#34;type&#34;:&#34;single&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;capturetheme&#34;:{&#34;css&#34;:&#34;.selected_theme_SVGID_ { stroke:gray; }&#34;,&#34;type&#34;:&#34;single&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;toolbar&#34;:{&#34;position&#34;:&#34;topright&#34;,&#34;saveaspng&#34;:true,&#34;pngname&#34;:&#34;diagram&#34;},&#34;sizing&#34;:{&#34;rescale&#34;:true,&#34;width&#34;:1}}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Broadly, there are three generic categories that the songs on the current playlist fall into: high energy and valence, low energy, or low valence (songs with low energy and valence will fall into one of the “low” categories). Rather than manually assign categories, we can use tidyclust to cluster the songs into three groups using the &lt;a href=&#34;https://en.wikipedia.org/wiki/K-means_clustering&#34;&gt;kmeans&lt;/a&gt; algorithm.&lt;/p&gt;
&lt;p&gt;There’s some &lt;a href=&#34;https://emilhvitfeldt.github.io/tidyclust/articles/k_means.html&#34;&gt;great documentation on the package site&lt;/a&gt;, but to get started, we’ll categorize the songs on the current playlist by “fitting” a kmeans model (using the &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/stats/html/00Index.html&#34;&gt;stats&lt;/a&gt; engine under the hood).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create a clustering obj
set.seed(918)
ding_dong_clusters &amp;lt;- 
  k_means(num_clusters = 3) %&amp;gt;%
  fit(~ valence + energy,
      data = ding_dong) &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:4500px;height:3000px;&#34; class=&#34;girafe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;html&#34;:&#34;&lt;?xml version=\&#34;1.0\&#34; encoding=\&#34;UTF-8\&#34;?&gt;\n&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; xmlns:xlink=&#39;http://www.w3.org/1999/xlink&#39; id=&#39;svg_601cff31-88a0-46b6-969f-6bcc8fa5e9b6&#39; viewBox=&#39;0 0 432 360&#39;&gt;\n &lt;defs&gt;\n  &lt;clipPath id=&#39;svg_601cff31-88a0-46b6-969f-6bcc8fa5e9b6_c1&#39;&gt;\n   &lt;rect x=&#39;0&#39; y=&#39;0&#39; width=&#39;432&#39; height=&#39;360&#39;/&gt;\n  &lt;\/clipPath&gt;\n  &lt;clipPath id=&#39;svg_601cff31-88a0-46b6-969f-6bcc8fa5e9b6_c2&#39;&gt;\n   &lt;rect x=&#39;55.03&#39; y=&#39;49.46&#39; width=&#39;370&#39; height=&#39;270.51&#39;/&gt;\n  &lt;\/clipPath&gt;\n &lt;\/defs&gt;\n &lt;g&gt;\n  &lt;g clip-path=&#39;url(#svg_601cff31-88a0-46b6-969f-6bcc8fa5e9b6_c1)&#39;&gt;\n   &lt;rect x=&#39;0&#39; y=&#39;0&#39; width=&#39;432&#39; height=&#39;360&#39; fill=&#39;#FFFFFF&#39; stroke=&#39;#FFFFFF&#39; stroke-width=&#39;0.75&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39;/&gt;\n   &lt;rect x=&#39;0&#39; y=&#39;0&#39; width=&#39;432&#39; height=&#39;360&#39; fill=&#39;#FFFFFF&#39; stroke=&#39;#FFFFFF&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39;/&gt;\n  &lt;\/g&gt;\n  &lt;g clip-path=&#39;url(#svg_601cff31-88a0-46b6-969f-6bcc8fa5e9b6_c2)&#39;&gt;\n   &lt;polyline points=&#39;55.03,245.04 425.03,245.04&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,167.70 425.03,167.70&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,90.37 425.03,90.37&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;155.20,319.97 155.20,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;259.92,319.97 259.92,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;364.64,319.97 364.64,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,283.70 425.03,283.70&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,206.37 425.03,206.37&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,129.04 425.03,129.04&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,51.70 425.03,51.70&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;102.84,319.97 102.84,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;207.56,319.97 207.56,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;312.28,319.97 312.28,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;417.00,319.97 417.00,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;circle cx=&#39;402.76&#39; cy=&#39;93.08&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Rasputin&#39;/&gt;\n   &lt;circle cx=&#39;345.38&#39; cy=&#39;212.94&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Fergalicious&#39;/&gt;\n   &lt;circle cx=&#39;225.99&#39; cy=&#39;284.09&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;I Got You&#39;/&gt;\n   &lt;circle cx=&#39;408.21&#39; cy=&#39;116.66&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;September&#39;/&gt;\n   &lt;circle cx=&#39;198.35&#39; cy=&#39;307.67&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Mr. Blue Sky&#39;/&gt;\n   &lt;circle cx=&#39;371.77&#39; cy=&#39;126.33&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Mambo No. 5 (a Little Bit of...)&#39;/&gt;\n   &lt;circle cx=&#39;98.65&#39; cy=&#39;83.41&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Mr. Brightside&#39;/&gt;\n   &lt;circle cx=&#39;279.19&#39; cy=&#39;70.65&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Ymca&#39;/&gt;\n   &lt;circle cx=&#39;300.56&#39; cy=&#39;167.7&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Ride Wit Me&#39;/&gt;\n   &lt;circle cx=&#39;234.79&#39; cy=&#39;181.62&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Country Grammar (Hot Shit)&#39;/&gt;\n   &lt;circle cx=&#39;172.38&#39; cy=&#39;103.52&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Shout, Pts. 1 &amp;amp;amp; 2&#39;/&gt;\n   &lt;circle cx=&#39;125.46&#39; cy=&#39;202.89&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Low (feat. T-Pain)&#39;/&gt;\n   &lt;circle cx=&#39;325.27&#39; cy=&#39;183.94&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Come On Eileen&#39;/&gt;\n   &lt;circle cx=&#39;259.92&#39; cy=&#39;178.53&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Gold Digger&#39;/&gt;\n   &lt;circle cx=&#39;330.71&#39; cy=&#39;76.06&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Fireball (feat. John Ryan)&#39;/&gt;\n   &lt;circle cx=&#39;312.7&#39; cy=&#39;134.45&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Bang Bang&#39;/&gt;\n   &lt;circle cx=&#39;352.92&#39; cy=&#39;81.86&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Can&amp;amp;#39;t Hold Us (feat. Ray Dalton)&#39;/&gt;\n   &lt;circle cx=&#39;249.87&#39; cy=&#39;103.9&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Don&amp;amp;#39;t Stop Me Now - Remastered 2011&#39;/&gt;\n   &lt;circle cx=&#39;365.06&#39; cy=&#39;89.6&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Take on Me&#39;/&gt;\n   &lt;circle cx=&#39;350.82&#39; cy=&#39;143.73&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Uncertain Smile&#39;/&gt;\n   &lt;circle cx=&#39;205.47&#39; cy=&#39;61.76&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Everytime We Touch&#39;/&gt;\n   &lt;circle cx=&#39;400.67&#39; cy=&#39;146.82&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;It&amp;amp;#39;s Tricky&#39;/&gt;\n   &lt;circle cx=&#39;272.49&#39; cy=&#39;211.78&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Brass Monkey&#39;/&gt;\n   &lt;circle cx=&#39;263.69&#39; cy=&#39;282.54&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Wagon Wheel&#39;/&gt;\n   &lt;circle cx=&#39;270.81&#39; cy=&#39;148.76&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Love On Top&#39;/&gt;\n   &lt;circle cx=&#39;391.87&#39; cy=&#39;158.81&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;I Think We&amp;amp;#39;re Alone Now&#39;/&gt;\n   &lt;circle cx=&#39;361.29&#39; cy=&#39;119.76&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;I Wanna Dance with Somebody (Who Loves Me)&#39;/&gt;\n   &lt;circle cx=&#39;386.84&#39; cy=&#39;202.89&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Uptown Funk (feat. Bruno Mars)&#39;/&gt;\n   &lt;circle cx=&#39;112.06&#39; cy=&#39;212.56&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Single Ladies (Put a Ring on It)&#39;/&gt;\n   &lt;circle cx=&#39;392.71&#39; cy=&#39;129.04&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Shake It Off&#39;/&gt;\n   &lt;circle cx=&#39;340.77&#39; cy=&#39;163.84&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Jump Around&#39;/&gt;\n   &lt;circle cx=&#39;315.63&#39; cy=&#39;119.76&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Hips Don&amp;amp;#39;t Lie (feat. Wyclef Jean)&#39;/&gt;\n   &lt;circle cx=&#39;241.91&#39; cy=&#39;142.96&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Body (feat. Brando)&#39;/&gt;\n   &lt;circle cx=&#39;285.89&#39; cy=&#39;115.89&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Jackie Chan&#39;/&gt;\n   &lt;circle cx=&#39;151.85&#39; cy=&#39;214.88&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Wake Up in the Sky&#39;/&gt;\n   &lt;circle cx=&#39;187.46&#39; cy=&#39;228.02&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Summer, Highland Falls - Live at the Bayou, Washington, D.C. - July 1980&#39;/&gt;\n   &lt;circle cx=&#39;297.2&#39; cy=&#39;108.16&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Magic Dance&#39;/&gt;\n   &lt;circle cx=&#39;315.22&#39; cy=&#39;95.78&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Dancing with Myself - 2001 Remaster&#39;/&gt;\n   &lt;circle cx=&#39;271.65&#39; cy=&#39;71.42&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Waterloo&#39;/&gt;\n   &lt;circle cx=&#39;231.02&#39; cy=&#39;90.76&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Potential Breakup Song&#39;/&gt;\n   &lt;circle cx=&#39;381.4&#39; cy=&#39;119.37&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Levitating (feat. DaBaby)&#39;/&gt;\n   &lt;circle cx=&#39;184.52&#39; cy=&#39;121.69&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Forever&#39;/&gt;\n   &lt;circle cx=&#39;344.54&#39; cy=&#39;136&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;White Walls (feat. ScHoolboy Q &amp;amp;amp; Hollis)&#39;/&gt;\n   &lt;circle cx=&#39;253.22&#39; cy=&#39;85.73&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Rose Tattoo&#39;/&gt;\n   &lt;circle cx=&#39;197.09&#39; cy=&#39;85.73&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Kiss Me I&amp;amp;#39;m #!@&amp;amp;#39;faced&#39;/&gt;\n   &lt;circle cx=&#39;162.32&#39; cy=&#39;137.93&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;All Night (feat. Knox Fortune)&#39;/&gt;\n   &lt;circle cx=&#39;282.54&#39; cy=&#39;131.74&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Don&amp;amp;#39;t Start Now&#39;/&gt;\n   &lt;circle cx=&#39;380.56&#39; cy=&#39;106.61&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;The Legend of Chavo Guerrero&#39;/&gt;\n   &lt;circle cx=&#39;398.57&#39; cy=&#39;168.86&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Bam Bam (feat. Ed Sheeran)&#39;/&gt;\n   &lt;circle cx=&#39;228.51&#39; cy=&#39;142.18&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;The Spins&#39;/&gt;\n   &lt;circle cx=&#39;167.77&#39; cy=&#39;188.2&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Hung Up&#39;/&gt;\n   &lt;circle cx=&#39;355.43&#39; cy=&#39;95.4&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;BREAK MY SOUL&#39;/&gt;\n   &lt;circle cx=&#39;270.81&#39; cy=&#39;190.13&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Post Malone (feat. RANI)&#39;/&gt;\n   &lt;circle cx=&#39;218.87&#39; cy=&#39;182.01&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Take Me Out&#39;/&gt;\n   &lt;circle cx=&#39;200.86&#39; cy=&#39;90.37&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Sk8er Boi&#39;/&gt;\n   &lt;circle cx=&#39;328.2&#39; cy=&#39;66.01&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Timber (feat. Ke$ha)&#39;/&gt;\n   &lt;circle cx=&#39;301.39&#39; cy=&#39;128.26&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Time of Our Lives&#39;/&gt;\n   &lt;circle cx=&#39;192.48&#39; cy=&#39;94.62&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Levels - Radio Edit&#39;/&gt;\n   &lt;circle cx=&#39;285.89&#39; cy=&#39;157.65&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Body On My&#39;/&gt;\n   &lt;circle cx=&#39;402.34&#39; cy=&#39;61.76&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Hey Ya!&#39;/&gt;\n   &lt;circle cx=&#39;297.2&#39; cy=&#39;114.73&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;TiK ToK&#39;/&gt;\n   &lt;circle cx=&#39;401.92&#39; cy=&#39;212.94&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;SexyBack (feat. Timbaland)&#39;/&gt;\n   &lt;circle cx=&#39;315.63&#39; cy=&#39;86.89&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Nice For What&#39;/&gt;\n   &lt;circle cx=&#39;341.19&#39; cy=&#39;130.97&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Up&#39;/&gt;\n   &lt;circle cx=&#39;71.84&#39; cy=&#39;136.38&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Clarity&#39;/&gt;\n   &lt;circle cx=&#39;286.31&#39; cy=&#39;181.62&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;good 4 u&#39;/&gt;\n   &lt;circle cx=&#39;373.44&#39; cy=&#39;147.21&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Cake By The Ocean&#39;/&gt;\n   &lt;circle cx=&#39;124.21&#39; cy=&#39;134.06&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Titanium (feat. Sia)&#39;/&gt;\n   &lt;circle cx=&#39;229.34&#39; cy=&#39;99.65&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Where Them Girls At (feat. Nicki Minaj &amp;amp;amp; Flo Rida)&#39;/&gt;\n   &lt;circle cx=&#39;330.3&#39; cy=&#39;155.72&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;I Know You Want Me (Calle Ocho)&#39;/&gt;\n   &lt;circle cx=&#39;196.25&#39; cy=&#39;102.36&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Club Can&amp;amp;#39;t Handle Me (feat. David Guetta)&#39;/&gt;\n   &lt;circle cx=&#39;343.28&#39; cy=&#39;88.05&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;I Love It (feat. Charli XCX)&#39;/&gt;\n   &lt;circle cx=&#39;390.61&#39; cy=&#39;170.8&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Treasure&#39;/&gt;\n   &lt;circle cx=&#39;357.52&#39; cy=&#39;71.04&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;She Bangs - English Version&#39;/&gt;\n   &lt;circle cx=&#39;327.78&#39; cy=&#39;78&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Magic (feat. Rivers Cuomo)&#39;/&gt;\n   &lt;circle cx=&#39;385.17&#39; cy=&#39;114.34&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Toxic&#39;/&gt;\n   &lt;circle cx=&#39;220.13&#39; cy=&#39;75.29&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Give Me Everything (feat. Ne-Yo, Afrojack &amp;amp;amp; Nayer)&#39;/&gt;\n   &lt;circle cx=&#39;342.44&#39; cy=&#39;206.37&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Temperature&#39;/&gt;\n  &lt;\/g&gt;\n  &lt;g clip-path=&#39;url(#svg_601cff31-88a0-46b6-969f-6bcc8fa5e9b6_c1)&#39;&gt;\n   &lt;text x=&#39;27.91&#39; y=&#39;287.68&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;40%&lt;\/text&gt;\n   &lt;text x=&#39;28.14&#39; y=&#39;210.35&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;60%&lt;\/text&gt;\n   &lt;text x=&#39;28.22&#39; y=&#39;133.02&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;80%&lt;\/text&gt;\n   &lt;text x=&#39;23.41&#39; y=&#39;55.68&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;100%&lt;\/text&gt;\n   &lt;text x=&#39;92.81&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;25%&lt;\/text&gt;\n   &lt;text x=&#39;197.43&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;50%&lt;\/text&gt;\n   &lt;text x=&#39;302.26&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;75%&lt;\/text&gt;\n   &lt;text x=&#39;404.33&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;100%&lt;\/text&gt;\n   &lt;text x=&#39;214.33&#39; y=&#39;350.04&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;valence&lt;\/text&gt;\n   &lt;text transform=&#39;translate(16.93,207.49) rotate(-90.00)&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;energy&lt;\/text&gt;\n   &lt;text x=&#39;6.97&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;Clustered&lt;\/text&gt;\n   &lt;text x=&#39;72.85&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;into&lt;\/text&gt;\n   &lt;text x=&#39;102.64&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#DD5129&#39;&gt;zesty&lt;\/text&gt;\n   &lt;text x=&#39;141.53&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#DD5129&#39;&gt;bops&lt;\/text&gt;\n   &lt;text x=&#39;174.96&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;,&lt;\/text&gt;\n   &lt;text x=&#39;181.19&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#43B284&#39;&gt;dark&lt;\/text&gt;\n   &lt;text x=&#39;214.64&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#43B284&#39;&gt;bangers&lt;\/text&gt;\n   &lt;text x=&#39;269.08&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;,&lt;\/text&gt;\n   &lt;text x=&#39;275.32&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;and&lt;\/text&gt;\n   &lt;text x=&#39;304.04&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#0F7BA2&#39;&gt;mellow&lt;\/text&gt;\n   &lt;text x=&#39;355.37&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#0F7BA2&#39;&gt;jams&lt;\/text&gt;\n   &lt;text x=&#39;6.97&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;Clusters&lt;\/text&gt;\n   &lt;text x=&#39;75.74&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;in&lt;\/text&gt;\n   &lt;text x=&#39;96.28&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;the&lt;\/text&gt;\n   &lt;text x=&#39;125.92&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;current&lt;\/text&gt;\n   &lt;text x=&#39;188.75&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;playlist&lt;\/text&gt;\n  &lt;\/g&gt;\n &lt;\/g&gt;\n&lt;\/svg&gt;&#34;,&#34;js&#34;:null,&#34;uid&#34;:&#34;svg_601cff31-88a0-46b6-969f-6bcc8fa5e9b6&#34;,&#34;ratio&#34;:1.2,&#34;settings&#34;:{&#34;tooltip&#34;:{&#34;css&#34;:&#34;.tooltip_SVGID_ { color:white;padding:2px;border-radius:2px;font-family:Roboto Slab; ; position:absolute;pointer-events:none;z-index:999;}&#34;,&#34;placement&#34;:&#34;doc&#34;,&#34;offx&#34;:10,&#34;offy&#34;:0,&#34;use_cursor_pos&#34;:true,&#34;opacity&#34;:0.8,&#34;usefill&#34;:true,&#34;usestroke&#34;:false,&#34;delay&#34;:{&#34;over&#34;:200,&#34;out&#34;:500}},&#34;hover&#34;:{&#34;css&#34;:&#34;.hover_SVGID_ { fill:#1279BF;stroke:#1279BF;cursor:pointer; }&#34;,&#34;reactive&#34;:false},&#34;hoverkey&#34;:{&#34;css&#34;:&#34;.hover_key_SVGID_ { stroke:red; }&#34;,&#34;reactive&#34;:false},&#34;hovertheme&#34;:{&#34;css&#34;:&#34;.hover_theme_SVGID_ { fill:green; }&#34;,&#34;reactive&#34;:false},&#34;hoverinv&#34;:{&#34;css&#34;:&#34;&#34;},&#34;zoom&#34;:{&#34;min&#34;:1,&#34;max&#34;:1},&#34;capture&#34;:{&#34;css&#34;:&#34;.selected_SVGID_ { fill:red;stroke:gray; }&#34;,&#34;type&#34;:&#34;multiple&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;capturekey&#34;:{&#34;css&#34;:&#34;.selected_key_SVGID_ { stroke:gray; }&#34;,&#34;type&#34;:&#34;single&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;capturetheme&#34;:{&#34;css&#34;:&#34;.selected_theme_SVGID_ { stroke:gray; }&#34;,&#34;type&#34;:&#34;single&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;toolbar&#34;:{&#34;position&#34;:&#34;topright&#34;,&#34;saveaspng&#34;:true,&#34;pngname&#34;:&#34;diagram&#34;},&#34;sizing&#34;:{&#34;rescale&#34;:true,&#34;width&#34;:1}}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;As expected, the majority of songs in the current playlist fall into the bop cluster. Let’s explore this cluster using in more detail with the custom metric &lt;code&gt;vibe&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# assign to clusters
ding_dong_vibes &amp;lt;- 
  ding_dong_clusters %&amp;gt;%
  augment(ding_dong) %&amp;gt;%
  select(track_name,
         valence, 
         energy, 
         .pred_cluster) %&amp;gt;%
  mutate(vibe = valence + energy)

# what are songs with the biggest vibe?
ding_dong_vibes %&amp;gt;%
  arrange(desc(vibe)) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table style=&#34;width:100%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;48%&#34; /&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;col width=&#34;10%&#34; /&gt;
&lt;col width=&#34;20%&#34; /&gt;
&lt;col width=&#34;8%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;track_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;valence&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;energy&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;.pred_cluster&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;vibe&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hey Ya!&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.965&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.974&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.939&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Rasputin&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.966&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.893&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.859&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;September&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.979&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.832&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.811&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;She Bangs - English Version&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.858&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.950&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.808&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Take on Me&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.876&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.902&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.778&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;The Legend of Chavo Guerrero&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.913&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.858&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.771&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Can’t Hold Us (feat. Ray Dalton)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.847&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.922&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.769&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Toxic&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.924&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.838&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.762&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Timber (feat. Ke$ha)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.788&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.963&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.751&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Shake It Off&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.942&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.800&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.742&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As expected, when arranging by &lt;code&gt;vibe&lt;/code&gt;, the top songs are all a part of the first cluster. And they are, indeed, &lt;em&gt;a vibe&lt;/em&gt;:&lt;/p&gt;



&lt;iframe src=&#34;https://open.spotify.com/embed/track/1uPrIHgYztXSkkcts9jet8&#34;
    width=&#34;100%&#34;
    height=&#34;380&#34;
    frameborder=&#34;0&#34;
    allowtransparency=&#34;true&#34;
    allow=&#34;encrypted-media&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;Compare that with the second cluster, which are generally lower energy (I’d personally disagree with spotify ranking Mr. Blue Sky and Single Ladies as “low energy,” but most others make sense).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ding_dong_vibes %&amp;gt;%
  filter(.pred_cluster == &amp;quot;Cluster_2&amp;quot;) %&amp;gt;%
  arrange(vibe) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;67%&#34; /&gt;
&lt;col width=&#34;7%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;5%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;track_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;valence&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;energy&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;.pred_cluster&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;vibe&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mr. Blue Sky&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.478&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.338&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Single Ladies (Put a Ring on It)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.272&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.584&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.856&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Low (feat. T-Pain)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.304&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.609&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.913&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;I Got You&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.544&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.399&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.943&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Wake Up in the Sky&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.367&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.578&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.945&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Summer, Highland Falls - Live at the Bayou, Washington, D.C. - July 1980&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.452&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.544&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.996&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Wagon Wheel&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.634&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.403&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.037&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hung Up&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.405&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.647&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.052&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Take Me Out&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.527&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.663&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.190&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Country Grammar (Hot Shit)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.565&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.664&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.229&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;



&lt;iframe src=&#34;https://open.spotify.com/embed/track/12jjuxN1gxlm29cqL5M6MW&#34;
    width=&#34;100%&#34;
    height=&#34;380&#34;
    frameborder=&#34;0&#34;
    allowtransparency=&#34;true&#34;
    allow=&#34;encrypted-media&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;Finally, the third cluster mostly contains songs with low valence but relatively high energy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ding_dong_vibes %&amp;gt;%
  filter(.pred_cluster == &amp;quot;Cluster_3&amp;quot;) %&amp;gt;%
  arrange(vibe) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;54%&#34; /&gt;
&lt;col width=&#34;10%&#34; /&gt;
&lt;col width=&#34;9%&#34; /&gt;
&lt;col width=&#34;18%&#34; /&gt;
&lt;col width=&#34;7%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;track_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;valence&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;energy&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;.pred_cluster&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;vibe&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Clarity&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.176&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.781&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.957&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Titanium (feat. Sia)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.301&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.787&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.088&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mr. Brightside&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.240&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.918&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.158&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;All Night (feat. Knox Fortune)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.392&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.777&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.169&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Forever&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.445&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.819&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.264&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Shout, Pts. 1 &amp;amp; 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.416&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.866&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.282&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;The Spins&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.550&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.766&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.316&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Club Can’t Handle Me (feat. David Guetta)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.473&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.869&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.342&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Body (feat. Brando)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.582&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.764&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.346&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Levels - Radio Edit&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.464&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.889&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.353&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;



&lt;iframe src=&#34;https://open.spotify.com/embed/track/3n3Ppam7vgaVa1iaRUc9Lp&#34;
    width=&#34;100%&#34;
    height=&#34;380&#34;
    frameborder=&#34;0&#34;
    allowtransparency=&#34;true&#34;
    allow=&#34;encrypted-media&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;Now that I have the songs in the current playlist sorted by cluster, let’s pull in some new songs and assign them to the appropriate cluster!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-new-songs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding new songs&lt;/h2&gt;
&lt;p&gt;To go searching for new songs, we’ll start by casting a wide net then narrow the search with some of the &lt;code&gt;get_*()&lt;/code&gt; functions from spotifyr. I’ll start by using &lt;code&gt;get_categories()&lt;/code&gt; to explore the categories available in spotify.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_categories() %&amp;gt;%
  as_tibble() %&amp;gt;%
  select(id, name) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;toplists&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Top Lists&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;hiphop&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Hip-Hop&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;pop&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Pop&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;country&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Country&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;0JQ5DAqbMKFxXaXKP7zcDp&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Latin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;rock&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Rock&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;summer&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Summer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;0JQ5DAqbMKFAXlCG6QvYQ4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Workout&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;0JQ5DAqbMKFEZPnFQSFB1T&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R&amp;amp;B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;edm_dance&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Dance/Electronic&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I don’t really want to play country music or R&amp;amp;B during the wedding, so I’ll filter to a few categories before using &lt;code&gt;get_category_playlists()&lt;/code&gt; to pull in the featured playlists available in each category.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# pull in playlist ids
playlists &amp;lt;- 
  get_categories() %&amp;gt;%
  as_tibble() %&amp;gt;%
  filter(id %in% c(&amp;quot;toplists&amp;quot;, &amp;quot;hiphop&amp;quot;, &amp;quot;pop&amp;quot;, &amp;quot;rock&amp;quot;, &amp;quot;summer&amp;quot;)) %&amp;gt;%
  pull(id) %&amp;gt;%
  map_dfr(get_category_playlists) %&amp;gt;%
  as_tibble() %&amp;gt;%
  select(id, name, description) %&amp;gt;%
  distinct(id, .keep_all = TRUE)

playlists %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;18%&#34; /&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;col width=&#34;67%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;name&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZF1DXcBWIGoYBM5M&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Today’s Top Hits&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Steve Lacy is on top of the Hottest 50!&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZF1DX0XUsuxWHRQd&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RapCaviar&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Music from Drake, Offset and 42 Dugg.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZF1DXcF6B6QPhFDv&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Rock This&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;The latest from Panic! At The Disco along with the Rock songs you need to hear today.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZF1DX4dyzvuaRJ0n&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;mint&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;The world’s biggest dance hits. Cover: Zedd &amp;amp; Maren Morris&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZF1DX1lVhptIYRda&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Hot Country&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Today’s top country hits of the week, worldwide! Cover: Tyler Hubbard&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZF1DX10zKzsJ2jva&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Viva Latino&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Today’s top Latin hits, elevando nuestra música. Cover: Anitta, Maluma.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZF1DX4SBhb3fqCJd&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Are &amp;amp; Be&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;The pulse of R&amp;amp;B music today. Cover: Tink&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZEVXbLRQDuF5jeBp&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Top 50 - USA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Your daily update of the most played tracks right now - USA.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZEVXbMDoHDwVN2tF&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Top 50 - Global&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Your daily update of the most played tracks right now - Global.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZEVXbLiRSasKsNU9&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Viral 50 - Global&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Your daily update of the most viral tracks right now - Global.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There’s a lot of playlists in &lt;code&gt;playlists&lt;/code&gt;, so I’ve gone through and selected a few that I’m interested in exploring further.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;selected_playlists &amp;lt;-
  c(&amp;quot;Today&amp;#39;s Top Hits&amp;quot;,
    &amp;quot;mint&amp;quot;,
    &amp;quot;Top 50 - US&amp;quot;,
    &amp;quot;Top 50 - Global&amp;quot;,
    &amp;quot;Viral 50 - US&amp;quot;,
    &amp;quot;Viral 50 - Global&amp;quot;,
    &amp;quot;New Music Friday&amp;quot;,
    &amp;quot;Most Necessary&amp;quot;,
    &amp;quot;Internet People&amp;quot;,
    &amp;quot;Gold School&amp;quot;,
    &amp;quot;Hot Hits USA&amp;quot;,
    &amp;quot;Pop Rising&amp;quot;,
    &amp;quot;teen beats&amp;quot;,
    &amp;quot;big on the internet&amp;quot;,
    &amp;quot;Party Hits&amp;quot;,
    &amp;quot;Mega Hit Mix&amp;quot;,
    &amp;quot;Pumped Pop&amp;quot;,
    &amp;quot;Hit Rewind&amp;quot;,
    &amp;quot;The Ultimate Hit Mix&amp;quot;,
    &amp;quot;00s Rock Anthems&amp;quot;,
    &amp;quot;Summer Hits&amp;quot;,
    &amp;quot;Barack Obama&amp;#39;s Summer 2022 Playlist&amp;quot;,
    &amp;quot;Summer Hits of the 10s&amp;quot;,
    &amp;quot;Family Road Trip&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this shorter list of playlists, I can pull in the all the songs that appear on each with &lt;code&gt;get_playlist_tracks()&lt;/code&gt;. Some songs may appear on multiple playlists, so we’ll only look at unique songs by &lt;code&gt;track_id&lt;/code&gt;. I’ve already pulled in features for songs currently on the playlist, so we can filter those out as well. Finally, &lt;code&gt;get_track_audio_features()&lt;/code&gt; limits queries to a maximum of 100 songs, so we’ll select the top 100 most popular songs within the sample.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_songs &amp;lt;- 
  playlists %&amp;gt;%
  filter(name %in% selected_playlists) %&amp;gt;%
  pull(id) %&amp;gt;%
  map_dfr(get_playlist_tracks) %&amp;gt;%
  as_tibble()

new_songs &amp;lt;- 
  new_songs %&amp;gt;%
  select(track.id,
         track.name,
         track.popularity) %&amp;gt;%
  rename_with(~stringr::str_replace(.x, &amp;quot;\\.&amp;quot;, &amp;quot;_&amp;quot;)) %&amp;gt;%
  distinct(track_id, .keep_all = TRUE) %&amp;gt;%
  arrange(desc(track_popularity)) %&amp;gt;%
  filter(!track_id %in% ding_dong$track_id) %&amp;gt;%
  slice_head(n = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;28%&#34; /&gt;
&lt;col width=&#34;50%&#34; /&gt;
&lt;col width=&#34;21%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;track_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;track_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;track_popularity&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2tTmW7RDtMQtBk7m2rYeSw&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Quevedo: Bzrp Music Sessions, Vol. 52&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6Sq7ltF9Qa7SNFBsV5Cogx&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Me Porto Bonito&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;99&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6xGruZOHLs39ZbVccQTuPZ&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Glimpse of Us&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;97&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1IHWl5LamUGEuP4ozKQSXZ&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Tití Me Preguntó&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;97&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;4LRPiXqCikLlN15c3yImP7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;As It Was&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5Eax0qFko2dh7Rl2lYs3bx&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Efecto&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3k3NWokhRRkEPhCzPmV8TW&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Ojitos Lindos&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;75FEaRjZTKLhTrFGsfMUXR&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Running Up That Hill (A Deal With God)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;95&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6Xom58OOXk2SoU711L2IXO&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Moscow Mule&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;95&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;0mBP9X2gPCuapvpZ7TGDk3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Left and Right (Feat. Jung Kook of BTS)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;94&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now let’s assign these 100 news songs to the clusters we found earlier based on their valence and energy!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_song_features &amp;lt;- 
  new_songs %&amp;gt;%
  pull(track_id) %&amp;gt;%
  get_track_audio_features()

new_songs &amp;lt;- 
  new_songs %&amp;gt;%
  left_join(new_song_features, by = c(&amp;quot;track_id&amp;quot; = &amp;quot;id&amp;quot;))

new_songs_clustered &amp;lt;- 
  ding_dong_clusters %&amp;gt;%
  augment(new_songs) %&amp;gt;%
  select(track_name,
         valence,
         energy,
         .pred_cluster) %&amp;gt;%
  mutate(vibe = valence + energy)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-3&#34; style=&#34;width:4500px;height:3000px;&#34; class=&#34;girafe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-3&#34;&gt;{&#34;x&#34;:{&#34;html&#34;:&#34;&lt;?xml version=\&#34;1.0\&#34; encoding=\&#34;UTF-8\&#34;?&gt;\n&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; xmlns:xlink=&#39;http://www.w3.org/1999/xlink&#39; id=&#39;svg_dfb0e27c-dcd6-496a-830b-9e5e9368f9b8&#39; viewBox=&#39;0 0 432 360&#39;&gt;\n &lt;defs&gt;\n  &lt;clipPath id=&#39;svg_dfb0e27c-dcd6-496a-830b-9e5e9368f9b8_c1&#39;&gt;\n   &lt;rect x=&#39;0&#39; y=&#39;0&#39; width=&#39;432&#39; height=&#39;360&#39;/&gt;\n  &lt;\/clipPath&gt;\n  &lt;clipPath id=&#39;svg_dfb0e27c-dcd6-496a-830b-9e5e9368f9b8_c2&#39;&gt;\n   &lt;rect x=&#39;50.53&#39; y=&#39;49.46&#39; width=&#39;374.5&#39; height=&#39;270.51&#39;/&gt;\n  &lt;\/clipPath&gt;\n &lt;\/defs&gt;\n &lt;g&gt;\n  &lt;g clip-path=&#39;url(#svg_dfb0e27c-dcd6-496a-830b-9e5e9368f9b8_c1)&#39;&gt;\n   &lt;rect x=&#39;0&#39; y=&#39;0&#39; width=&#39;432&#39; height=&#39;360&#39; fill=&#39;#FFFFFF&#39; stroke=&#39;#FFFFFF&#39; stroke-width=&#39;0.75&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39;/&gt;\n   &lt;rect x=&#39;0&#39; y=&#39;0&#39; width=&#39;432&#39; height=&#39;360&#39; fill=&#39;#FFFFFF&#39; stroke=&#39;#FFFFFF&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39;/&gt;\n  &lt;\/g&gt;\n  &lt;g clip-path=&#39;url(#svg_dfb0e27c-dcd6-496a-830b-9e5e9368f9b8_c2)&#39;&gt;\n   &lt;polyline points=&#39;50.53,284.39 425.03,284.39&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;50.53,211.64 425.03,211.64&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;50.53,138.88 425.03,138.88&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;50.53,66.12 425.03,66.12&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;82.94,319.97 82.94,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;178.88,319.97 178.88,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;274.83,319.97 274.83,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;370.78,319.97 370.78,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;50.53,248.01 425.03,248.01&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;50.53,175.26 425.03,175.26&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;50.53,102.50 425.03,102.50&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;130.91,319.97 130.91,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;226.86,319.97 226.86,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;322.80,319.97 322.80,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;418.75,319.97 418.75,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;circle cx=&#39;246.05&#39; cy=&#39;109.05&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Quevedo: Bzrp Music Sessions, Vol. 52&#39;/&gt;\n   &lt;circle cx=&#39;198.07&#39; cy=&#39;134.51&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Me Porto Bonito&#39;/&gt;\n   &lt;circle cx=&#39;137.82&#39; cy=&#39;278.21&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Glimpse of Us&#39;/&gt;\n   &lt;circle cx=&#39;106.73&#39; cy=&#39;133.42&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Tití Me Preguntó&#39;/&gt;\n   &lt;circle cx=&#39;289.03&#39; cy=&#39;127.6&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;As It Was&#39;/&gt;\n   &lt;circle cx=&#39;124.77&#39; cy=&#39;220.73&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Efecto&#39;/&gt;\n   &lt;circle cx=&#39;137.82&#39; cy=&#39;143.97&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Ojitos Lindos&#39;/&gt;\n   &lt;circle cx=&#39;110.57&#39; cy=&#39;194.54&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Running Up That Hill (A Deal With God)&#39;/&gt;\n   &lt;circle cx=&#39;147.03&#39; cy=&#39;148.34&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Moscow Mule&#39;/&gt;\n   &lt;circle cx=&#39;310.91&#39; cy=&#39;178.17&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Left and Right (Feat. Jung Kook of BTS)&#39;/&gt;\n   &lt;circle cx=&#39;332.4&#39; cy=&#39;166.89&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;DESPECHÁ&#39;/&gt;\n   &lt;circle cx=&#39;238.37&#39; cy=&#39;205.82&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;PROVENZA&#39;/&gt;\n   &lt;circle cx=&#39;351.59&#39; cy=&#39;103.59&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;I Ain&amp;amp;#39;t Worried&#39;/&gt;\n   &lt;circle cx=&#39;380.75&#39; cy=&#39;128.69&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Late Night Talking&#39;/&gt;\n   &lt;circle cx=&#39;361.18&#39; cy=&#39;146.52&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;La Bachata&#39;/&gt;\n   &lt;circle cx=&#39;289.03&#39; cy=&#39;127.6&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;As It Was&#39;/&gt;\n   &lt;circle cx=&#39;195.77&#39; cy=&#39;144.7&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Tarot&#39;/&gt;\n   &lt;circle cx=&#39;255.64&#39; cy=&#39;162.16&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Te Felicito&#39;/&gt;\n   &lt;circle cx=&#39;312.06&#39; cy=&#39;123.24&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;About Damn Time&#39;/&gt;\n   &lt;circle cx=&#39;215.73&#39; cy=&#39;102.87&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Party&#39;/&gt;\n   &lt;circle cx=&#39;216.11&#39; cy=&#39;149.79&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;I Like You (A Happier Song) (with Doja Cat)&#39;/&gt;\n   &lt;circle cx=&#39;358.11&#39; cy=&#39;133.79&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Sunroof&#39;/&gt;\n   &lt;circle cx=&#39;408&#39; cy=&#39;185.08&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Doja&#39;/&gt;\n   &lt;circle cx=&#39;69.47&#39; cy=&#39;171.26&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;MIDDLE OF THE NIGHT&#39;/&gt;\n   &lt;circle cx=&#39;175.43&#39; cy=&#39;148.7&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Jimmy Cooks (feat. 21 Savage)&#39;/&gt;\n   &lt;circle cx=&#39;257.18&#39; cy=&#39;107.59&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;La Corriente&#39;/&gt;\n   &lt;circle cx=&#39;299.78&#39; cy=&#39;209.09&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Bad Habit&#39;/&gt;\n   &lt;circle cx=&#39;88.31&#39; cy=&#39;199.63&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Running Up That Hill (A Deal With God) - 2018 Remaster&#39;/&gt;\n   &lt;circle cx=&#39;318.97&#39; cy=&#39;174.89&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Vegas (From the Original Motion Picture Soundtrack ELVIS)&#39;/&gt;\n   &lt;circle cx=&#39;135.52&#39; cy=&#39;162.16&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Where Are You Now&#39;/&gt;\n   &lt;circle cx=&#39;300.54&#39; cy=&#39;142.52&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Ferrari&#39;/&gt;\n   &lt;circle cx=&#39;122.09&#39; cy=&#39;208.73&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Until I Found You&#39;/&gt;\n   &lt;circle cx=&#39;187.71&#39; cy=&#39;99.96&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Sweater Weather&#39;/&gt;\n   &lt;circle cx=&#39;163.15&#39; cy=&#39;127.97&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Blinding Lights&#39;/&gt;\n   &lt;circle cx=&#39;354.27&#39; cy=&#39;194.17&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Una Noche en Medellín&#39;/&gt;\n   &lt;circle cx=&#39;262.17&#39; cy=&#39;79.95&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;One Kiss (with Dua Lipa)&#39;/&gt;\n   &lt;circle cx=&#39;209.2&#39; cy=&#39;141.79&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Dandelions&#39;/&gt;\n   &lt;circle cx=&#39;401.48&#39; cy=&#39;80.31&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Bad Decisions (with BTS &amp;amp;amp; Snoop Dogg)&#39;/&gt;\n   &lt;circle cx=&#39;159.31&#39; cy=&#39;188.72&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;First Class&#39;/&gt;\n   &lt;circle cx=&#39;373.08&#39; cy=&#39;115.6&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Woman&#39;/&gt;\n   &lt;circle cx=&#39;218.41&#39; cy=&#39;115.6&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;STAY (with Justin Bieber)&#39;/&gt;\n   &lt;circle cx=&#39;165.07&#39; cy=&#39;159.98&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;WAIT FOR U (feat. Drake &amp;amp;amp; Tems)&#39;/&gt;\n   &lt;circle cx=&#39;396.49&#39; cy=&#39;103.23&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Cold Heart - PNAU Remix&#39;/&gt;\n   &lt;circle cx=&#39;238.76&#39; cy=&#39;202.54&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Heat Waves&#39;/&gt;\n   &lt;circle cx=&#39;153.94&#39; cy=&#39;142.52&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Under The Influence&#39;/&gt;\n   &lt;circle cx=&#39;204.21&#39; cy=&#39;123.96&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Ghost&#39;/&gt;\n   &lt;circle cx=&#39;67.55&#39; cy=&#39;270.21&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;traitor&#39;/&gt;\n   &lt;circle cx=&#39;85.24&#39; cy=&#39;198.18&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Another Love&#39;/&gt;\n   &lt;circle cx=&#39;248.73&#39; cy=&#39;96.68&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Watermelon Sugar&#39;/&gt;\n   &lt;circle cx=&#39;241.06&#39; cy=&#39;68.67&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Bad Habits&#39;/&gt;\n   &lt;circle cx=&#39;244.51&#39; cy=&#39;85.77&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;THATS WHAT I WANT&#39;/&gt;\n   &lt;circle cx=&#39;350.44&#39; cy=&#39;81.04&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Shivers&#39;/&gt;\n   &lt;circle cx=&#39;196.92&#39; cy=&#39;103.59&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Belly Dancer&#39;/&gt;\n   &lt;circle cx=&#39;94.45&#39; cy=&#39;125.42&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;I Was Never There&#39;/&gt;\n   &lt;circle cx=&#39;87.54&#39; cy=&#39;236.74&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;drivers license&#39;/&gt;\n   &lt;circle cx=&#39;103.28&#39; cy=&#39;170.89&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;deja vu&#39;/&gt;\n   &lt;circle cx=&#39;102.13&#39; cy=&#39;177.8&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Call Out My Name&#39;/&gt;\n   &lt;circle cx=&#39;257.56&#39; cy=&#39;112.69&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Betty (Get Money)&#39;/&gt;\n   &lt;circle cx=&#39;150.1&#39; cy=&#39;197.09&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;In The Stars&#39;/&gt;\n   &lt;circle cx=&#39;202.3&#39; cy=&#39;184.72&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Kesariya (From &amp;amp;quot;Brahmastra&amp;amp;quot;)&#39;/&gt;\n   &lt;circle cx=&#39;247.97&#39; cy=&#39;108.69&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Enemy (with JID) - from the series Arcane League of Legends&#39;/&gt;\n   &lt;circle cx=&#39;176.97&#39; cy=&#39;204.36&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Night Changes&#39;/&gt;\n   &lt;circle cx=&#39;109.04&#39; cy=&#39;199.27&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Don’t Blame Me&#39;/&gt;\n   &lt;circle cx=&#39;171.98&#39; cy=&#39;133.42&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Music For a Sushi Restaurant&#39;/&gt;\n   &lt;circle cx=&#39;240.29&#39; cy=&#39;104.32&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Everybody Wants To Rule The World&#39;/&gt;\n   &lt;circle cx=&#39;151.64&#39; cy=&#39;71.58&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Only Love Can Hurt Like This&#39;/&gt;\n   &lt;circle cx=&#39;221.48&#39; cy=&#39;179.99&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Starboy&#39;/&gt;\n   &lt;circle cx=&#39;379.99&#39; cy=&#39;138.88&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;MAMIII&#39;/&gt;\n   &lt;circle cx=&#39;204.6&#39; cy=&#39;114.87&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Pepas&#39;/&gt;\n   &lt;circle cx=&#39;213.04&#39; cy=&#39;115.96&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;The Motto&#39;/&gt;\n   &lt;circle cx=&#39;81.02&#39; cy=&#39;285.85&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;lovely (with Khalid)&#39;/&gt;\n   &lt;circle cx=&#39;194.24&#39; cy=&#39;197.09&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;abcdefu&#39;/&gt;\n   &lt;circle cx=&#39;290.57&#39; cy=&#39;109.78&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Believer&#39;/&gt;\n   &lt;circle cx=&#39;144.34&#39; cy=&#39;153.07&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Yellow&#39;/&gt;\n   &lt;circle cx=&#39;277.52&#39; cy=&#39;111.96&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Numb&#39;/&gt;\n   &lt;circle cx=&#39;104.43&#39; cy=&#39;225.82&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;STAYING ALIVE (feat. Drake &amp;amp;amp; Lil Baby)&#39;/&gt;\n   &lt;circle cx=&#39;80.64&#39; cy=&#39;307.67&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;TV&#39;/&gt;\n   &lt;circle cx=&#39;124&#39; cy=&#39;170.89&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Boyfriend&#39;/&gt;\n   &lt;circle cx=&#39;295.94&#39; cy=&#39;61.76&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Afraid To Feel&#39;/&gt;\n   &lt;circle cx=&#39;303.61&#39; cy=&#39;213.82&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Bad Habit&#39;/&gt;\n   &lt;circle cx=&#39;218.41&#39; cy=&#39;115.6&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;STAY (with Justin Bieber)&#39;/&gt;\n   &lt;circle cx=&#39;251.8&#39; cy=&#39;237.46&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;cómo dormiste?&#39;/&gt;\n   &lt;circle cx=&#39;230.7&#39; cy=&#39;202.54&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Die For You&#39;/&gt;\n   &lt;circle cx=&#39;229.16&#39; cy=&#39;96.32&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;House of Memories&#39;/&gt;\n   &lt;circle cx=&#39;292.87&#39; cy=&#39;137.06&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Wait a Minute!&#39;/&gt;\n   &lt;circle cx=&#39;116.71&#39; cy=&#39;219.28&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Atlantis&#39;/&gt;\n   &lt;circle cx=&#39;176.97&#39; cy=&#39;186.9&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Softcore&#39;/&gt;\n   &lt;circle cx=&#39;87.54&#39; cy=&#39;188.35&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;The Hills&#39;/&gt;\n   &lt;circle cx=&#39;213.04&#39; cy=&#39;134.15&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Crazy What Love Can Do&#39;/&gt;\n   &lt;circle cx=&#39;186.94&#39; cy=&#39;125.78&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Envolver&#39;/&gt;\n   &lt;circle cx=&#39;378.07&#39; cy=&#39;137.42&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;INDUSTRY BABY (feat. Jack Harlow)&#39;/&gt;\n   &lt;circle cx=&#39;247.2&#39; cy=&#39;116.33&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Circles&#39;/&gt;\n   &lt;circle cx=&#39;213.04&#39; cy=&#39;140.34&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Peaches (feat. Daniel Caesar &amp;amp;amp; Giveon)&#39;/&gt;\n   &lt;circle cx=&#39;224.56&#39; cy=&#39;148.7&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Infinity&#39;/&gt;\n   &lt;circle cx=&#39;206.13&#39; cy=&#39;246.2&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Someone You Loved&#39;/&gt;\n   &lt;circle cx=&#39;261.01&#39; cy=&#39;102.5&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Beggin&amp;amp;#39;&#39;/&gt;\n   &lt;circle cx=&#39;138.59&#39; cy=&#39;238.92&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Heather&#39;/&gt;\n   &lt;circle cx=&#39;254.11&#39; cy=&#39;123.6&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Bones&#39;/&gt;\n   &lt;circle cx=&#39;367.71&#39; cy=&#39;139.61&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Locked out of Heaven&#39;/&gt;\n   &lt;circle cx=&#39;125.54&#39; cy=&#39;62.12&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Mr. Brightside&#39;/&gt;\n  &lt;\/g&gt;\n  &lt;g clip-path=&#39;url(#svg_dfb0e27c-dcd6-496a-830b-9e5e9368f9b8_c1)&#39;&gt;\n   &lt;text x=&#39;23.41&#39; y=&#39;251.99&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;40%&lt;\/text&gt;\n   &lt;text x=&#39;23.64&#39; y=&#39;179.24&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;60%&lt;\/text&gt;\n   &lt;text x=&#39;23.72&#39; y=&#39;106.48&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;80%&lt;\/text&gt;\n   &lt;text x=&#39;120.88&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;25%&lt;\/text&gt;\n   &lt;text x=&#39;216.73&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;50%&lt;\/text&gt;\n   &lt;text x=&#39;312.78&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;75%&lt;\/text&gt;\n   &lt;text x=&#39;406.08&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;100%&lt;\/text&gt;\n   &lt;text x=&#39;212.08&#39; y=&#39;350.04&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;valence&lt;\/text&gt;\n   &lt;text transform=&#39;translate(16.93,207.49) rotate(-90.00)&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;energy&lt;\/text&gt;\n   &lt;text x=&#39;6.97&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;Clustered&lt;\/text&gt;\n   &lt;text x=&#39;72.85&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;into&lt;\/text&gt;\n   &lt;text x=&#39;102.64&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#DD5129&#39;&gt;zesty&lt;\/text&gt;\n   &lt;text x=&#39;141.53&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#DD5129&#39;&gt;bops&lt;\/text&gt;\n   &lt;text x=&#39;174.96&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;,&lt;\/text&gt;\n   &lt;text x=&#39;181.19&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#43B284&#39;&gt;dark&lt;\/text&gt;\n   &lt;text x=&#39;214.64&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#43B284&#39;&gt;bangers&lt;\/text&gt;\n   &lt;text x=&#39;269.08&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;,&lt;\/text&gt;\n   &lt;text x=&#39;275.32&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;and&lt;\/text&gt;\n   &lt;text x=&#39;304.04&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#0F7BA2&#39;&gt;mellow&lt;\/text&gt;\n   &lt;text x=&#39;355.37&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#0F7BA2&#39;&gt;jams&lt;\/text&gt;\n   &lt;text x=&#39;6.97&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;New&lt;\/text&gt;\n   &lt;text x=&#39;47.85&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;songs,&lt;\/text&gt;\n   &lt;text x=&#39;101.82&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;same&lt;\/text&gt;\n   &lt;text x=&#39;148.56&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;old&lt;\/text&gt;\n   &lt;text x=&#39;177.3&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;clusters&lt;\/text&gt;\n  &lt;\/g&gt;\n &lt;\/g&gt;\n&lt;\/svg&gt;&#34;,&#34;js&#34;:null,&#34;uid&#34;:&#34;svg_dfb0e27c-dcd6-496a-830b-9e5e9368f9b8&#34;,&#34;ratio&#34;:1.2,&#34;settings&#34;:{&#34;tooltip&#34;:{&#34;css&#34;:&#34;.tooltip_SVGID_ { color:white;padding:2px;border-radius:2px;font-family:Roboto Slab; ; position:absolute;pointer-events:none;z-index:999;}&#34;,&#34;placement&#34;:&#34;doc&#34;,&#34;offx&#34;:10,&#34;offy&#34;:0,&#34;use_cursor_pos&#34;:true,&#34;opacity&#34;:0.8,&#34;usefill&#34;:true,&#34;usestroke&#34;:false,&#34;delay&#34;:{&#34;over&#34;:200,&#34;out&#34;:500}},&#34;hover&#34;:{&#34;css&#34;:&#34;.hover_SVGID_ { fill:#1279BF;stroke:#1279BF;cursor:pointer; }&#34;,&#34;reactive&#34;:false},&#34;hoverkey&#34;:{&#34;css&#34;:&#34;.hover_key_SVGID_ { stroke:red; }&#34;,&#34;reactive&#34;:false},&#34;hovertheme&#34;:{&#34;css&#34;:&#34;.hover_theme_SVGID_ { fill:green; }&#34;,&#34;reactive&#34;:false},&#34;hoverinv&#34;:{&#34;css&#34;:&#34;&#34;},&#34;zoom&#34;:{&#34;min&#34;:1,&#34;max&#34;:1},&#34;capture&#34;:{&#34;css&#34;:&#34;.selected_SVGID_ { fill:red;stroke:gray; }&#34;,&#34;type&#34;:&#34;multiple&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;capturekey&#34;:{&#34;css&#34;:&#34;.selected_key_SVGID_ { stroke:gray; }&#34;,&#34;type&#34;:&#34;single&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;capturetheme&#34;:{&#34;css&#34;:&#34;.selected_theme_SVGID_ { stroke:gray; }&#34;,&#34;type&#34;:&#34;single&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;toolbar&#34;:{&#34;position&#34;:&#34;topright&#34;,&#34;saveaspng&#34;:true,&#34;pngname&#34;:&#34;diagram&#34;},&#34;sizing&#34;:{&#34;rescale&#34;:true,&#34;width&#34;:1}}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Nice! It looks like the new songs are far more broad than the original playlist, but we can look at just the songs in the first cluster with the biggest vibe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_songs_clustered %&amp;gt;%
  filter(.pred_cluster == &amp;quot;Cluster_1&amp;quot;) %&amp;gt;%
  arrange(desc(vibe)) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;52%&#34; /&gt;
&lt;col width=&#34;10%&#34; /&gt;
&lt;col width=&#34;9%&#34; /&gt;
&lt;col width=&#34;19%&#34; /&gt;
&lt;col width=&#34;8%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;track_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;valence&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;energy&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;.pred_cluster&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;vibe&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Bad Decisions (with BTS &amp;amp; Snoop Dogg)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.955&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.861&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cold Heart - PNAU Remix&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.942&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.798&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.740&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Shivers&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.822&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.859&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.681&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Woman&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.881&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.764&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.645&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Late Night Talking&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.901&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.728&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.629&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;I Ain’t Worried&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.825&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.797&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.622&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;MAMIII&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.899&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.700&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.599&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;INDUSTRY BABY (feat. Jack Harlow)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.894&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.704&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.598&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Afraid To Feel&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.680&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.912&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.592&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Locked out of Heaven&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.867&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.698&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.565&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now for the true vibe check — do these songs &lt;em&gt;belong&lt;/em&gt; on the playlist?&lt;/p&gt;



&lt;iframe src=&#34;https://open.spotify.com/embed/track/0xzI1KAr0Yd9tv8jlIk3sn&#34;
    width=&#34;100%&#34;
    height=&#34;380&#34;
    frameborder=&#34;0&#34;
    allowtransparency=&#34;true&#34;
    allow=&#34;encrypted-media&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;Oh &lt;strong&gt;&lt;em&gt;hell&lt;/em&gt; yeah!&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Some notes&lt;/strong&gt;: this analysis was done on Aug. 8th, 2022 — spotify’s featured playlists and tracks change on on a regular basis and may also depend on unique user data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introducing {nplyr}</title>
      <link>https://www.thedatadiary.net/blog/2022-07-24-introducing-nplyr/</link>
      <pubDate>Sun, 24 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-07-24-introducing-nplyr/</guid>
      <description>


&lt;p&gt;Data manipulation and transformation is a fundamental part of any analysis. There are excellent tools in the R ecosystem for manipulating data frames (&lt;a href=&#34;https://dplyr.tidyverse.org/&#34;&gt;dplyr&lt;/a&gt;, &lt;a href=&#34;https://rdatatable.gitlab.io/data.table/&#34;&gt;data.table&lt;/a&gt;, and &lt;a href=&#34;https://arrow.apache.org/docs/r/&#34;&gt;arrow&lt;/a&gt;, to name a few). Sometimes, however, it is desirable to work with &lt;em&gt;nested&lt;/em&gt; data frames, for which few tools are readily available.&lt;/p&gt;
&lt;p&gt;This is where &lt;a href=&#34;https://markjrieke.github.io/nplyr/&#34;&gt;nplyr&lt;/a&gt; comes into play! nplyr is a grammar of nested data manipulation that allows users to perform dplyr-like manipulations on data frames nested within a list-col of another data frame. Most dplyr verbs have nested equivalents in nplyr. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nest_mutate()&lt;/code&gt; is the nested equivalent of &lt;code&gt;mutate()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nest_select()&lt;/code&gt; is the nested equivalent of &lt;code&gt;select()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nest_filter()&lt;/code&gt; is the nested equivalent of &lt;code&gt;filter()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nest_summarise()&lt;/code&gt; is the nested equivalent of &lt;code&gt;summarise()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nest_group_by()&lt;/code&gt; is the nested equivalent of &lt;code&gt;group_by()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;installation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Installation&lt;/h3&gt;
&lt;p&gt;nplyr 0.1.0 is available on &lt;a href=&#34;https://cran.r-project.org/web/packages/nplyr/index.html&#34;&gt;CRAN&lt;/a&gt;. Alternatively, you can install the development version from github with the &lt;a href=&#34;https://cran.r-project.org/package=devtools&#34;&gt;devtools&lt;/a&gt; or &lt;a href=&#34;https://cran.r-project.org/package=remotes&#34;&gt;remotes&lt;/a&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install from CRAN
install.packages(&amp;quot;nplyr&amp;quot;)

# install from github
devtools::install_github(&amp;quot;markjrieke/nplyr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;usage&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Usage&lt;/h3&gt;
&lt;p&gt;To get started, we’ll create a nested column for the country data within each continent from the &lt;a href=&#34;https://cran.r-project.org/package=gapminder&#34;&gt;gapminder&lt;/a&gt; dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nplyr)

gm_nest &amp;lt;-
  gapminder::gapminder_unfiltered %&amp;gt;%
  tidyr::nest(country_data = -continent)

gm_nest&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 2
##   continent country_data        
##   &amp;lt;fct&amp;gt;     &amp;lt;list&amp;gt;              
## 1 Asia      &amp;lt;tibble [578 × 5]&amp;gt;  
## 2 Europe    &amp;lt;tibble [1,302 × 5]&amp;gt;
## 3 Africa    &amp;lt;tibble [637 × 5]&amp;gt;  
## 4 Americas  &amp;lt;tibble [470 × 5]&amp;gt;  
## 5 FSU       &amp;lt;tibble [139 × 5]&amp;gt;  
## 6 Oceania   &amp;lt;tibble [187 × 5]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;dplyr can perform operations on the top-level data frame, but with nplyr, we can perform operations on the nested data frames:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gm_nest_example &amp;lt;-
  gm_nest %&amp;gt;%
  nest_filter(country_data, year == max(year)) %&amp;gt;%
  nest_mutate(country_data, pop_millions = pop/1000000)

# each nested tibble is now filtered to the most recent year
gm_nest_example&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 2
##   continent country_data     
##   &amp;lt;fct&amp;gt;     &amp;lt;list&amp;gt;           
## 1 Asia      &amp;lt;tibble [43 × 6]&amp;gt;
## 2 Europe    &amp;lt;tibble [34 × 6]&amp;gt;
## 3 Africa    &amp;lt;tibble [53 × 6]&amp;gt;
## 4 Americas  &amp;lt;tibble [33 × 6]&amp;gt;
## 5 FSU       &amp;lt;tibble [9 × 6]&amp;gt; 
## 6 Oceania   &amp;lt;tibble [11 × 6]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# if we unnest, we can see that a new column for pop_millions has been created
gm_nest_example %&amp;gt;%
  slice_head(n = 1) %&amp;gt;%
  tidyr::unnest(country_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 43 × 7
##    continent country           year lifeExp        pop gdpPercap pop_millions
##    &amp;lt;fct&amp;gt;     &amp;lt;fct&amp;gt;            &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;      &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
##  1 Asia      Afghanistan       2007    43.8   31889923      975.       31.9  
##  2 Asia      Azerbaijan        2007    67.5    8017309     7709.        8.02 
##  3 Asia      Bahrain           2007    75.6     708573    29796.        0.709
##  4 Asia      Bangladesh        2007    64.1  150448339     1391.      150.   
##  5 Asia      Bhutan            2007    65.6    2327849     4745.        2.33 
##  6 Asia      Brunei            2007    77.1     386511    48015.        0.387
##  7 Asia      Cambodia          2007    59.7   14131858     1714.       14.1  
##  8 Asia      China             2007    73.0 1318683096     4959.     1319.   
##  9 Asia      Hong Kong, China  2007    82.2    6980412    39725.        6.98 
## 10 Asia      India             2007    64.7 1110396331     2452.     1110.   
## # … with 33 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;nplyr also supports grouped operations with &lt;code&gt;nest_group_by()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gm_nest_example &amp;lt;-
  gm_nest %&amp;gt;%
  nest_group_by(country_data, year) %&amp;gt;%
  nest_summarise(
    country_data,
    n = n(),
    lifeExp = median(lifeExp),
    pop = median(pop),
    gdpPercap = median(gdpPercap)
  )

gm_nest_example&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 2
##   continent country_data     
##   &amp;lt;fct&amp;gt;     &amp;lt;list&amp;gt;           
## 1 Asia      &amp;lt;tibble [58 × 5]&amp;gt;
## 2 Europe    &amp;lt;tibble [58 × 5]&amp;gt;
## 3 Africa    &amp;lt;tibble [13 × 5]&amp;gt;
## 4 Americas  &amp;lt;tibble [57 × 5]&amp;gt;
## 5 FSU       &amp;lt;tibble [44 × 5]&amp;gt;
## 6 Oceania   &amp;lt;tibble [56 × 5]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# unnesting shows summarised tibbles for each continent
gm_nest_example %&amp;gt;%
  slice(2) %&amp;gt;%
  tidyr::unnest(country_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 58 × 6
##    continent  year     n lifeExp      pop gdpPercap
##    &amp;lt;fct&amp;gt;     &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 Europe     1950    22    65.8 7408264      6343.
##  2 Europe     1951    18    65.7 7165515      6509.
##  3 Europe     1952    31    65.9 7124673      5210.
##  4 Europe     1953    17    67.3 7346100      6774.
##  5 Europe     1954    17    68.0 7423300      7046.
##  6 Europe     1955    17    68.5 7499400      7817.
##  7 Europe     1956    17    68.5 7575800      8224.
##  8 Europe     1957    31    67.5 7363802      6093.
##  9 Europe     1958    18    69.6 8308052.     8833.
## 10 Europe     1959    18    69.6 8379664.     9088.
## # … with 48 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;other-use-cases&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other use cases&lt;/h3&gt;
&lt;p&gt;In the previous set of examples, the output from nplyr’s nested operations could be obtained by unnesting and performing grouped dplyr operations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# we can use nplyr to perform operations on the nested data
gm_nest %&amp;gt;%
  nest_filter(country_data, year == max(year)) %&amp;gt;%
  nest_mutate(country_data, pop_millions = pop/1000000) %&amp;gt;%
  slice_head(n = 1) %&amp;gt;%
  tidyr::unnest(country_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 43 × 7
##    continent country           year lifeExp        pop gdpPercap pop_millions
##    &amp;lt;fct&amp;gt;     &amp;lt;fct&amp;gt;            &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;      &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
##  1 Asia      Afghanistan       2007    43.8   31889923      975.       31.9  
##  2 Asia      Azerbaijan        2007    67.5    8017309     7709.        8.02 
##  3 Asia      Bahrain           2007    75.6     708573    29796.        0.709
##  4 Asia      Bangladesh        2007    64.1  150448339     1391.      150.   
##  5 Asia      Bhutan            2007    65.6    2327849     4745.        2.33 
##  6 Asia      Brunei            2007    77.1     386511    48015.        0.387
##  7 Asia      Cambodia          2007    59.7   14131858     1714.       14.1  
##  8 Asia      China             2007    73.0 1318683096     4959.     1319.   
##  9 Asia      Hong Kong, China  2007    82.2    6980412    39725.        6.98 
## 10 Asia      India             2007    64.7 1110396331     2452.     1110.   
## # … with 33 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# in this case, we could have obtained the same result with tidyr and dplyr
gm_nest %&amp;gt;%
  tidyr::unnest(country_data) %&amp;gt;%
  group_by(continent) %&amp;gt;%
  filter(year == max(year)) %&amp;gt;%
  mutate(pop_millions = pop/1000000) %&amp;gt;%
  ungroup() %&amp;gt;%
  filter(continent == &amp;quot;Asia&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 43 × 7
##    continent country           year lifeExp        pop gdpPercap pop_millions
##    &amp;lt;fct&amp;gt;     &amp;lt;fct&amp;gt;            &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;      &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
##  1 Asia      Afghanistan       2007    43.8   31889923      975.       31.9  
##  2 Asia      Azerbaijan        2007    67.5    8017309     7709.        8.02 
##  3 Asia      Bahrain           2007    75.6     708573    29796.        0.709
##  4 Asia      Bangladesh        2007    64.1  150448339     1391.      150.   
##  5 Asia      Bhutan            2007    65.6    2327849     4745.        2.33 
##  6 Asia      Brunei            2007    77.1     386511    48015.        0.387
##  7 Asia      Cambodia          2007    59.7   14131858     1714.       14.1  
##  8 Asia      China             2007    73.0 1318683096     4959.     1319.   
##  9 Asia      Hong Kong, China  2007    82.2    6980412    39725.        6.98 
## 10 Asia      India             2007    64.7 1110396331     2452.     1110.   
## # … with 33 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why, then, might we need to use nplyr? Well, in other scenarios, it may be far more convenient to work with nested data frames or it may not even be possible to unnest!&lt;/p&gt;
&lt;p&gt;Consider a set of surveys that an organization might use to gather market data. It is common for organization to have separate surveys for separate purposes but to gather the same baseline set of data across all surveys (for example , a respondent’s age and gender may be recorded across all surveys, but each survey will have a different set of questions). Let’s use two fake surveys with the below questions for this example:&lt;/p&gt;
&lt;div id=&#34;survey-1-job&#34; class=&#34;section level6&#34;&gt;
&lt;h6&gt;Survey 1: Job&lt;/h6&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How old are you? (multiple choice)&lt;/li&gt;
&lt;li&gt;What city do you live in? (multiple choice)&lt;/li&gt;
&lt;li&gt;What field do you work in? (multiple choice)&lt;/li&gt;
&lt;li&gt;Overall, how satisfied are you with your job? (multiple choice)&lt;/li&gt;
&lt;li&gt;What is your annual salary? (numeric entry)&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;survey-2-personal-life&#34; class=&#34;section level6&#34;&gt;
&lt;h6&gt;Survey 2: Personal Life&lt;/h6&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How old are you? (multiple choice)&lt;/li&gt;
&lt;li&gt;What city do you live in? (multiple choice)&lt;/li&gt;
&lt;li&gt;What field do you work in? (mulitple choice)&lt;/li&gt;
&lt;li&gt;Overall, how satisfied are you with your personal life (multiple choice)&lt;/li&gt;
&lt;li&gt;Please provide any additional detail (text entry)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this scenario, both surveys are collecting demographic information — age, location, and industry — but differ in the remaining questions. A convenient way to get the response files into the environment would be to use &lt;code&gt;purrr::map()&lt;/code&gt; to read each file to a nested data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;path &amp;lt;- &amp;quot;https://raw.githubusercontent.com/markjrieke/nplyr/main/data-raw/&amp;quot;

surveys &amp;lt;- 
  tibble::tibble(survey_file = c(&amp;quot;job_survey&amp;quot;, &amp;quot;personal_survey&amp;quot;)) %&amp;gt;%
  mutate(survey_data = purrr::map(survey_file, ~readr::read_csv(paste0(path, .x, &amp;quot;.csv&amp;quot;))))

surveys&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 2
##   survey_file     survey_data            
##   &amp;lt;chr&amp;gt;           &amp;lt;list&amp;gt;                 
## 1 job_survey      &amp;lt;spec_tbl_df [500 × 6]&amp;gt;
## 2 personal_survey &amp;lt;spec_tbl_df [750 × 6]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;tidyr::unnest()&lt;/code&gt; can usually handle idiosyncracies in layout when unnesting, but in this case unnesting throws an error!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;surveys %&amp;gt;%
  tidyr::unnest(survey_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error:
## ! Can&amp;#39;t combine `Q5` &amp;lt;double&amp;gt; and `Q5` &amp;lt;character&amp;gt;.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is because the surveys share column names but not necessarily column types! In this case, both data frames contain a column named &lt;code&gt;Q5&lt;/code&gt;, but in &lt;code&gt;job_survey&lt;/code&gt; it’s a double and in &lt;code&gt;personal_survey&lt;/code&gt; it’s a character.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;surveys %&amp;gt;%
  slice(1) %&amp;gt;%
  tidyr::unnest(survey_data) %&amp;gt;%
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 500
## Columns: 7
## $ survey_file &amp;lt;chr&amp;gt; &amp;quot;job_survey&amp;quot;, &amp;quot;job_survey&amp;quot;, &amp;quot;job_survey&amp;quot;, &amp;quot;job_survey&amp;quot;, &amp;quot;j…
## $ survey_name &amp;lt;chr&amp;gt; &amp;quot;job&amp;quot;, &amp;quot;job&amp;quot;, &amp;quot;job&amp;quot;, &amp;quot;job&amp;quot;, &amp;quot;job&amp;quot;, &amp;quot;job&amp;quot;, &amp;quot;job&amp;quot;, &amp;quot;job&amp;quot;, &amp;quot;j…
## $ Q1          &amp;lt;dbl&amp;gt; 100, 81, 51, 81, 80, 32, 65, 57, 43, 94, 25, 83, 61, 66, 8…
## $ Q2          &amp;lt;chr&amp;gt; &amp;quot;Austin&amp;quot;, &amp;quot;San Antonio&amp;quot;, &amp;quot;Austin&amp;quot;, &amp;quot;Austin&amp;quot;, &amp;quot;Dallas&amp;quot;, &amp;quot;Fo…
## $ Q3          &amp;lt;chr&amp;gt; &amp;quot;Consulting&amp;quot;, &amp;quot;Consulting&amp;quot;, &amp;quot;Consulting&amp;quot;, &amp;quot;Technology&amp;quot;, &amp;quot;C…
## $ Q4          &amp;lt;chr&amp;gt; &amp;quot;Somewhat dissatisfied&amp;quot;, &amp;quot;Neither satisfied nor dissatisfi…
## $ Q5          &amp;lt;dbl&amp;gt; 163, 48, 190, 25, 143, 233, 43, 243, 158, 235, 245, 195, 2…&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;surveys %&amp;gt;%
  slice(2) %&amp;gt;%
  tidyr::unnest(survey_data) %&amp;gt;%
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 750
## Columns: 7
## $ survey_file &amp;lt;chr&amp;gt; &amp;quot;personal_survey&amp;quot;, &amp;quot;personal_survey&amp;quot;, &amp;quot;personal_survey&amp;quot;, &amp;quot;…
## $ survey_name &amp;lt;chr&amp;gt; &amp;quot;personal&amp;quot;, &amp;quot;personal&amp;quot;, &amp;quot;personal&amp;quot;, &amp;quot;personal&amp;quot;, &amp;quot;personal&amp;quot;…
## $ Q1          &amp;lt;dbl&amp;gt; 91, 32, 40, 23, 88, 69, 96, 40, 57, 40, 39, 70, 29, 38, 57…
## $ Q2          &amp;lt;chr&amp;gt; &amp;quot;Austin&amp;quot;, &amp;quot;San Antonio&amp;quot;, &amp;quot;San Antonio&amp;quot;, &amp;quot;Austin&amp;quot;, &amp;quot;Dallas&amp;quot;…
## $ Q3          &amp;lt;chr&amp;gt; &amp;quot;Energy&amp;quot;, &amp;quot;Healthcare&amp;quot;, &amp;quot;Consulting&amp;quot;, &amp;quot;Consulting&amp;quot;, &amp;quot;Techn…
## $ Q4          &amp;lt;chr&amp;gt; &amp;quot;Neither satisfied nor dissatisfied&amp;quot;, &amp;quot;Extremely satisfied…
## $ Q5          &amp;lt;chr&amp;gt; &amp;quot;Blandit eros! A, ligula facilisis imperdiet! Interdum pla…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could potentially get around this issue with unnesting by reading in all columns as characters via &lt;code&gt;readr::read_csv(x, col_types = cols(.default = &#34;c&#34;))&lt;/code&gt;, but this presents its own challenges. &lt;code&gt;Q5&lt;/code&gt; would still be better represented as a double in &lt;code&gt;job_survey&lt;/code&gt; and, from the survey question text, &lt;code&gt;Q4&lt;/code&gt; has similar, but distinctly different, meanings across the survey files.&lt;/p&gt;
&lt;p&gt;This is where nplyr can assist! Rather than malign the data types or create separate objects for each survey file, we can use nplyr to perform operations directly on the nested data frames.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;surveys &amp;lt;- 
  surveys %&amp;gt;%
  nest_mutate(survey_data,
              age_group = if_else(Q1 &amp;lt; 65, &amp;quot;Adult&amp;quot;, &amp;quot;Retirement Age&amp;quot;)) %&amp;gt;%
  nest_group_by(survey_data, Q3) %&amp;gt;%
  nest_add_count(survey_data, 
                 name = &amp;quot;n_respondents_in_industry&amp;quot;) %&amp;gt;%
  nest_mutate(survey_data, 
              median_industry_age = median(Q1)) %&amp;gt;%
  nest_ungroup(survey_data)

surveys %&amp;gt;%
  slice(1) %&amp;gt;%
  tidyr::unnest(survey_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 500 × 10
##    survey_file survey_name    Q1 Q2          Q3            Q4       Q5 age_group
##    &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    
##  1 job_survey  job           100 Austin      Consulting    Some…   163 Retireme…
##  2 job_survey  job            81 San Antonio Consulting    Neit…    48 Retireme…
##  3 job_survey  job            51 Austin      Consulting    Extr…   190 Adult    
##  4 job_survey  job            81 Austin      Technology    Extr…    25 Retireme…
##  5 job_survey  job            80 Dallas      Consulting    Extr…   143 Retireme…
##  6 job_survey  job            32 Fort Worth  Energy        Some…   233 Adult    
##  7 job_survey  job            65 Dallas      Consulting    Some…    43 Retireme…
##  8 job_survey  job            57 Houston     Healthcare    Some…   243 Adult    
##  9 job_survey  job            43 Dallas      Government S… Neit…   158 Adult    
## 10 job_survey  job            94 Fort Worth  Healthcare    Extr…   235 Retireme…
## # … with 490 more rows, and 2 more variables: n_respondents_in_industry &amp;lt;int&amp;gt;,
## #   median_industry_age &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;surveys %&amp;gt;%
  slice(2) %&amp;gt;%
  tidyr::unnest(survey_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 750 × 10
##    survey_file     survey_name    Q1 Q2          Q3        Q4    Q5    age_group
##    &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;    
##  1 personal_survey personal       91 Austin      Energy    Neit… Blan… Retireme…
##  2 personal_survey personal       32 San Antonio Healthca… Extr… Elem… Adult    
##  3 personal_survey personal       40 San Antonio Consulti… Some… Eget… Adult    
##  4 personal_survey personal       23 Austin      Consulti… Extr… Scel… Adult    
##  5 personal_survey personal       88 Dallas      Technolo… Neit… Aene… Retireme…
##  6 personal_survey personal       69 Fort Worth  Technolo… Neit… Inte… Retireme…
##  7 personal_survey personal       96 Houston     Healthca… Extr… Blan… Retireme…
##  8 personal_survey personal       40 Houston     Consulti… Extr… Scel… Adult    
##  9 personal_survey personal       57 Fort Worth  Energy    Extr… Pede… Adult    
## 10 personal_survey personal       40 Fort Worth  Healthca… Extr… Phar… Adult    
## # … with 740 more rows, and 2 more variables: n_respondents_in_industry &amp;lt;int&amp;gt;,
## #   median_industry_age &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Math Behind workboots</title>
      <link>https://www.thedatadiary.net/blog/2022-07-05-the-math-behind-workboots/</link>
      <pubDate>Tue, 05 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-07-05-the-math-behind-workboots/</guid>
      <description>


&lt;p&gt;Generating prediction intervals with workboots hinges on a few core concepts: bootstrap resampling, estimating prediction error for each resample, and aggregating the resampled prediction errors for each observation. The &lt;a href=&#34;https://rsample.tidymodels.org/reference/bootstraps.html&#34;&gt;&lt;code&gt;bootstraps()&lt;/code&gt; documentation from {rsample}&lt;/a&gt; gives a concise definition of bootstrap resampling:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A bootstrap sample is a sample that is the same size as the original data set that is made using replacement. This results in analysis samples that have multiple replicates of some of the original rows of the data. The assessment set is defined as the rows of the original data that were not included in the bootstrap sample. This is often referred to as the “out-of-bag” (OOB) sample.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This vignette will walk through the details of estimating and aggregating prediction errors — additional resources can be found in Davison and Hinkley’s book, &lt;a href=&#34;https://www.cambridge.org/core/books/bootstrap-methods-and-their-application/ED2FD043579F27952363566DC09CBD6A&#34;&gt;&lt;em&gt;Bootstrap Methods and their Application&lt;/em&gt;&lt;/a&gt;, or Efron and Tibshirani’s paper, &lt;a href=&#34;https://www.jstor.org/stable/2965703&#34;&gt;&lt;em&gt;Improvements on Cross-Validation: The Bootstrap .632+ Method&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;the-bootstrap-.632-method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Bootstrap .632+ Method&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;What follows here is largely a summary of &lt;a href=&#34;https://stats.stackexchange.com/questions/96739/what-is-the-632-rule-in-bootstrapping/96750#96750&#34;&gt;this explanation&lt;/a&gt; of the .632+ error rate by Benjamin Deonovic.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When working with bootstrap resamples of a dataset, there are two error estimates we can work with: the bootstrap training error and the out-of-bag (oob) error. Using the &lt;a href=&#34;https://modeldata.tidymodels.org/reference/Sacramento.html&#34;&gt;Sacramento housing dataset&lt;/a&gt;, we can estimate the training and oob error for a single bootstrap.&lt;/p&gt;
&lt;pre class=&#34;r fold-show&#34;&gt;&lt;code&gt;sacramento_boots
#&amp;gt; # Bootstrap sampling 
#&amp;gt; # A tibble: 1 × 2
#&amp;gt;   splits            id        
#&amp;gt;   &amp;lt;list&amp;gt;            &amp;lt;chr&amp;gt;     
#&amp;gt; 1 &amp;lt;split [699/261]&amp;gt; Bootstrap1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using a &lt;a href=&#34;https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#k-NN_regression&#34;&gt;k-nearest-neighbor regression model&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Root-mean-square_deviation#:~:text=The%20root%2Dmean%2Dsquare%20deviation,estimator%20and%20the%20values%20observed.&#34;&gt;rmse&lt;/a&gt; as our error metric, we find that the training and oob error differ, with the training error lesser than the oob error.&lt;/p&gt;
&lt;pre class=&#34;r fold-show&#34;&gt;&lt;code&gt;sacramento_train_err
#&amp;gt; [1] 0.08979873
sacramento_oob_err
#&amp;gt; [1] 0.1661675&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The training error is overly optimistic in the model’s performance and likely to under-estimate the prediction error. We are interested in the model’s performance on new data. The oob error, on the other hand, is likely to over-estimate the prediction error! This is due to non-distinct observations in the bootstrap sample that results from sampling with replacement. Given that &lt;a href=&#34;https://stats.stackexchange.com/questions/88980/why-on-average-does-each-bootstrap-sample-contain-roughly-two-thirds-of-observat?lq=1&#34;&gt;the average number of distinct observations in a bootstrap training set is about &lt;code&gt;0.632 * total_observations&lt;/code&gt;&lt;/a&gt;, Efron and Tibshirani proposed a blend of the training and oob error with the 0.632 estimate:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
Err_{.632} &amp;amp; = 0.368 Err_{train} + 0.632 Err_{oob}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r fold-show&#34;&gt;&lt;code&gt;sacramento_632 &amp;lt;- 0.368 * sacramento_train_err + 0.632 * sacramento_oob_err
sacramento_632
#&amp;gt; [1] 0.1380638&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If, however, the model is highly overfit to the bootstrap training set, the training error will approach 0 and the 0.632 estimate will &lt;em&gt;under estimate&lt;/em&gt; the prediction error.&lt;/p&gt;
&lt;p&gt;An example from &lt;a href=&#34;http://appliedpredictivemodeling.com/&#34;&gt;&lt;em&gt;Applied Predictive Modeling&lt;/em&gt;&lt;/a&gt; shows that as model complexity increases, the reported resample accuracy by the 0.632 estimate continues to increase whereas other resampling strategies report diminishing returns:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5731043/157986232-9c32c1c2-a7ed-4f9f-b28e-7d8ccb7ac41c.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As an alternative to the 0.632 estimate, Efron &amp;amp; Tibshirani also propose the 0.632+ estimate, which re-weights the blend of training and oob error based on the model overfit rate:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
Err_{0.632+} &amp;amp; = (1 - w) Err_{train} + w Err_{oob} \\
\\
w &amp;amp; = \frac{0.632}{1 - 0.368 R} \\
\\
R &amp;amp; = \frac{Err_{oob} - Err_{train}}{\gamma - Err_{train}}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; represents the overfit rate and &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is the no-information error rate, estimated by evaulating all combinations of predictions and actual values in the bootstrap training set.&lt;/p&gt;
&lt;pre class=&#34;r fold-show&#34;&gt;&lt;code&gt;sacramento_632_plus &amp;lt;- (1 - w) * sacramento_train_err + w * sacramento_oob_err
sacramento_632_plus
#&amp;gt; [1] 0.1450502&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When there is no overfitting (i.e., &lt;span class=&#34;math inline&#34;&gt;\(R = 0\)&lt;/span&gt;) the 0.632+ estimate will equal the 0.632 estimate. In this case, however, the model is overfitting the training set and the 0.632+ error estimate is pushed a bit closer to the oob error.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prediction-intervals-with-many-bootstraps&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Prediction intervals with many bootstraps&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Root-mean-square_deviation#Formula&#34;&gt;For an unbiased estimator, rmse is the standard deviation of the residuals&lt;/a&gt;. With this in mind, we can modify our predictions to include a sample from the residual distribution (for more information, see Algorithm 6.4 from Davison and Hinkley’s &lt;em&gt;Bootstrap Methods and their Application&lt;/em&gt;):&lt;/p&gt;
&lt;pre class=&#34;r fold-show&#34;&gt;&lt;code&gt;set.seed(999)
resid_train_add &amp;lt;- rnorm(length(preds_train), 0, sacramento_632_plus)
preds_train_mod &amp;lt;- preds_train + resid_train_add&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus far, we’ve been working with a single bootstrap resample. When working with a single bootstrap resample, adding this residual term gives a pretty poor estimate for each observation:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-07-05-the-math-behind-workboots/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With workboots, however, we can repeat this process over many bootstrap datasets to generate a prediction distribution for each observation:&lt;/p&gt;
&lt;pre class=&#34;r fold-show&#34;&gt;&lt;code&gt;library(workboots)
# fit and predict price in sacramento_test from 100 models
# the default number of resamples is 2000 - dropping here to speed up knitting
set.seed(555)
sacramento_pred_int &amp;lt;-
  sacramento_wf %&amp;gt;%
  predict_boots(
    n = 100,
    training_data = sacramento_train,
    new_data = sacramento_test
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-07-05-the-math-behind-workboots/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This methodology produces prediction distributions that are &lt;a href=&#34;https://markjrieke.github.io/workboots/articles/Estimating-Linear-Intervals.html&#34;&gt;consistent with what we might expect from linear models&lt;/a&gt; while making no assumptions about model type (i.e., we can use a non-parametric model; in this case, a k-nearest neighbors regression).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Estimate your uncertainty</title>
      <link>https://www.thedatadiary.net/blog/2022-06-12-estimate-your-uncertainty/</link>
      <pubDate>Sun, 12 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-06-12-estimate-your-uncertainty/</guid>
      <description>


&lt;p&gt;I recently picked up &lt;a href=&#34;http://varianceexplained.org/about/&#34;&gt;David Robinson’s&lt;/a&gt; book, &lt;a href=&#34;http://varianceexplained.org/r/empirical-bayes-book/&#34;&gt;Introduction to Empirical Bayes&lt;/a&gt;. It’s available online for a price of your own choosing (operating under a “pay-what-you-want” model), so you can technically pick it up for free, but it’s well worth the suggested price of $9.95. The book has a particular focus on practical steps for implementing Bayesian methods with code, which I appreciate. I’ve made it through Part I (of four), which makes for a good stopping point to practice what I’ve read.&lt;/p&gt;
&lt;p&gt;The first section is highly focused on modeling the probability of success/failure of some binary outcome using a &lt;a href=&#34;https://en.wikipedia.org/wiki/Beta_distribution&#34;&gt;beta distribution&lt;/a&gt;. This is highly relevant to my work as an analyst, where whether or not a patient responded positively to a particular question on a survey can be modeled with this method. Thus far, however, I’ve taken the &lt;a href=&#34;https://en.wikipedia.org/wiki/Frequentist_inference&#34;&gt;frequentist&lt;/a&gt; approach to analyses, which assumes we know nothing about what the data ought to look like prior to analyzing it. This is largely because I didn’t know of a robust way to estimate a &lt;a href=&#34;https://en.wikipedia.org/wiki/Prior_probability&#34;&gt;prior&lt;/a&gt; for a Bayesian analysis.&lt;/p&gt;
&lt;p&gt;Thankfully, however, the book walks through examples of exactly how to do this! We can use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&#34;&gt;maximum likelihood estimator&lt;/a&gt; to estimate a reasonable prior given the current data. That’s quite a bit of statistical mumbo-jumbo — in this post I’ll walk through an example that spells it out a bit more clearly using fake hospital satisfaction data (N.B.; this is largely a recreation of the steps taken in the book — practice makes perfect!).&lt;/p&gt;
&lt;div id=&#34;setting-up-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting up the data&lt;/h2&gt;
&lt;p&gt;First, let’s simulate responses to patient satisfaction surveys. I tend to look at patient satisfaction scores across individual hospital units (e.g., ED, ICU, IMU, etc.). Units can have varying numbers of discharges, so we’ll use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Log-normal_distribution&#34;&gt;log-normal&lt;/a&gt; distribution to estimate the number of responses for each unit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# simulate 1,500 hospital units with an average of 150 survey returns per unit
set.seed(123)
survey_data &amp;lt;- 
  rlnorm(1500, log(150), 1.5) %&amp;gt;%
  as_tibble() %&amp;gt;%
  rename(n = value)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-06-12-estimate-your-uncertainty/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The spectrum of responses is incredibly broad — some units have a massive number of returns (in the tens of thousands!) while others have just a handful. This is fairly consistent with the real-world data that I’ve seen (though the units on the high-side are a bit over-represented here).&lt;/p&gt;
&lt;p&gt;Next, let’s assume that there is some true satisfaction rate that is associated with each unit. If each unit had an infinite number of survey returns, the satisfaction rate from the survey returns would approach this true value. In this case, we’ll set the true satisfaction for each unit randomly but have it hover around 66%.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set the true satisfaction to be different for each unit, but hover around 66%
set.seed(234)
survey_data &amp;lt;- 
  survey_data %&amp;gt;%
  rowwise() %&amp;gt;%
  mutate(true_satisfaction = rbeta(1, 66, 34))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although there is a true satisfaction associated with each unit, we wouldn’t expect that the reported survey scores would match this exactly. This is especially true when there are few responses — if a unit has a true satisfaction rate of 75% but only 3 responses, it’s impossible for the reported score to match the underlying true rate!&lt;/p&gt;
&lt;p&gt;We can simulate the number of patients who responded positively (in survey terms, the number of “topbox” responses) by generating &lt;code&gt;n&lt;/code&gt; responses for each unit using a &lt;a href=&#34;https://en.wikipedia.org/wiki/Binomial_distribution&#34;&gt;binomial distribution&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# simulate the number of patients responding with the topbox value
# we *know* the true value, but the actual score may vary!
set.seed(345)
survey_data &amp;lt;-
  survey_data %&amp;gt;%
  mutate(n = round(n),
         topbox = rbinom(1, n, true_satisfaction)) %&amp;gt;%
  ungroup() %&amp;gt;%
  
  # name each unit
  rowid_to_column() %&amp;gt;%
  mutate(unit = paste(&amp;quot;Unit&amp;quot;, rowid)) %&amp;gt;%
  relocate(unit) %&amp;gt;%
  
  # remove the true satisfaction so we don&amp;#39;t know what it is!
  select(-rowid, -true_satisfaction)

# find patient satisfaction scores
survey_data &amp;lt;- 
  survey_data %&amp;gt;%
  mutate(score = topbox/n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-06-12-estimate-your-uncertainty/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, most of our simulated data hovers around a score of 66%. However, there are a few scores at the extremes of 0% and 100% — given how we simulated the data, it is unlikely that these units are &lt;em&gt;really&lt;/em&gt; performing so poorly/so well and it’s likelier that they just have few returns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# which units have the highest scores?
survey_data %&amp;gt;%
  arrange(desc(score)) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;unit&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;topbox&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 26&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 591&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 616&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 811&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 943&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1217&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1435&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1437&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 863&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9473684&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 372&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9230769&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# which units have the lowest scores?
survey_data %&amp;gt;%
  arrange(score) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;unit&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;topbox&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1092&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 248&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2500000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1120&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2857143&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 416&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3333333&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 456&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3333333&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 972&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3333333&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 113&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3846154&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 260&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 695&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1352&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4117647&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As expected, the units on either end of the spectrum aren’t necessarily outperforming/underperforming — they simply don’t have a lot of survey responses! We can use Bayesian inference to estimate the true satisfaction rate by specifying and updating a prior!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generating-a-prior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generating a prior distribution&lt;/h2&gt;
&lt;p&gt;When looking at the entire dataset, the distribution of scores is thrown off a bit by the units with few responses. If we restrict the dataset to only the units that have more than 30 responses (which, &lt;a href=&#34;https://www.thedatadiary.net/blog/2022-04-28-30-is-not-statistical/&#34;&gt;as I’ve written about before&lt;/a&gt;, isn’t necessarily a data-driven cutoff for analysis) we can get a clearer idea of the distribution of the scores.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;survey_data_filtered &amp;lt;-
  survey_data %&amp;gt;%
  filter(n &amp;gt; 30)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-06-12-estimate-your-uncertainty/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Alternatively, we can represent this distribution with a density plot:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-06-12-estimate-your-uncertainty/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This looks suspiciously like a beta distribution! A beta distribution’s shape can be defined by two parameters — alpha and beta. Varying these parameters lets us adjust the center and width to match any possible beta distribution.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/7/78/PDF_of_the_Beta_distribution.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What may make sense would be to use &lt;em&gt;this distribution&lt;/em&gt; as our prior. I.e., if we have no responses for a unit, we can probably guess that their score would be somewhere around 66% with some healthy room on either side for variability. To do so, we need to estimate an appropriate alpha and beta — rather than guess the values using trial and error we can pass the work off to our computer to find parameters that &lt;strong&gt;maximize&lt;/strong&gt; the &lt;strong&gt;likelihood&lt;/strong&gt; that our &lt;strong&gt;estimated distribution&lt;/strong&gt; matches the true distribution (hence the name, &lt;em&gt;maximum likelihood estimator&lt;/em&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stats4)

# log-likelihood function
log_likelihood &amp;lt;- function(alpha, beta) {
  -sum(dbeta(survey_data_filtered$score, alpha, beta, log = TRUE))
}

# pass various alphas &amp;amp; betas to `log_likelihood` 
# to find combination that maximizes the likelihood!
params &amp;lt;- 
  mle(
    log_likelihood, 
    start = list(alpha = 50, beta = 50),
    lower = c(1, 1)
  )

# extract alpha &amp;amp; beta
params &amp;lt;- coef(params)
alpha0 &amp;lt;- params[1]
beta0 &amp;lt;- params[2]

print(paste(&amp;quot;alpha:&amp;quot;, round(alpha0, 1), &amp;quot;beta:&amp;quot;, round(beta0, 1)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;alpha: 39.7 beta: 20.5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How well does a beta distribution defined by these parameters match our actual data?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-06-12-estimate-your-uncertainty/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a pretty good representation of our initial data! When we have no survey responses, we can use a beta distribution with the initial parameters as specified by the maximum likelihood estimation. As a unit gets more responses, we can update our estimation to rely more heavily on the data rather than the prior:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# update alpha &amp;amp; beta as new responses come in!
alpha_new &amp;lt;- alpha0 + n_topbox
beta_new &amp;lt;- beta0 + n - n_topbox&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;updating-our-priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Updating our priors&lt;/h2&gt;
&lt;p&gt;With a prior distribution defined by &lt;code&gt;alpha0&lt;/code&gt; and &lt;code&gt;beta0&lt;/code&gt;, we can upgrade our frequentest estimation of each unit’s score to a Bayesian estimation!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# empirical bayes estimation of satisfaction score
survey_eb &amp;lt;-
  survey_data %&amp;gt;%
  mutate(eb_estimate = (topbox + alpha0) / (n + alpha0 + beta0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What are the top and bottom performing units by this new Bayesian estimation?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# which units have the highest estimated scores?
survey_eb %&amp;gt;%
  arrange(desc(eb_estimate)) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;unit&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;topbox&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;score&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;eb_estimate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 133&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;133&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8312500&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7841640&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1004&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;123&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;103&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8373984&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7787827&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 172&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;165&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;133&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8060606&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7667547&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1042&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;372&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;291&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7822581&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7650930&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1294&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1409&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1083&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7686302&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7641391&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 892&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;349&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;273&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7822350&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7641085&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 306&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;247&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;195&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7894737&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7639102&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1249&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1234&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;943&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7641815&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7592901&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 427&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5469&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4151&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7590053&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7579168&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 920&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1637&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1243&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7593158&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7557585&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# which units have the lowest estimated scores?
survey_eb %&amp;gt;%
  arrange(eb_estimate) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;unit&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;topbox&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;score&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;eb_estimate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 613&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1886&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;932&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4941676&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4992689&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 760&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;112&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;49&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4375000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5149645&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 363&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;226&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;112&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4955752&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5299674&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 316&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;431&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;224&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5197216&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5368008&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1032&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;235&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;119&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5063830&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5375222&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1093&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;354&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;183&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5169492&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5376064&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 749&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5286&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2839&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5370791&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5384528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 291&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;865&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;460&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5317919&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5400741&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 515&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;60&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4333333&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5463929&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 622&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;242&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;127&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5247934&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5515432&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There are a few things that are worth noting with these estimates:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The estimated score is not the same as the actual reported score! As more responses come in, however, the estimated score converges to the actual.&lt;/li&gt;
&lt;li&gt;The prior pulls estimated scores towards the prior mean — low scores are pulled up a bit and high scores are pulled down a bit.&lt;/li&gt;
&lt;li&gt;The top (and bottom) performing units are no longer dominated by units with few returns!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also estimate the uncertainty around the estimated score with a &lt;a href=&#34;https://en.wikipedia.org/wiki/Credible_interval&#34;&gt;credible interval&lt;/a&gt;. Credible intervals are the Bayesian counterpart to a frequentist’s &lt;a href=&#34;https://en.wikipedia.org/wiki/Confidence_interval&#34;&gt;confidence interval&lt;/a&gt; — both estimate the region that the true value could fall in given a certain probability — credible intervals, however, are informed by the prior distribution.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-06-12-estimate-your-uncertainty/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Because credible intervals are informed in part by the prior, they are tighter than their confidence interval counterparts. Like with the estimated score, however, as n-size increases, the Bayesian and frequentist interval estimations converge. In the absence of larger swathes of data, Bayesian methods can offer additional insight into our data by means of a prior distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-closing-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some closing thoughts&lt;/h2&gt;
&lt;p&gt;Overall, this has been a fairly glowing review of the methods laid out in the first section of &lt;em&gt;Introduction to Empirical Bayes&lt;/em&gt;. That being said, Bayesian methods of inference are not inherently &lt;em&gt;better&lt;/em&gt; than frequentist methods — while they can offer additional context via a prior, there are situations where frequentist methods are preferred. From a math perspective, the prior provides diminishing returns as sample size increases, so it may be better forgoe Bayesian analysis when sample sizes are large. From an organizational perspective, Bayesian inference may be difficult to explain. In my own work, it’s highly unlikely that I’ll use Bayesian inference in any critical projects any time soon — I can imagine a lengthy uphill battle trying to explain the difference between the reported score and the estimated score informed by a prior.&lt;/p&gt;
&lt;p&gt;Finally, there are a few things in this toy analysis that I am hoping to improve upon as I progress further through the book:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As I mentioned above and &lt;a href=&#34;https://www.thedatadiary.net/blog/2022-04-28-30-is-not-statistical/&#34;&gt;in previous writings&lt;/a&gt;, using &lt;code&gt;n = 30&lt;/code&gt; is a relatively arbitrary cutoff point for analysis. In this case, the prior distribution is fairly sensitive to the cutoff point selected — I am hoping that later sections in the book highilight more robust ways of partitioning data for setting priors.&lt;/li&gt;
&lt;li&gt;In the above analysis we’re only examining one variable (univariate analysis) — I am looking forward to extending these methods to multivariate analyses and regressions.&lt;/li&gt;
&lt;li&gt;The beta distribution is appropriate for modeling the probability distribution of binary outcomes. In this example, where the outcome is simply the proportion of patients that responded favorably to the survey, modeling the outcome with a beta distribution is appropriate (responses can either be in the “topbox” or not). When there are more than two possible outcomes — for example, when trying to model &lt;a href=&#34;https://en.wikipedia.org/wiki/Net_promoter_score&#34;&gt;Net Promoter Score&lt;/a&gt; as the proportion of “promoters,” “passives,” and “detractors” — the more general &lt;a href=&#34;https://en.wikipedia.org/wiki/Dirichlet_distribution&#34;&gt;Dirichlet distribution&lt;/a&gt; is more appropriate.&lt;/li&gt;
&lt;li&gt;I’m hoping also that the book covers methods for dealing with time-dependent data. For example, we’d expect that concerted efforts (or lack thereof) by the hospital units could significantly impact the underlying “true satisfaction” that we’re attempting to estimate via surveying. We expect that more recent survey responses should be more impactful in informing our posterior estimation, but I’ve yet to find any robust literature on how to weight the recency of responses. In the past, I’ve used &lt;a href=&#34;https://en.wikipedia.org/wiki/Exponential_decay&#34;&gt;exponentional decay&lt;/a&gt; to reduce the weight of old responses, but this feels a bit arbitrary.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overall, this has been a long way of saying that I’m happy with the book so far and I’m excited to see what comes next as I continue reading!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical Data Visualization Tips for Excel Users</title>
      <link>https://www.thedatadiary.net/blog/2022-05-31-practical-data-vizualization-tips-for-excel-users/</link>
      <pubDate>Tue, 31 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-05-31-practical-data-vizualization-tips-for-excel-users/</guid>
      <description>


&lt;p&gt;I am an avid R user and will always advocate that others use R (or another programming language) for generating reproducible visualizations. In just about every organization, however, Excel plays an important role in an analyst’s toolkit. In this post, I’ll share some visualization design practices that I picked up while learning R but are ubiquitous and transferable to Excel (most of these suggestions are ripped directly from &lt;a href=&#34;https://www.williamrchase.com/about/&#34;&gt;Will Chase’s&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=h5cTacaWE6I&#34;&gt;“Glamour of Graphics”&lt;/a&gt; talk from rstudio::conf(2020), which has &lt;em&gt;heavily informed&lt;/em&gt; how I think about visualization design).&lt;/p&gt;
&lt;div id=&#34;a-motivating-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A motivating example&lt;/h2&gt;
&lt;p&gt;For the purposes of this exercise, let’s use fake patient satisfaction data from the Sesame Street Health System, which includes several hospitals and campuses. Let’s say that our system-wide patient satisfaction for the current fiscal year looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/system_px.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Most of the hospitals have pretty high satisfaction scores — generally greater than &lt;strong&gt;75%&lt;/strong&gt;! The overall system score, however, sits at &lt;strong&gt;65%&lt;/strong&gt;. Concerned that there may be an error in the data pipeline or dashboard, your boss asks that you investigate what’s going on and provide an update at the next team meeting.&lt;/p&gt;
&lt;p&gt;At first glance, it may be pretty obvious what’s going on — Big Bird Emergency has a pretty low satisfaction score and you know from experience that it’s a larger hospital that generates a lot of survey returns, which may be driving the score down. Since you’re presenting, however, it’s best to use a visualization to communicate this.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-together-a-bad-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Putting together a bad plot&lt;/h2&gt;
&lt;p&gt;Downloading the hospital data and opening in Excel confirms that Big Bird Emergency has far more survey returns than other hospitals.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/system_raw.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A quick plot that &lt;em&gt;technically&lt;/em&gt; includes all the information needed may look something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_01.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While this does answer the question originally asked, the answer is not &lt;strong&gt;clearly communicated by a quick glance&lt;/strong&gt;. Viewers who know what was originally asked have to do some extra mental work to decode the plot, and viewers who see this without the original context may not find anything useful. Our goal should be to provide a visualization that:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Clearly communicates the message we want to convey.&lt;/li&gt;
&lt;li&gt;Is able to stand alone in other contexts and still communicate the same message.&lt;/li&gt;
&lt;li&gt;Is visually appealing.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s get started!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-together-a-good-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Putting together a good plot&lt;/h2&gt;
&lt;div id=&#34;changing-to-a-bar-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Changing to a bar plot&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_02.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The first order of business is to convert the plot from a pie chart to a bar plot. Pie charts are loved by executives but loathed by visualization practitioners, since information is encoded in each slice’s angle and &lt;a href=&#34;https://en.wikipedia.org/wiki/Graphical_perception&#34;&gt;differences in angle are difficult for the human eye to detect&lt;/a&gt;. Bar plots encode the same information with relative position on a scale, which is the most effective method for showing differences. This also has the added benefit in Excel of automatically converting each hospital to the same color, which reduces a lot of the visual noise that was in the original plot.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;flipping-axes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Flipping axes&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_03.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In order to read the hospital names in the previous plot, viewers need to crane their necks to align with the axis text. The angled text also takes up a lot of whitespace and makes the important part — the actual data — look a bit squished. Changing to a horizontal bar plot alleviates both of these issues (horizontal bar plots are preferred over vertical ones in general for this reason).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ordering-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ordering the data&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_04.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unordered categories in a plot can be messy and visually confusing — the viewer’s eye needs to dart around to determine which values are greater than other ones. Ordering the categories reduces this cognitive load and allows the viewer to simply read through the list. In this case (and in most cases), we don’t care about the exact values (just the &lt;em&gt;relative difference&lt;/em&gt; between values), so we can also remove the data labels.&lt;/p&gt;
&lt;p&gt;To order a plot in Excel, we don’t actually need to do anything to the plot itself — simply turn the raw data into a table then arrange the rows by &lt;code&gt;survey_returns&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/ordered_campuses.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;giving-the-plot-a-narrative&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Giving the plot a narrative&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_05.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The original title, “FYTD Surveys”, while technically informative, is uninspiring. We’re putting together this plot to answer the specific question, &lt;em&gt;why is the system satisfaction score 65% when most hospitals have a higher score?&lt;/em&gt; A good visualization will directly answer this without needing additional context from the analyst — the title is a great place to state that &lt;em&gt;Big Bird Emergency is driving the system score down&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Also note that there are no axis labels on this plot. Axis labels are often unnecessary — they take up valuable whitespace with information that is either readily apparent (I don’t need a label to know that the y-axis refers to each hospital!) or encoded elsewhere. When possible, remove axis labels and describe the necessary detail elsewhere (i.e., the title or subtitle).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;highlighting-the-important-bits&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Highlighting the important bits&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_06.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Color can be a great way to draw our attention to a particular portion of a plot. In this case, not all of the hospitals are equally important in this plot’s narrative — we’re making a distinct point regarding Big Bird Emergency. Highlighting the text and bar for Big Bird Emergency in yellow (Big Bird is, after all, a big yellow bird) while graying out the other bars visually communicates to the viewer this is the hospital deserving the most attention in this plot.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;realigning-the-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Realigning the plot&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_07.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the vast majority of cases, a left-aligned the title/subtitle is strongly preferred over center-aligned one. In western countries, we tend to naturally orient our attention in the top-left corner of plots when we first view them, then migrate our gaze downwards and leftwards (eye-tracking studies confirm this, however I can’t seem to find the source I heard this from, so you’ll just have to take my word for it here). By aligning the title to the left, we reduce how much the viewer needs to dart their eyes around the plot to understand it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-the-final-touches&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding the final touches&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_08.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Adding some final formatting touches to polish up the visualization not only improves the quality of the plot but also shows the viewer that you’re serious about your craft and willing to go the extra mile to really make a visualization shine. In this case, applying comma-formatting to the x-axis, changing the font, and updating the background to an off-white are all minor edits, but their effects have a big impact on the visualization’s overall presentation.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;My buddy is an electrician and told me a few months ago that he always leaves the screws in a vertical position on jobs as a sign of craftsmanship. Been thinking ever since what my “vertical screws” equivalent is for product design. &lt;a href=&#34;https://t.co/dM9CFEG8MF&#34;&gt;pic.twitter.com/dM9CFEG8MF&lt;/a&gt;&lt;/p&gt;&amp;mdash; Mike Rundle (@flyosity) &lt;a href=&#34;https://twitter.com/flyosity/status/1495087213150879747?ref_src=twsrc%5Etfw&#34;&gt;February 19, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>&#34;30 is not Statistical&#34;</title>
      <link>https://www.thedatadiary.net/blog/2022-04-28-30-is-not-statistical/</link>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-04-28-30-is-not-statistical/</guid>
      <description>


&lt;p&gt;In my role as an analyst, my team and I are required to put together reports that summarize each hospital’s patient satisfaction performance in a table. These are reviewed by our system’s executive leadership team and the hospital directors in monthly operational reviews (MORs). The format I inherited, loosely recreated below with fake data, color codes each month’s performance against the hospital’s goal: green when outperforming and red when underperforming.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/table.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;“But wait!” you may ask, “what do the gray cells mean?” &lt;em&gt;That&lt;/em&gt;, dear reader, has been the source of most of my organizational frustration in this role. When the number of surveys returned is less than 30 for a given month, we simply gray-out that cell under the guise of the phrase, “30 is not statistical.”&lt;/p&gt;
&lt;p&gt;I don’t think this practice (or something similar) is unique to my organization — I’ve seen similar outputs from previous employers and in other companies’ published reports. While this isn’t the best use of the underlying data, I understand &lt;em&gt;why&lt;/em&gt; this sort of method gets adapted into so many organizational lexicons: companies want their decisions to be based on data and their understanding, albeit incorrect, is that a sample size less than 30 doesn’t provide meaningful info. For this reason, I think it’s important to explore &lt;strong&gt;where&lt;/strong&gt; this sentiment came from, &lt;strong&gt;what&lt;/strong&gt; the problems with this data-presentation style are, &lt;strong&gt;what&lt;/strong&gt; I think would be a better way of presenting the data, and ultimately &lt;strong&gt;why&lt;/strong&gt; companies may be hesitant to update their methodology.&lt;/p&gt;
&lt;div id=&#34;where-does-this-come-from&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Where does this come from?&lt;/h2&gt;
&lt;p&gt;At first glance, 30 is a pretty arbitrary number to use as a cutoff. In this case, this cutoff can cause downstream issues with interpreting the data because the difference between &lt;code&gt;n = 30&lt;/code&gt; and &lt;code&gt;n = 31&lt;/code&gt; is so visually distinct! In our case, the cutoff of 30 was passed down from one of our previous survey vendors, but I believe that the wider root of why this value appears has to do with the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34;&gt;central limit theorem&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The central limit theorem states that as a sample size increases, the probability distribution of the sample mean approaches a normal distribution, &lt;em&gt;regardless of the source distribution!&lt;/em&gt; As a rule of thumb, this theorem holds true when the &lt;a href=&#34;https://stats.stackexchange.com/questions/2541/what-references-should-be-cited-to-support-using-30-as-a-large-enough-sample-siz&#34;&gt;sample size is at least 30&lt;/a&gt;. In practice, this means that when there are at least 30 samples, we can generally approximate the distribution as normal. The central limit theorem is incredibly useful and an important foundation for a wide array of statistical techniques. Stating that the data doesn’t meet the criteria for the central limit theorem to hold, however, is very different from saying that data is worthless when the sample size is less than 30!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problems-with-this-approach&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Problems with this approach&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Sometimes, the source distribution is known, and whether or not the central limit theorem holds is irrelevant! In my particular case, I am often dealing with patient satisfaction data that lies on a 0-100% scale. This is the perfect use case for modeling the sample with the &lt;a href=&#34;https://en.wikipedia.org/wiki/Beta_distribution&#34;&gt;beta distribution&lt;/a&gt; (which is bound by 0 and 1).&lt;/li&gt;
&lt;li&gt;As mentioned above, graying-out samples where the sample size is less than 30 visually communicates that some months can be ignored. This is a waste of valuable data! While it is true that a larger sample size implies greater confidence in the score, the confidence interval widths for &lt;code&gt;n = 30&lt;/code&gt; and &lt;code&gt;n = 31&lt;/code&gt; are not &lt;em&gt;so different&lt;/em&gt; and we can still estimate the uncertainty with the smaller sample.&lt;/li&gt;
&lt;li&gt;Tabular data is incredibly difficult to parse at-a-glance! &lt;a href=&#34;https://www.rstudio.com/resources/rstudioconf-2020/effective-visualizations/&#34;&gt;Research shows&lt;/a&gt; that spatial encoding (e.g., length, position) is the most interpretable mode of data presentation. Intuitively, it makes sense — there’s a lot less mental overhead involved in looking at a set of points and comparing positions &lt;em&gt;collectively&lt;/em&gt; than stringing together several comparisons of individual pairs of numbers in your head.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;a-better-approach&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A better approach&lt;/h2&gt;
&lt;p&gt;When the underlying distribution is known, a better approach would be to display the data in a plot, regardless of n-size, and use a confidence interval to indicate uncertainty. In this case, we can plot each survey’s scores over time with a line and use a shaded area for the confidence interval.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-04-28-30-is-not-statistical/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This, I believe, has a few benefits.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;This is simply more visually appealing than the table. I (along with most people, I’d assume) prefer the look of a well formatted plot over a well formatted table (even if well formatted, a table is still a big block of text).&lt;/li&gt;
&lt;li&gt;It is far easier to discern the overall trend. Instead of reading and trying to compare values, we can simply see which direction the line is moving!&lt;/li&gt;
&lt;li&gt;Most imortantly, &lt;em&gt;we do not throw out valuable data because of sample size.&lt;/em&gt; We actually end up encoding &lt;em&gt;more&lt;/em&gt; information — n-size, which was missing from the table, is encoded in the width of the confidence interval (a smaller confidence interval indicates a larger sample). In this toy example, surveys B and D included a few months with fewer than 30 returns — can you tell which months they were without looking at the table?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The tradeoff is that we can no longer explicitly see each month’s score and it is a bit harder to tell if a hospital is meeting the goal when the score is close. In my experience, however, this is not how formatted tables are used — executives that I interact with typically try to determine overall trends from tabular data!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;organizational-resistance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Organizational resistance&lt;/h2&gt;
&lt;p&gt;While the changes suggested here have clear benefits over the table, I’ve thus far been unsuccessful in any attempts to change the reporting methodology and I suspect that similar efforts at other companies would encounter similar organizational resistance. Much of what’s stated below is anec-data, but I assume will ring true to anyone who has struggled with getting their proposed operational changes implemented.&lt;/p&gt;
&lt;p&gt;As companies scale, it becomes more &amp;amp; more difficult to implement change. On top of that, some industries (including Healthcare, the one I work in) are similarly inertial on an industry level. In this particular case, changing a report’s format may seem small in the grand scheme of things, but this is the same format the executive team has been seeing since 2017! The system executives and individual hospital leaders have a rapport and vernacular built around these monthly reports in this format — updating the format similarly requires that the executives and leaders update their long-held understanding and language built around tabular data.&lt;/p&gt;
&lt;p&gt;Tabular data in general shows up in reports across industries. My hunch is that the main driver of this is the widespread integration of Microsoft Excel as the workhorse for most analysts’ tasks. Excel get wide use as a calculator, a data storage system (eek!), and a presentation tool. Most analysts are incredibly comfortable working in Excel and while it is possible to create plots that show both the score and confidence interval, it is far simpler to apply a bit of conditional formatting and submit the raw data as the report itself.&lt;/p&gt;
&lt;p&gt;This is not to say that tabular reports have no use — when individual values are important, tabular reports are preferred! If, however, the goal is to understand trends over time or relationships between values, plots are a far better option!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Impeachment Republicans and Democracy</title>
      <link>https://www.thedatadiary.net/blog/2022-04-06-impeachment-republicans-and-democracy/</link>
      <pubDate>Wed, 06 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-04-06-impeachment-republicans-and-democracy/</guid>
      <description>


&lt;p&gt;A few months ago, Harrison Lavelle wrote &lt;a href=&#34;https://split-ticket.org/2022/01/10/impeachment-republicans-where-are-they-now/&#34;&gt;a piece for Split Ticket&lt;/a&gt; reviewing the electoral challenges faced by house republicans who voted to impeach Donald Trump for his role in the assault on the capitol. Examining the voting records of these republicans who broke with their colleagues shows, unsurprisingly, that they tend to be more supportive of bills protecting democratic (note — small “d”) norms.&lt;/p&gt;
&lt;p&gt;FiveThirtyEight &lt;a href=&#34;https://fivethirtyeight.com/features/which-senators-and-representatives-vote-in-favor-of-democracy/&#34;&gt;developed a democracy index&lt;/a&gt; to evaluate how members of congress vote to protect democratic basics and create a more inclusive democracy (higher scores indicate that the congressman/woman is more supportive of bills protecting/expanding democracy). The linked article is well worth reading and walks through the caveats and limitations of the metric, but, notably, house republicans who voted to impeach Donald Trump are on average more supportive of pro-democracy bills than their colleagues who voted to acquit (on bills through September 1st of last year, the day the article was published).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-04-06-impeachment-republicans-and-democracy/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are some outliers: Tom Rice (SC-7) voted to impeach Trump but his votes on (small d) democratic bills are in line with the rest of house republicans whereas Brian Fitzpatrick (PA-1) and Tom Reed (NY-23) voted to acquit but have the highest pro-democracy scores by this ranking (there are other republicans who voted to acquit that have democracy scores similar to their colleagues who voted to impeach, but to avoid cluttering the plot, only the mathematical outliers are shown).&lt;/p&gt;
&lt;div id=&#34;polling-bites&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Polling bites&lt;/h3&gt;
&lt;p&gt;There has been very little movement in the generic congressional ballot — republicans are still &lt;em&gt;slightly favored&lt;/em&gt; by ~2 points. Biden’s presidential approval rating received a slight bump at the onset of Russia’s invasion of Ukraine, but the marginal gains since then have faded away and he currently sits at a &lt;strong&gt;-12.6%&lt;/strong&gt; net approval.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;png/generic_ballot_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;png/approval_disapproval_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;png/net_approval_current.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introducing {workboots}</title>
      <link>https://www.thedatadiary.net/blog/2022-03-14-introducing-workboots/</link>
      <pubDate>Mon, 14 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-03-14-introducing-workboots/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-03-14-introducing-workboots/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Sometimes, we want a model that generates a range of possible outcomes around each prediction and may opt for a model that can generate a prediction interval, like a linear model. Other times, we just care about point predictions and may opt to use a more powerful model like XGBoost. But what if we want the best of both worlds: getting a range of predictions while still using a powerful model? That’s where &lt;a href=&#34;https://github.com/markjrieke/workboots&#34;&gt;&lt;code&gt;{workboots}&lt;/code&gt;&lt;/a&gt; comes to the rescue! &lt;code&gt;{workboots}&lt;/code&gt; uses bootstrap resampling to train many models which can be used to generate a range of outcomes — regardless of model type.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/workboots.PNG&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;installation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Installation&lt;/h3&gt;
&lt;p&gt;Version 0.1.0 of &lt;code&gt;{workboots}&lt;/code&gt; is available on &lt;a href=&#34;https://cran.r-project.org/web/packages/workboots/index.html&#34;&gt;CRAN&lt;/a&gt;. Given that the package is still in early development, however, I’d recommend installing the development version from &lt;a href=&#34;https://github.com/markjrieke/workboots&#34;&gt;github&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install from CRAN
install.packages(&amp;quot;workboots&amp;quot;)

# or install the development version
devtools::install_github(&amp;quot;markjrieke/workboots&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;usage&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Usage&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;{workboots}&lt;/code&gt; builds on top of the &lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;&lt;code&gt;{tidymodels}&lt;/code&gt;&lt;/a&gt; suite of packages and is intended to be used in conjunction with a &lt;a href=&#34;https://workflows.tidymodels.org/&#34;&gt;tidymodel workflow&lt;/a&gt;. Teaching how to use &lt;code&gt;{tidymodels}&lt;/code&gt; is beyond the scope of this post, but some helpful resources are linked at the bottom for further exploration.&lt;/p&gt;
&lt;p&gt;We’ll walk through two examples that show the benefit of the package: estimating a linear model’s prediction interval and generating a prediction interval for a boosted tree model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-a-prediction-interval&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Estimating a prediction interval&lt;/h3&gt;
&lt;p&gt;Let’s get started with a model we know can generate a prediction interval: a basic linear model. In this example, we’ll use the &lt;a href=&#34;https://modeldata.tidymodels.org/reference/ames.html&#34;&gt;Ames housing dataset&lt;/a&gt; to predict a home’s price based on its square footage.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels)

# setup our data
data(&amp;quot;ames&amp;quot;)
ames_mod &amp;lt;- ames %&amp;gt;% select(First_Flr_SF, Sale_Price)

# relationship between square footage and price
ames_mod %&amp;gt;%
  ggplot(aes(x = First_Flr_SF, y = Sale_Price)) +
  geom_point(alpha = 0.25) +
  scale_y_continuous(labels = scales::dollar_format(), trans = &amp;quot;log10&amp;quot;) +
  scale_x_continuous(labels = scales::comma_format(), trans = &amp;quot;log10&amp;quot;) +
  labs(title = &amp;quot;Relationship between Square Feet and Sale Price&amp;quot;,
       subtitle = &amp;quot;Linear relationship between the log transforms of square footage and price&amp;quot;,
       x = NULL,
       y = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-03-14-introducing-workboots/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can use a linear model to predict the log transform of &lt;code&gt;Sale_Price&lt;/code&gt; based on the log transform of &lt;code&gt;First_Flr_SF&lt;/code&gt;. In this example, we’ll train a linear model then plot our predictions against a holdout set with a prediction interval.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# log transform
ames_mod &amp;lt;- 
  ames_mod %&amp;gt;%
  mutate(across(everything(), log10))

# split into train/test data
set.seed(918)
ames_split &amp;lt;- initial_split(ames_mod)
ames_train &amp;lt;- training(ames_split)
ames_test &amp;lt;- testing(ames_split)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# train a linear model
set.seed(314)
mod &amp;lt;- lm(Sale_Price ~ First_Flr_SF, data = ames_train)

# predict on new data with a prediction interval
ames_preds &amp;lt;-
  mod %&amp;gt;%
  predict(ames_test, interval = &amp;quot;predict&amp;quot;) %&amp;gt;%
  as_tibble()

# plot!
ames_preds %&amp;gt;%
  
  # re-scale predictions to match the original dataset&amp;#39;s scale
  bind_cols(ames_test) %&amp;gt;%
  mutate(across(everything(), ~10^.x)) %&amp;gt;%
  
  # add geoms
  ggplot(aes(x = First_Flr_SF)) +
  geom_point(aes(y = Sale_Price),
             alpha = 0.25) +
  geom_line(aes(y = fit),
            size = 1) +
  geom_ribbon(aes(ymin = lwr,
                  ymax = upr),
              alpha = 0.25) +
  scale_y_continuous(labels = scales::dollar_format(), trans = &amp;quot;log10&amp;quot;) +
  scale_x_continuous(labels = scales::comma_format(), trans = &amp;quot;log10&amp;quot;) +
  labs(title = &amp;quot;Linear Model of Sale Price predicted by Square Footage&amp;quot;,
       subtitle = &amp;quot;Shaded area represents the 95% prediction interval&amp;quot;,
       x = NULL,
       y = NULL) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-03-14-introducing-workboots/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;{workboots}&lt;/code&gt;, we can approximate the linear model’s prediction interval by passing a workflow built on a linear model to the function &lt;code&gt;predict_boots()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels)
library(workboots)

# setup a workflow with a linear model
ames_wf &amp;lt;-
  workflow() %&amp;gt;%
  add_recipe(recipe(Sale_Price ~ First_Flr_SF, data = ames_train)) %&amp;gt;%
  add_model(linear_reg())

# generate bootstrap predictions on ames_test
set.seed(713)
ames_preds_boot &amp;lt;-
  ames_wf %&amp;gt;%
  predict_boots(
    n = 2000,
    training_data = ames_train,
    new_data = ames_test
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;predict_boots()&lt;/code&gt; works by creating 2000 &lt;a href=&#34;https://rsample.tidymodels.org/reference/bootstraps.html&#34;&gt;bootstrap resamples&lt;/a&gt; of the training data, fitting a linear model to each resample, then generating 2000 predictions for each home’s price in the holdout set. We can then use &lt;code&gt;summarise_predictions()&lt;/code&gt; to generate upper and lower intervals for each prediction.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ames_preds_boot %&amp;gt;%
  summarise_predictions()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 733 x 5
##    rowid .preds               .pred_lower .pred .pred_upper
##    &amp;lt;int&amp;gt; &amp;lt;list&amp;gt;                     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
##  1     1 &amp;lt;tibble [2,000 x 2]&amp;gt;        5.17  5.44        5.71
##  2     2 &amp;lt;tibble [2,000 x 2]&amp;gt;        4.98  5.27        5.55
##  3     3 &amp;lt;tibble [2,000 x 2]&amp;gt;        4.97  5.25        5.52
##  4     4 &amp;lt;tibble [2,000 x 2]&amp;gt;        5.12  5.40        5.67
##  5     5 &amp;lt;tibble [2,000 x 2]&amp;gt;        5.15  5.44        5.71
##  6     6 &amp;lt;tibble [2,000 x 2]&amp;gt;        4.93  5.21        5.49
##  7     7 &amp;lt;tibble [2,000 x 2]&amp;gt;        4.67  4.94        5.22
##  8     8 &amp;lt;tibble [2,000 x 2]&amp;gt;        4.85  5.13        5.40
##  9     9 &amp;lt;tibble [2,000 x 2]&amp;gt;        4.87  5.14        5.41
## 10    10 &amp;lt;tibble [2,000 x 2]&amp;gt;        5.14  5.41        5.69
## # ... with 723 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By overlaying the intervals on top of one another, we can see that the prediction interval generated by &lt;code&gt;predict_boots()&lt;/code&gt; is a good approximation of the theoretical interval generated by &lt;code&gt;lm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ames_preds_boot %&amp;gt;%
  summarise_predictions() %&amp;gt;%
  bind_cols(ames_preds) %&amp;gt;%
  bind_cols(ames_test) %&amp;gt;%
  mutate(across(c(.pred_lower:Sale_Price), ~10^.x)) %&amp;gt;%
  ggplot(aes(x = First_Flr_SF)) +
  geom_point(aes(y = Sale_Price),
             alpha = 0.25) +
  geom_line(aes(y = fit),
            size = 1) +
  geom_ribbon(aes(ymin = lwr,
                  ymax = upr),
              alpha = 0.25) +
  geom_point(aes(y = .pred),
             color = &amp;quot;blue&amp;quot;,
             alpha = 0.25) +
  geom_errorbar(aes(ymin = .pred_lower,
                    ymax = .pred_upper),
                color = &amp;quot;blue&amp;quot;,
                alpha = 0.25,
                width = 0.0125) +
  scale_y_continuous(labels = scales::dollar_format(), trans = &amp;quot;log10&amp;quot;) +
  scale_x_continuous(labels = scales::comma_format(), trans = &amp;quot;log10&amp;quot;) +
  labs(title = &amp;quot;Linear Model of Sale Price predicted by Square Footage&amp;quot;,
       subtitle = &amp;quot;Bootstrap prediction interval closely matches theoretical prediction interval&amp;quot;,
       x = NULL,
       y = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-03-14-introducing-workboots/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both &lt;code&gt;lm()&lt;/code&gt; and &lt;code&gt;summarise_predictions()&lt;/code&gt; use a 95% prediction interval by default but we can generate other intervals by passing different values to the parameter &lt;code&gt;conf&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ames_preds_boot %&amp;gt;%
  
  # generate 95% prediction interval
  summarise_predictions(conf = 0.95) %&amp;gt;%
  rename(.pred_lower_95 = .pred_lower,
         .pred_upper_95 = .pred_upper) %&amp;gt;%
  select(-.pred) %&amp;gt;%
  
  # generate 80% prediction interval
  summarise_predictions(conf = 0.80) %&amp;gt;%
  rename(.pred_lower_80 = .pred_lower,
         .pred_upper_80 = .pred_upper) %&amp;gt;%
  bind_cols(ames_test) %&amp;gt;%
  mutate(across(c(.pred_lower_95:Sale_Price), ~10^.x)) %&amp;gt;%
  
  # plot!
  ggplot(aes(x = First_Flr_SF)) +
  geom_point(aes(y = Sale_Price),
             alpha = 0.25) +
  geom_line(aes(y = .pred),
            size = 1,
            color = &amp;quot;blue&amp;quot;) +
  geom_ribbon(aes(ymin = .pred_lower_95,
                  ymax = .pred_upper_95),
              alpha = 0.25,
              fill = &amp;quot;blue&amp;quot;) +
  geom_ribbon(aes(ymin = .pred_lower_80,
                  ymax = .pred_upper_80),
              alpha = 0.25,
              fill = &amp;quot;blue&amp;quot;) +
  scale_y_continuous(labels = scales::dollar_format(), trans = &amp;quot;log10&amp;quot;) +
  scale_x_continuous(labels = scales::comma_format(), trans = &amp;quot;log10&amp;quot;) +
  labs(title = &amp;quot;Linear Model of Sale Price predicted by Square Footage&amp;quot;,
       subtitle = &amp;quot;Predictions alongside 95% and 80% bootstrap prediction interval&amp;quot;,
       x = NULL,
       y = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-03-14-introducing-workboots/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As this example shows, &lt;code&gt;{workboots}&lt;/code&gt; can approximate linear prediction intervals pretty well! But this isn’t very useful, since we can just generate a linear prediction interval from a linear model directly. The real benefit of &lt;code&gt;{workboots}&lt;/code&gt; comes from generating prediction intervals from &lt;em&gt;any&lt;/em&gt; model!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bootstrap-prediction-intervals-with-non-linear-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bootstrap prediction intervals with non-linear models&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://xgboost.readthedocs.io/en/stable/&#34;&gt;XGBoost&lt;/a&gt; is one of my favorite models. Up until now, however, in situations that require a prediction interval, I’ve had to opt for a simpler model. With &lt;code&gt;{workboots}&lt;/code&gt;, that’s no longer an issue! In this example, we’ll use XGBoost and &lt;code&gt;{workboots}&lt;/code&gt; to generate predictions of a penguins weight from the &lt;a href=&#34;https://modeldata.tidymodels.org/reference/penguins.html&#34;&gt;Palmer Penguins dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To get started, let’s build a workflow and train an individual model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load and prep data
data(&amp;quot;penguins&amp;quot;)

penguins &amp;lt;-
  penguins %&amp;gt;%
  drop_na()

# split data into training and testing sets
set.seed(123)
penguins_split &amp;lt;- initial_split(penguins)
penguins_test &amp;lt;- testing(penguins_split)
penguins_train &amp;lt;- training(penguins_split)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create a workflow
penguins_wf &amp;lt;-
  workflow() %&amp;gt;%
  
  # add preprocessing steps
  add_recipe(
    recipe(body_mass_g ~ ., data = penguins_train) %&amp;gt;%
      step_dummy(all_nominal_predictors()) 
  ) %&amp;gt;%
  
  # add xgboost model spec
  add_model(
    boost_tree(&amp;quot;regression&amp;quot;)
  )

# fit to training data &amp;amp; predict on test data
set.seed(234)
penguins_preds &amp;lt;-
  penguins_wf %&amp;gt;%
  fit(penguins_train) %&amp;gt;%
  predict(penguins_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As mentioned above, XGBoost models can only generate point predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;penguins_preds %&amp;gt;%
  bind_cols(penguins_test) %&amp;gt;%
  ggplot(aes(x = body_mass_g,
             y = .pred)) +
  geom_point() +
  geom_abline(linetype = &amp;quot;dashed&amp;quot;,
              color = &amp;quot;gray&amp;quot;) +
  labs(title = &amp;quot;XGBoost Model of Penguin Weight&amp;quot;,
       subtitle = &amp;quot;Individual model can only output individual predictions&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-03-14-introducing-workboots/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;{workboots}&lt;/code&gt;, however, we can generate a prediction interval from our XGBoost model for each penguin’s weight!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create 2000 models from bootstrap resamples and make predictions on the test set
set.seed(345)
penguins_preds_boot &amp;lt;-
  penguins_wf %&amp;gt;%
  predict_boots(
    n = 2000,
    training_data = penguins_train,
    new_data = penguins_test
  )

penguins_preds_boot %&amp;gt;%
  summarise_predictions()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 84 x 5
##    rowid .preds               .pred_lower .pred .pred_upper
##    &amp;lt;int&amp;gt; &amp;lt;list&amp;gt;                     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
##  1     1 &amp;lt;tibble [2,000 x 2]&amp;gt;       2788. 3470.       4136.
##  2     2 &amp;lt;tibble [2,000 x 2]&amp;gt;       2838. 3534.       4231.
##  3     3 &amp;lt;tibble [2,000 x 2]&amp;gt;       2942. 3598.       4301.
##  4     4 &amp;lt;tibble [2,000 x 2]&amp;gt;       3354. 4158.       4889.
##  5     5 &amp;lt;tibble [2,000 x 2]&amp;gt;       3186. 3870.       4500.
##  6     6 &amp;lt;tibble [2,000 x 2]&amp;gt;       2884. 3519.       4208.
##  7     7 &amp;lt;tibble [2,000 x 2]&amp;gt;       2790. 3434.       4094.
##  8     8 &amp;lt;tibble [2,000 x 2]&amp;gt;       3394. 4071.       4772.
##  9     9 &amp;lt;tibble [2,000 x 2]&amp;gt;       2812. 3447.       4096.
## 10    10 &amp;lt;tibble [2,000 x 2]&amp;gt;       2744. 3404.       4063.
## # ... with 74 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How does our bootstrap model perform?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;penguins_preds_boot %&amp;gt;%
  summarise_predictions() %&amp;gt;%
  bind_cols(penguins_test) %&amp;gt;%
  ggplot(aes(x = body_mass_g,
             y = .pred,
             ymin = .pred_lower,
             ymax = .pred_upper)) +
  geom_abline(linetype = &amp;quot;dashed&amp;quot;,
              color = &amp;quot;gray&amp;quot;) +
  geom_errorbar(alpha = 0.5,
                color = &amp;quot;blue&amp;quot;) +
  geom_point(alpha = 0.5,
             color = &amp;quot;blue&amp;quot;) +
  labs(title = &amp;quot;XGBoost Model of Penguin Weight&amp;quot;,
       subtitle = &amp;quot;Bootstrap models can generate prediction intervals&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-03-14-introducing-workboots/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This particular model may be in need of some tuning for better performance, but the important takeaway is that we were able to generate a prediction distribution for the model! This method works with other regression models as well — just create a workflow then let &lt;code&gt;{workboots}&lt;/code&gt; take care of the rest!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidymodel-resources&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tidymodel Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tidymodels.org/start/&#34;&gt;Getting Started with Tidymodels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tmwr.org/&#34;&gt;Tidy Modeling with R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://juliasilge.com/blog/&#34;&gt;Julia Silge’s Blog&lt;/a&gt; provides use cases of tidymodels with weekly &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;#tidytuesday&lt;/a&gt; datasets.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Data Science Hierarchy of Needs</title>
      <link>https://www.thedatadiary.net/blog/2022-02-22-the-data-science-hierarchy-of-needs/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-02-22-the-data-science-hierarchy-of-needs/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-02-22-the-data-science-hierarchy-of-needs/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I’ve never built a house (shocking, I know), but from far too much time spent watching HGTV, I understand the basic gist of it. You lay a foundation, setup framing and walls, route mechanical and electrical, then work on final touches like painting and decorating (to be sure, I’m hand-waiving a lot of detail away here). There’s a bit of wiggle room in the order you go about things — you can paint the living room walls before the ones in the bathroom or vice versa — but some steps definitely need to happen before others — you can’t paint either rooms until the walls themselves are actually up!&lt;/p&gt;
&lt;p&gt;The same logic applies for data science — there are certain activities that are exceptionally hard to do without the proper infrastructure in place. Sometimes, we’re asked to chase after ~shiny objects~ without the support system to do so, when doing so may actually make our job more difficult in the future!&lt;/p&gt;
&lt;p&gt;I recently stumbled across &lt;a href=&#34;https://towardsdatascience.com/the-data-science-pyramid-8a018013c490&#34;&gt;an article&lt;/a&gt; that summarized this really succinctly with the following graphic: &lt;strong&gt;The Data Science Hierarchy of Needs&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/ds_hierarchy.png&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;collect&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Collect&lt;/h4&gt;
&lt;p&gt;At a baseline, to do any sort of data work you need to actually have data on hand to work with! Whether there’s a formal process for collecting data or you need to gather data from disparate public sources, getting raw data out of the wild and into your system is the first step to being able to do any sort of analysis. In my case, as an analyst with a hospital’s patient satisfaction group, we need to actually send patients surveys.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;movestore&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Move/Store&lt;/h4&gt;
&lt;p&gt;Once you know where your data is coming from, setting up a reliable data flow from the source to your environment is needed. This is where a lot of headache can come from. Gathering data can be difficult but if the data is going to be used once for a one-off analysis, you don’t need to worry too much about repeatability, edge cases, or computing speed. Once you need to gather new data, thinking about infrastructure around new data gathering becomes much more important. A good chunk of the last eight months of my job has been working with our new survey vendor on this piece of the puzzle: standardizing data layouts, catching bugs in the pipeline, and setting up standards for access.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploretransform&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Explore/Transform&lt;/h4&gt;
&lt;p&gt;With a reliable flow of new/updated data streaming in, you now need to make sure the data is appropriate for general use. Automated anomaly/fraud/bot detection, light wrangling, and removing errant responses can all be considered a part of this single stage. This is necessary to ensure that any analyses you do or models you build are based on what you expect from the underlying data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregatelabel&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Aggregate/Label&lt;/h4&gt;
&lt;p&gt;I can’t recall the source, but the following quote about data science has stuck with me: “99% of data science work is counting — sometimes dividing.” A significant portion of my day-to-day work involves the tried-and-trusted &lt;code&gt;group_by() %&amp;gt;% summarise()&lt;/code&gt; pipeline. Making counts, percentages, running totals, etc. accessible to stakeholders via a dashboard can likely answer ~80% of the questions an analyst would have to field otherwise. It’s &lt;em&gt;so, so&lt;/em&gt; important, however, to have the collection, storage, and preparation stages setup prior to ensure that stakeholders can &lt;em&gt;trust&lt;/em&gt; that the data they’re seeing is accurate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;learnoptimize&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Learn/Optimize&lt;/h4&gt;
&lt;p&gt;If 80% of questions asked can be solved with grouped summaries and 20% require a model, it’s likely that 80% of that remaining 20% can be solved by a simple linear model. For example, “What effect does patient age have on their overall satisfaction?” can be answered with &lt;code&gt;lm(satisfaction_score ~ age)&lt;/code&gt;. As relationships become more complex, you can add more terms to the model, or switch model architectures, but — in my own experience — the majority of modeling in practice can be represented by linear models (and, by extension, regularized models via &lt;code&gt;{glmnet}&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;complex-models&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Complex Models&lt;/h4&gt;
&lt;p&gt;Finally, a small subset of problems may require a more complex or powerful model type. But before you spin your wheels building a neural net or some other wacky architecture, you should first check if something simpler gets you what you need.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-closing-thoughts&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Some Closing Thoughts&lt;/h4&gt;
&lt;p&gt;This post is partially meant to be able to share some useful info and partially a reminder to me to look for the simple solution! I have a tendency to start off with something complex then realize that I could save a lot of work if I just switch to something simpler. The three baseline layers upstream of my domain are &lt;em&gt;super important&lt;/em&gt; and definitely need oversight from someone with an eye for data engineering.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polling-bites&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Polling Bites&lt;/h3&gt;
&lt;p&gt;The generic congressional ballot is starting to show some movement away from even split as Republicans have slowly climbed to &lt;strong&gt;51.2%&lt;/strong&gt; in the polls and Democrats have fallen to &lt;strong&gt;48.8%&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/generic_ballot_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Biden’s net approval hasn’t shifted significantly since the last post — currently sitting at &lt;strong&gt;10.9%&lt;/strong&gt; underwater with &lt;strong&gt;41.8%&lt;/strong&gt; approval and &lt;strong&gt;52.7%&lt;/strong&gt; disapproval.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/approval_disapproval_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/net_approval_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(p.s., I’ve updated the color palettes here with the &lt;a href=&#34;https://github.com/BlakeRMills/MetBrewer&#34;&gt;&lt;code&gt;{MetBrewer}&lt;/code&gt;&lt;/a&gt; package, which provides colorblind friendly palettes based on artwork in hte Metropolitan Museum of Art in New York).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Pull Yourself Up by Your Bootstraps</title>
      <link>https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/</link>
      <pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Note (3/14/22): This article was written prior to the release of the &lt;a href=&#34;https://github.com/markjrieke/workboots&#34;&gt;{workboots}&lt;/a&gt; package. Since the release of that package, I’ve discovered some errors with the methodology described here and would recommend instead referencing the &lt;a href=&#34;https://thedatadiary.net/blog/2022-03-14-introducing-workboots&#34;&gt;post associated with the release&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Statistical modeling sometimes presents conflicting goals. Oftentimes, building a model involves a mix of objectives that don’t necessarily mesh well together: super-accurate point predictions, explainability, fast performance, or an expression of confidence in the prediction. In my work as an analyst, I generally am focused on how explainable the model is while being able to express a confidence interval around each prediction. For that, simple linear models do the trick. If, however, I want to regularize via &lt;code&gt;{glmnet}&lt;/code&gt; (which — with good reason — &lt;a href=&#34;https://stats.stackexchange.com/questions/224796/why-are-confidence-intervals-and-p-values-not-reported-as-default-for-penalized&#34;&gt;doesn’t provide confidence intervals&lt;/a&gt;) or use a non-linear model like &lt;code&gt;{xgboost}&lt;/code&gt;, I have to drop the confidence interval around predictions. Or so I had previously thought! As it turns out, building a series of models from bootstrap resamples provides an alternative method of generating a confidence interval around a prediction.&lt;/p&gt;
&lt;div id=&#34;setting-a-baseline-with-penguins&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting a baseline with penguins&lt;/h3&gt;
&lt;p&gt;First, let’s build out a baseline linear model with the Palmer Penguins dataset. This dataset contains information on 344 penguins across three species types and three islands. For this example, we’ll use the penguin information to predict &lt;code&gt;body_mass_g&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the data in from the tidytuesdayR package
penguins_src &amp;lt;- tidytuesdayR::tt_load(2020, week = 31)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Downloading file 1 of 2: `penguins.csv`
##  Downloading file 2 of 2: `penguins_raw.csv`&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract out the penguins dataset
penguins &amp;lt;- penguins_src$penguins
rm(penguins_src)

penguins&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 344 x 8
##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
##  1 Adelie  Torgersen           39.1          18.7               181        3750
##  2 Adelie  Torgersen           39.5          17.4               186        3800
##  3 Adelie  Torgersen           40.3          18                 195        3250
##  4 Adelie  Torgersen           NA            NA                  NA          NA
##  5 Adelie  Torgersen           36.7          19.3               193        3450
##  6 Adelie  Torgersen           39.3          20.6               190        3650
##  7 Adelie  Torgersen           38.9          17.8               181        3625
##  8 Adelie  Torgersen           39.2          19.6               195        4675
##  9 Adelie  Torgersen           34.1          18.1               193        3475
## 10 Adelie  Torgersen           42            20.2               190        4250
## # ... with 334 more rows, and 2 more variables: sex &amp;lt;chr&amp;gt;, year &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll need to do some lite preprocessing before we start modeling — it looks like there are some &lt;code&gt;NAs&lt;/code&gt; in &lt;code&gt;body_mass_g&lt;/code&gt; and in &lt;code&gt;sex&lt;/code&gt;. If I were creating a more serious model, I might keep the rows with &lt;code&gt;NAs&lt;/code&gt; for &lt;code&gt;sex&lt;/code&gt;, but since there are so few and this is an explainer, I’ll just filter them out.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# remove NA from body_mass_g and sex
penguins &amp;lt;- 
  penguins %&amp;gt;%
  filter(!is.na(body_mass_g),
         !is.na(sex))

penguins&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 333 x 8
##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
##  1 Adelie  Torgersen           39.1          18.7               181        3750
##  2 Adelie  Torgersen           39.5          17.4               186        3800
##  3 Adelie  Torgersen           40.3          18                 195        3250
##  4 Adelie  Torgersen           36.7          19.3               193        3450
##  5 Adelie  Torgersen           39.3          20.6               190        3650
##  6 Adelie  Torgersen           38.9          17.8               181        3625
##  7 Adelie  Torgersen           39.2          19.6               195        4675
##  8 Adelie  Torgersen           41.1          17.6               182        3200
##  9 Adelie  Torgersen           38.6          21.2               191        3800
## 10 Adelie  Torgersen           34.6          21.1               198        4400
## # ... with 323 more rows, and 2 more variables: sex &amp;lt;chr&amp;gt;, year &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s always good practice to explore the dataset prior to fitting a model, so let’s jump into some good ol’ fashioned EDA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how are species/island related to body mass?
penguins %&amp;gt;%
  ggplot(aes(x = species,
             y = body_mass_g,
             color = species)) +
  geom_boxplot() +
  geom_point(alpha = 0.25,
             position = position_jitter()) +
  facet_wrap(~island)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Interesting! It looks like the Gentoo and Chinstrap species are only found on the Biscoe and Dream islands, respectively, whereas the Adelie species can be found on all three islands. At first glance, there’s not a meaningful difference that Island has on the weight of the Adelie penguins, so I think we’re safe to toss out the &lt;code&gt;island&lt;/code&gt; feature and just keep &lt;code&gt;species&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how does sex relate to body mass?
penguins %&amp;gt;%
  ggplot(aes(x = sex,
             y = body_mass_g,
             color = sex)) +
  geom_boxplot() +
  geom_point(alpha = 0.25,
             position = position_jitter())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unsurprisingly, male penguins are typically heavier than female penguins.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# are penguins getting heavier or lighter as years progress?
penguins %&amp;gt;%
  mutate(year = as.character(year)) %&amp;gt;%
  ggplot(aes(x = year,
             y = body_mass_g)) +
  geom_boxplot() +
  geom_point(alpha = 0.25,
             position = position_jitter())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It doesn’t look like there is significant signal being drawn from &lt;code&gt;year&lt;/code&gt;, so we’ll toss that out as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how do other body measurements compare with the total body mass?
penguins %&amp;gt;%
  select(bill_length_mm:body_mass_g) %&amp;gt;%
  pivot_longer(ends_with(&amp;quot;mm&amp;quot;),
               names_to = &amp;quot;measurement&amp;quot;,
               values_to = &amp;quot;value&amp;quot;) %&amp;gt;%
  ggplot(aes(x = value,
             y = body_mass_g,
             color = measurement)) +
  geom_point(alpha = 0.5) + 
  facet_wrap(~measurement, scales = &amp;quot;free_x&amp;quot;) +
  geom_smooth(method = &amp;quot;lm&amp;quot;,
              se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For bill and flipper length, there’s a pretty clear relationship, but it looks like bill depth has a &lt;em&gt;classic&lt;/em&gt; case of &lt;a href=&#34;https://en.wikipedia.org/wiki/Simpson%27s_paradox&#34;&gt;Simpson’s paradox&lt;/a&gt;. Let’s explore that further to find a meaningful interaction to apply.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# which feature interacts with bill depth to produce simpson&amp;#39;s pardox?
penguins %&amp;gt;%
  ggplot(aes(x = bill_depth_mm,
             y = body_mass_g,
             color = species)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = &amp;quot;lm&amp;quot;,
              se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So, very clearly, the Gentoo species has a very different relationship between bill depth and body mass than the Adelie/Chinstrap species. We’ll add this as an interactive feature to the model.&lt;/p&gt;
&lt;p&gt;With all that completed, let’s (finally) setup and build the baseline linear model with confidence intervals around the prediction!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# remove features
penguins &amp;lt;- 
  penguins %&amp;gt;%
  select(-island, -year)

# split into testing and training datasets
set.seed(123)
penguins_split &amp;lt;- initial_split(penguins)
penguins_test &amp;lt;- testing(penguins_split)
penguins_train &amp;lt;- training(penguins_split)

# setup a pre-processing recipe
penguins_rec &amp;lt;- 
  recipe(body_mass_g ~ ., data = penguins_train) %&amp;gt;%
  step_dummy(all_nominal()) %&amp;gt;% 
  step_interact(~starts_with(&amp;quot;species&amp;quot;):bill_depth_mm)

# fit a workflow
penguins_lm &amp;lt;- 
  workflow() %&amp;gt;%
  add_recipe(penguins_rec) %&amp;gt;%
  add_model(linear_reg() %&amp;gt;% set_engine(&amp;quot;lm&amp;quot;)) %&amp;gt;%
  fit(penguins_train)

# predict on training data with confidence intervals
bind_cols(penguins_lm %&amp;gt;% predict(penguins_train),
          penguins_lm %&amp;gt;% predict(penguins_train, type = &amp;quot;conf_int&amp;quot;, level = 0.95),
          penguins_train) %&amp;gt;%
  ggplot(aes(x = body_mass_g,
             y = .pred)) +
  geom_point(alpha = 0.5) +
  geom_segment(aes(x = body_mass_g,
                   xend = body_mass_g,
                   y = .pred_lower,
                   yend = .pred_upper),
               alpha = 0.25) +
  labs(title = &amp;quot;Predicting the Palmer Penguins - Training&amp;quot;,
       subtitle = &amp;quot;Linear model predicting a penguin&amp;#39;s weight in grams&amp;quot;,
       x = &amp;quot;Actual weight (g)&amp;quot;,
       y = &amp;quot;Predicted weight (g)&amp;quot;,
       caption = &amp;quot;Errorbars represent the a 95% confidence interval&amp;quot;) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This model does generally okay, but the confidence interval around each prediction is pretty &lt;a href=&#34;https://mc-stan.org/rstanarm/articles/rstanarm.html&#34;&gt;clearly too confident&lt;/a&gt;! Let’s solve this with bootstrapping.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;whats-a-bootstrap&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What’s a bootstrap?&lt;/h3&gt;
&lt;p&gt;Before progressing any further, it’s probably important to define what exactly a bootstrap is/what bootstrapping is. Bootstrapping is a resampling method that lets us take one dataset and turn it into many datasets. Bootstrapping accomplishes this by repeatedly pulling a random row from the source dataset and, importantly, bootstrapping allows for rows to be repeated! Let’s look at an example for a bit more clarity.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;rowid&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;104&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;102&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;79&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let’s say we want to make bootstrap resamples of this dataset. We’ll draw five random rows from the dataset and, sometimes, we’ll have the same row show up in our new bootstrapped dataset multiple times:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;rowid&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;104&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;102&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Another bootstrap dataset might look like this:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;rowid&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;102&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;79&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;79&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Bootstrap datasets allow us to create many datasets from the original dataset and evaluate models across these bootstraps. Models that are well informed will give similar outputs across each dataset, despite of the randomness within each dataset, whereas less confident models will have a wider variation across the bootstrapped datasets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generating-some-confident-penguins&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Generating some confident penguins&lt;/h3&gt;
&lt;p&gt;Let’s say we want to use &lt;code&gt;{xgboost}&lt;/code&gt; to predict penguin weight and we’ll use bootstrapping to generate a confidence interval. Firstly, we’ll create the bootstrap datasets from our training set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;penguins_boot &amp;lt;- penguins_train %&amp;gt;% bootstraps()

penguins_boot&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # Bootstrap sampling 
## # A tibble: 25 x 2
##    splits           id         
##    &amp;lt;list&amp;gt;           &amp;lt;chr&amp;gt;      
##  1 &amp;lt;split [249/92]&amp;gt; Bootstrap01
##  2 &amp;lt;split [249/90]&amp;gt; Bootstrap02
##  3 &amp;lt;split [249/91]&amp;gt; Bootstrap03
##  4 &amp;lt;split [249/87]&amp;gt; Bootstrap04
##  5 &amp;lt;split [249/98]&amp;gt; Bootstrap05
##  6 &amp;lt;split [249/84]&amp;gt; Bootstrap06
##  7 &amp;lt;split [249/91]&amp;gt; Bootstrap07
##  8 &amp;lt;split [249/95]&amp;gt; Bootstrap08
##  9 &amp;lt;split [249/94]&amp;gt; Bootstrap09
## 10 &amp;lt;split [249/86]&amp;gt; Bootstrap10
## # ... with 15 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, the &lt;code&gt;bootstraps()&lt;/code&gt; function will create 25 bootstrap datasets, but we could theoretically create as many as we want. Now that we have our bootstraps, let’s create a function that will fit a model to each of the bootstraps and save to disk. We’ll use the default parameters for our &lt;code&gt;{xgboost}&lt;/code&gt; model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define a basic xgboost model
penguins_xgb &amp;lt;-
  boost_tree() %&amp;gt;%
  set_mode(&amp;quot;regression&amp;quot;) %&amp;gt;%
  set_engine(&amp;quot;xgboost&amp;quot;)

# function that will fit a model and save to a folder
fit_bootstrap &amp;lt;- function(index) {
  
  # pull out individual bootstrap to fit
  xgb_boot &amp;lt;- penguins_boot$splits[[index]] %&amp;gt;% training()
  
  # fit to a workflow
  workflow() %&amp;gt;%
    add_recipe(penguins_rec) %&amp;gt;%
    add_model(penguins_xgb) %&amp;gt;%
    fit(xgb_boot) %&amp;gt;%
    write_rds(paste0(&amp;quot;models/model_&amp;quot;, index, &amp;quot;.rds&amp;quot;))
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function will create a new model for each bootstrap, so we’ll end up with 25 separate models. Let’s fit!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fit to 25 bootstrapped datasets
for (i in 1:25) {
  
  fit_bootstrap(i)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s define a function that will predict based on these 25 bootstrapped models, then predict on our training data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_bootstrap &amp;lt;- function(new_data, index){
  
  read_rds(paste0(&amp;quot;models/model_&amp;quot;, index, &amp;quot;.rds&amp;quot;)) %&amp;gt;%
    predict(new_data) %&amp;gt;%
    rename(!!sym(paste0(&amp;quot;pred_&amp;quot;, index)) := .pred)
  
}

# predict!
training_preds &amp;lt;- 
  seq(1, 25) %&amp;gt;%
  map_dfc(~predict_bootstrap(penguins_train, .x))

training_preds&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 249 x 25
##    pred_1 pred_2 pred_3 pred_4 pred_5 pred_6 pred_7 pred_8 pred_9 pred_10
##     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1  5552.  5638.  5555.  5703.  5726.  5783.  5404.  5566.  5493.   5547.
##  2  3470.  3340.  3334.  3350.  3311.  3303.  3315.  3421.  3692.   3436.
##  3  5309.  5274.  5241.  5286.  5206.  5084.  5506.  5531.  5274.   5309.
##  4  4160.  4013.  3988.  4111.  4075.  4073.  4284.  4050.  4033.   4033.
##  5  4003.  3931.  4096.  3968.  4008.  3918.  3941.  4093.  3941.   3880.
##  6  3967.  4039.  4095.  4047.  4021.  4055.  3980.  4115.  4067.   4084.
##  7  4647.  4551.  4750.  4555.  4690.  4396.  4235.  4686.  4764.   4659.
##  8  5240.  5288.  5291.  5276.  5308.  5508.  5570.  5375.  5340.   5268.
##  9  4138.  4111.  4106.  4236.  4135.  4219.  4218.  4211.  4160.   4071.
## 10  4728.  4723.  4715.  4823.  4765.  4727.  4836.  4777.  4765.   4633.
## # ... with 239 more rows, and 15 more variables: pred_11 &amp;lt;dbl&amp;gt;, pred_12 &amp;lt;dbl&amp;gt;,
## #   pred_13 &amp;lt;dbl&amp;gt;, pred_14 &amp;lt;dbl&amp;gt;, pred_15 &amp;lt;dbl&amp;gt;, pred_16 &amp;lt;dbl&amp;gt;, pred_17 &amp;lt;dbl&amp;gt;,
## #   pred_18 &amp;lt;dbl&amp;gt;, pred_19 &amp;lt;dbl&amp;gt;, pred_20 &amp;lt;dbl&amp;gt;, pred_21 &amp;lt;dbl&amp;gt;, pred_22 &amp;lt;dbl&amp;gt;,
## #   pred_23 &amp;lt;dbl&amp;gt;, pred_24 &amp;lt;dbl&amp;gt;, pred_25 &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a column of predictions for each model — we can summarise our point prediction for each row with the average across all models and set the confidence interval based on the standard deviation of the predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_preds %&amp;gt;%
  bind_cols(penguins_train) %&amp;gt;%
  rowid_to_column() %&amp;gt;%
  pivot_longer(starts_with(&amp;quot;pred_&amp;quot;),
               names_to = &amp;quot;model&amp;quot;,
               values_to = &amp;quot;.pred&amp;quot;) %&amp;gt;%
  group_by(rowid) %&amp;gt;%
  summarise(body_mass_g = max(body_mass_g),
            .pred_mean = mean(.pred),
            std_dev = sd(.pred)) %&amp;gt;%
  riekelib::normal_interval(.pred_mean, std_dev) %&amp;gt;%
  ggplot(aes(x = body_mass_g,
             y = .pred_mean)) +
  geom_point(alpha = 0.5) +
  geom_segment(aes(x = body_mass_g, 
                   xend = body_mass_g,
                   y = ci_lower,
                   yend = ci_upper),
               alpha = 0.25) +
  labs(title = &amp;quot;Predicting the Palmer Penguins - Training&amp;quot;,
       subtitle = &amp;quot;XGBoost model predicting a penguin&amp;#39;s weight in grams&amp;quot;,
       x = &amp;quot;Actual weight (g)&amp;quot;,
       y = &amp;quot;Predicted weight (g)&amp;quot;,
       caption = &amp;quot;Errorbars represent the a 95% confidence interval&amp;quot;) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And just like that, we’ve trained a series of models with &lt;code&gt;{xgboost}&lt;/code&gt; that let us apply a confidence interval around a point prediction! Now that we’ve done so on the training set, let’s look at performance on the test set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(1, 25) %&amp;gt;%
  map_dfc(~predict_bootstrap(penguins_test, .x)) %&amp;gt;%
  bind_cols(penguins_test) %&amp;gt;%
  rowid_to_column() %&amp;gt;%
  pivot_longer(starts_with(&amp;quot;pred_&amp;quot;),
               names_to = &amp;quot;model&amp;quot;,
               values_to = &amp;quot;.pred&amp;quot;) %&amp;gt;%
  group_by(rowid) %&amp;gt;%
  summarise(body_mass_g = max(body_mass_g),
            .pred_mean = mean(.pred),
            std_dev = sd(.pred)) %&amp;gt;%
  riekelib::normal_interval(.pred_mean, std_dev) %&amp;gt;%
  ggplot(aes(x = body_mass_g,
             y = .pred_mean)) +
  geom_point(alpha = 0.5) +
  geom_segment(aes(x = body_mass_g, 
                   xend = body_mass_g,
                   y = ci_lower,
                   yend = ci_upper),
               alpha = 0.25) +
  labs(title = &amp;quot;Predicting the Palmer Penguins - Testing&amp;quot;,
       subtitle = &amp;quot;XGBoost model predicting a penguin&amp;#39;s weight in grams&amp;quot;,
       x = &amp;quot;Actual weight (g)&amp;quot;,
       y = &amp;quot;Predicted weight (g)&amp;quot;,
       caption = &amp;quot;Errorbars represent the a 95% confidence interval&amp;quot;) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The performance on the test data is slightly less accurate than on the training data, but that is to be expected. Importantly, we’ve used bootstrap resampling to generate a confidence interval from a model that otherwise normally returns a simple point prediction.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-noteworthy-caveats&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some noteworthy caveats&lt;/h3&gt;
&lt;p&gt;The prediction interval above is all well and good, but it comes with some &lt;em&gt;hefty&lt;/em&gt; caveats. Firstly, the confidence interval in the Testing plot is generated from the mean and standard deviation from each prediction. This assumes that the predictions are distributed normally, which may not necessarily be the case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_preds %&amp;gt;%
  slice_head(n = 1) %&amp;gt;%
  pivot_longer(starts_with(&amp;quot;pred&amp;quot;)) %&amp;gt;%
  ggplot(aes(x = value)) +
  geom_density()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This density plot for one of the predictions shows that there’s definitely some non-normal behavior! There’s a few ways of addressing this.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create many, many, more bootstraps and models so that the prediction distribution approaches normality (with only 25 points, we really shouldn’t even expect normality from this example).&lt;/li&gt;
&lt;li&gt;Report out the actual values of the percentiles in the distribution (e.g., the 2.5% percentile is below X, 97.5% is above Y, and the mean is at Z).&lt;/li&gt;
&lt;li&gt;Report out the actual distribution as the result.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Ideally, you should do all three.&lt;/p&gt;
&lt;p&gt;The second major caveat is that this is not one model, but a whole host of models and these take up a large amount of disk space. In this example, our 25 models take up 25 times more space than our original model and it takes some time to read in, fit, and wrangle the results. We can trade disk space for computation time by writing a function that fits and predicts without saving a model, but again, that’s a tradeoff between speed and space. For linear models, it may be a better route to have STAN simulate thousands of results via &lt;code&gt;{rstanarm}&lt;/code&gt; or &lt;code&gt;{brms}&lt;/code&gt;, but for non-linear models, boostrapping is the best way to go for now!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polling-bites&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Polling Bites&lt;/h3&gt;
&lt;p&gt;Currently, the Generic Ballot is holding steady with a slight sliver more Americans wanting Republicans in Congress than Democrats (&lt;strong&gt;50.7%&lt;/strong&gt; to &lt;strong&gt;49.3%&lt;/strong&gt;, respectively). Joe Biden’s net approval continues to slide, currently sitting at &lt;strong&gt;-11.4%&lt;/strong&gt; (&lt;strong&gt;41.8%&lt;/strong&gt; approve, &lt;strong&gt;53.1%&lt;/strong&gt; disapprove).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/generic_ballot_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/approval_disapproval_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/net_approval_current.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How Popular is Joe Biden?</title>
      <link>https://www.thedatadiary.net/blog/2022-01-26-how-popular-is-joe-biden/</link>
      <pubDate>Wed, 26 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-01-26-how-popular-is-joe-biden/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-01-26-how-popular-is-joe-biden/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;While the sitting president’s party tends to House lose seats in the midterm elections, the president’s approval rating can help inform us of the &lt;a href=&#34;https://fivethirtyeight.com/features/some-early-clues-about-how-the-midterms-will-go/&#34;&gt;magnitude of that loss&lt;/a&gt;. In general, the more unpopular the president, the more seats his party tends to lose. As a part of my long-term goal of &lt;a href=&#34;https://www.thedatadiary.net/blog/2022-01-12-the-data-diary-year-in-review/&#34;&gt;building a midterm election model&lt;/a&gt;, I put together a model for estimating Biden’s approval, disapproval, and net approval based on &lt;a href=&#34;https://projects.fivethirtyeight.com/biden-approval-rating/&#34;&gt;polls collected by FiveThirtyEight&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The methodology to build the approval model is &lt;em&gt;very similar&lt;/em&gt; to the &lt;a href=&#34;https://www.thedatadiary.net/blog/2021-12-14-do-voters-want-democrats-or-republicans-in-congress/&#34;&gt;Generic Congressional Ballot&lt;/a&gt; model — both weight polls by recency, sample size, methodology, pollster, and survey population — so I won’t rehash the details again. If you’re so inclined, you can &lt;a href=&#34;https://www.thedatadiary.net/blog/2021-12-14-do-voters-want-democrats-or-republicans-in-congress/&#34;&gt;read the methodology&lt;/a&gt; from the previous post or &lt;a href=&#34;https://github.com/markjrieke/2022-midterm-forecasts/blob/main/scripts/approval_trends_weighting.R&#34;&gt;explore the script&lt;/a&gt;, but we’ll keep this short and sweet and just explore the results!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/approval_disapproval_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/net_approval_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As of today, Joe Biden’s job approval is fairly underwater — and &lt;a href=&#34;https://www.cnn.com/2021/11/13/politics/midterms-democrats-biden-analysis/index.html&#34;&gt;history says it’s unlikely to improve as the midterms approach&lt;/a&gt;. Interestingly, the generic congressional ballot still shows that the country is about even-split on who they’d like to see in congress. As with the president’s approval, however, &lt;a href=&#34;https://twitter.com/geoffreyvs/status/1438509217107701768&#34;&gt;historical precedent implies a continuous slide away from the president’s party&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/generic_ballot_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Despite this precedent, politics today are far more polarized than previous midterm cycles, and &lt;a href=&#34;https://gelliottmorris.substack.com/p/is-joe-biden-actually-that-unpopular&#34;&gt;modeling that adjusts for this partisanship may imply a lesser midterm shellacking than history would otherwise suggest&lt;/a&gt;. While Biden would certainly prefer for his approval to be higher, it may not necessarily spell doom for democrats in the midterms (though, to be sure, the strong prior still that they will lose ground this cycle — the question now is how much).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Data Diary: Year in Review</title>
      <link>https://www.thedatadiary.net/blog/2022-01-12-the-data-diary-year-in-review/</link>
      <pubDate>Wed, 12 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-01-12-the-data-diary-year-in-review/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-01-12-the-data-diary-year-in-review/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In the year since I started this blog, there’s been a lot that’s happened: I learned to use R, picked up the basics of machine learning, and moved into a new job/industry. I spend a lot of time thinking about what’s coming down the pipeline and how much further I have to go on projects that I have planned, but it’s worthwhile every now and then to take a look back and see just how far I’ve come.&lt;/p&gt;
&lt;div id=&#34;some-accomplishments-im-proud-of&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some accomplishments I’m proud of&lt;/h2&gt;
&lt;div id=&#34;learning-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Learning R&lt;/h3&gt;
&lt;p&gt;A year ago, I couldn’t write a lick of R code — I lived and breathed Excel, and was a bit afraid of the transition from a GUI to an IDE. Now, I’d consider myself pretty well-versed in the language and am &lt;em&gt;so glad&lt;/em&gt; I made the switch. Having moved to R, I realized how restrictive Excel was — R (or any other analytics-focused programming language) allows for the freedom of expression needed for any sort of serious analysis. &lt;a href=&#34;https://www.youtube.com/watch?v=PURtmHwk_-0&#34;&gt;This talk by Hadley Wickham&lt;/a&gt; was instrumental in pushing me to pick up R and is well worth a watch if you have the time.&lt;/p&gt;
&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/PURtmHwk_-0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;completing-stanfords-machine-learning-course&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Completing Stanford’s Machine Learning course&lt;/h3&gt;
&lt;p&gt;When I started learning R, I was most interested in getting to the point where I’d be able to implement machine learning models (this specifically came from reading &lt;a href=&#34;https://projects.economist.com/us-2020-forecast/president/how-this-works&#34;&gt;the documentation&lt;/a&gt; for &lt;a href=&#34;https://github.com/TheEconomist/us-potus-model&#34;&gt;The Economist’s POTUS model&lt;/a&gt; and wanting to understand what was going on under-the-hood). &lt;a href=&#34;https://www.coursera.org/learn/machine-learning?&#34;&gt;Stanford’s online Machine Learning course&lt;/a&gt; was a thorough, technical introduction to the basics of machine learning. It doesn’t cover every model type, but gives a great foundation for &lt;em&gt;how to understand&lt;/em&gt; new models by requiring that you write the models yourself (this was very useful and practical, but you won’t catch me using MATLAB anytime soon!).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/coursera_ml_cert.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;committing-to-ropensci&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Committing to rOpenSci&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt; is a non-profit initiative committed to creating and maintaining a variety of open-source R packages. For work, I use one of their packages, &lt;a href=&#34;https://docs.ropensci.org/qualtRics/&#34;&gt;the qualtRics package&lt;/a&gt;, almost daily for extracting survey responses from Qualtrics’ API. I added a small function, &lt;a href=&#34;https://docs.ropensci.org/qualtRics/reference/fetch_id.html&#34;&gt;&lt;code&gt;fetch_id()&lt;/code&gt;&lt;/a&gt;, that allows you to pull in survey responses based on the survey’s name, rather than looking up the miscellaneous string of numbers that constitute the &lt;code&gt;survey_id&lt;/code&gt;. It’s a small helper function, but working on it taught me a lot about documentation, testing, package development, and contributing to open-source software.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(qualtRics)

all_surveys() %&amp;gt;% 
  fetch_id(&amp;quot;Mark&amp;#39;s Example Survey&amp;quot;) %&amp;gt;%
  fetch_survey() %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |======================================================================| 100%&lt;/code&gt;&lt;/pre&gt;
&lt;table style=&#34;width:100%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;5%&#34; /&gt;
&lt;col width=&#34;3%&#34; /&gt;
&lt;col width=&#34;3%&#34; /&gt;
&lt;col width=&#34;7%&#34; /&gt;
&lt;col width=&#34;3%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;5%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;5%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;4%&#34; /&gt;
&lt;col width=&#34;5%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;StartDate&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;EndDate&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Status&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;IPAddress&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Progress&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Duration (in seconds)&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Finished&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;RecordedDate&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;ResponseId&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;RecipientLastName&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;RecipientFirstName&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;RecipientEmail&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;ExternalReference&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;LocationLatitude&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;LocationLongitude&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;DistributionChannel&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;UserLanguage&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Q1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2022-01-12 15:20:31&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2022-01-12 15:20:38&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Survey Preview&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2022-01-12 15:20:40&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R_XFVjzAh4MalrLmF&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;29.73351&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-95.5564&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;preview&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;EN&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Strongly agree&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-together-a-developer-package&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Putting together a developer package&lt;/h3&gt;
&lt;p&gt;At some point this past year, I found myself either re-writing the same chunks of code repeatedly or re-defining functions across every project. After a lengthy period of hesitation, I finally picked up the &lt;a href=&#34;https://r-pkgs.org/&#34;&gt;R Packages&lt;/a&gt; book by Hadley Wickham and Jenny Bryan and put together my own personal package, &lt;a href=&#34;https://github.com/markjrieke/riekelib&#34;&gt;&lt;code&gt;{riekelib}&lt;/code&gt;&lt;/a&gt;. It’s just a collection of small helper functions that I use regularly for both personal and professional projects, but it’s really helped speed up workflows, since I can just load the library rather than re-write code or functions! Here are a few examples:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(riekelib)

# beta_interval() gives lower &amp;amp; upper bounds of a beta distribution&amp;#39;s confidence interval 
tibble::tibble(alpha = c(85, 100),
               beta = c(15, 500)) %&amp;gt;%
  beta_interval(alpha, beta) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;alpha&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;beta&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ci_lower&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ci_upper&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;85&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7741265&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9126452&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1379480&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1974895&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# percent() returns the percentage each value or combination of values appear in a tibble
iris %&amp;gt;%
  tibble::as_tibble() %&amp;gt;%
  percent(Species) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Species&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;pct&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;setosa&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3333333&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;versicolor&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3333333&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;virginica&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3333333&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;plots-across-the-year&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plots across the year&lt;/h2&gt;
&lt;p&gt;Possibly the most visually-engaging way to track growth throughout the past year is to look back on how different plots have evolved. Here’s a walkthrough of some choice plots that I’ve made throughout the year.&lt;/p&gt;
&lt;div id=&#34;baby-steps&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.thedatadiary.net/blog/2021-01-10-baby-steps/&#34;&gt;Baby Steps&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The first plot I ever created in R, made with base R’s &lt;code&gt;plot()&lt;/code&gt; function, compares speed &amp;amp; distance from the &lt;code&gt;cars&lt;/code&gt; dataset. There’s not really anything visually compelling here, but it gives the starting point.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/baby_steps.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-ggplot2-and-plotly&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.thedatadiary.net/blog/2021-01-17-r-ggplot2-plotly/&#34;&gt;R, ggplot2, and plotly&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This next plot shows my first attempt at creating a &lt;code&gt;ggplot&lt;/code&gt;. I remember struggling &lt;em&gt;a lot&lt;/em&gt; with this when trying to learn the ins and outs of putting together and formatting the plot, but that struggle was well worth it. I learned not only the basics of how to put together a plot with &lt;code&gt;ggplot&lt;/code&gt;, but also, more importantly, how to search and troubleshoot issues. I also like that I was able to explore a topic visually with this plot: while the winner of the presidential election overperforms in the electoral college relative to the popular vote, republican candidates consistently have a slightly stronger electoral college overperformance due to small-state bias.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/r_ggplot2_plotly.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kind-of-projecting-the-2020-election&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.thedatadiary.net/blog/2021-02-21-kind-of-projecting-the-2020-election/&#34;&gt;(Kind of) Projecting the 2020 Election&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I started playing around with custom themes, and even setup my own &lt;a href=&#34;https://github.com/markjrieke/thedatadiary/blob/main/dd_theme_elements/dd_theme_elements.R&#34;&gt;theme elements&lt;/a&gt; so that I could reference them easily. This was the first time I broke away from the default theme for &lt;code&gt;ggplot&lt;/code&gt;. Additionally, this was the first time I used any sort of statistical methods to make a projection. The projection itself is pretty bad/underconfident, but the methodology was sound.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/projecting_2020_election.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;doug-collins-saved-raphael-warnocks-senate-bid&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.thedatadiary.net/blog/2021-02-28-doug-collins-saved-raphael-warnock-s-senate-bid/&#34;&gt;Doug Collins Saved Raphael Warnock’s Senate Bid&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Of all the posts I wrote in 2021, this may be the one I’m most proud of. Firstly, I learned a lot of new techniques needed to create maps and animations in R, but I also made a data-backed point: Raphael Warnock, one of the current Democratic senators from Georgia, likely won his election because Republican voters split their ticket between Kelly Loeffler and Doug Collins.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/ga_sen_01.png&#34; /&gt;
&lt;img src=&#34;pics/ga_sen_02.png&#34; /&gt;
&lt;img src=&#34;pics/ga_sen_03.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;artwork&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.thedatadiary.net/blog/2021-03-07-artwork/&#34;&gt;aRtwork!&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This next one is just fun, but I used R to make some artwork! I liked it so much, that I ended up making this my site header.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/artwork.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidymodels-and-the-titanic&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.thedatadiary.net/blog/2021-08-08-tidymodels-and-the-titanic/&#34;&gt;Tidymodels and the Titanic&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I spent a lot of time this past year learning how to implement machine learning methods, but eventually got to the point where I feel confident building and troubleshooting models with the tidymodel framework. I had an “aha” moment when working on a classifier for everyone’s favorite dataset, the &lt;a href=&#34;https://www.kaggle.com/c/titanic&#34;&gt;Titanic survival dataset&lt;/a&gt;, and everything &lt;em&gt;finally&lt;/em&gt; clicked. The model wasn’t great, but being able to quickly build and iterate was game-changing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/titanic.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;diamonds-are-forever-feature-engineering-with-the-diamonds-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/&#34;&gt;Diamonds are Forever: Feature Engineering with the Diamonds Dataset&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;With some machine learning under my belt, I spent some time practicing feature engineering with the &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/diamonds.html&#34;&gt;diamonds dataset&lt;/a&gt;. From the variable importance plot, I found that some of the engineered features were among the most important for predicting a diamond’s price!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/diamonds_01.png&#34; /&gt;
&lt;img src=&#34;pics/diamonds_02.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;do-voters-want-democrats-or-republicans-in-congress&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;a href=&#34;https://www.thedatadiary.net/blog/2021-12-14-do-voters-want-democrats-or-republicans-in-congress/&#34;&gt;Do Voters Want Democrats or Republicans in Congress?&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Most recently, I created a &lt;a href=&#34;https://github.com/markjrieke/2022-midterm-forecasts/blob/main/scripts/generic_ballot_weighting.R&#34;&gt;congressional ballot aggregator&lt;/a&gt; that weights polls by pollster, recency, sample size, and methodology. This was a huge effort to create a custom regression methodology, and I’m very happy with how it turned out! As of today, voters are just about even-split between Democrats and Republicans in the upcoming 2022 midterms.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/generic_ballot.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;plans-for-2022&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plans for 2022&lt;/h2&gt;
&lt;p&gt;I think I’ve come a long way in 2021 and I hope that in 2023, I can look back on 2022 and see a similar level of growth throughout the year. Here are a few things I plan on working on this year:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Writing&lt;/strong&gt;: I’ve spent a lot of time in 2021 working on technical skills, but haven’t really taken time to work on my writing. This will be an important focus for me in 2022, since technical information is useless if I’m not able to communicate it well.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bringing ML projects to the office&lt;/strong&gt;: In my current job, we’ve been spending the majority of the last six months focusing on troubleshooting the errors and getting over the speed-bumps involved with changing our primary survey vendor. We haven’t had the bandwidth to work on higher level/higher value projects, but should be able to do so in the upcoming year. Some projects I’m excited to work on this year include:
&lt;ul&gt;
&lt;li&gt;Variable importance in predicting positive sentiment in surveys;&lt;/li&gt;
&lt;li&gt;Patient segmentation with k-means clustering;&lt;/li&gt;
&lt;li&gt;Shiny App - “How confident am I?” - for educating our non-technical counterparts on confidence intervals vs. point estimates;&lt;/li&gt;
&lt;li&gt;Using NLP for predicting positive sentiment from patient comments;&lt;/li&gt;
&lt;li&gt;Topic modeling from comments for easier comment segmentation.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Natural Language Processing&lt;/strong&gt;: As alluded to above, I’d like to work with text data for predictive analysis this year — there’s a lot of valuable insight that can be drawn from text data once I understand how to extract it!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enrolling in a Master’s of D.S. program&lt;/strong&gt;: While I’ve been able to pick up a lot of knowledge from free/low-cost resources online (as well as spending countless hours on StackOverflow), I believe it’s time to further my formal education by pursuing a Master’s of Data Science part-time. Enrolling in a master’s program should help improve both my technical and non-technical skills, as well as formalize my transition from engineering to data science with a degree.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Forecasting the 2022 Midterms&lt;/strong&gt;: A long-term goal since starting this blog was to learn how to and ultimately deploy a forecast model for the 2022 midterms. I believe I’m well on my way, and hope to be able to publish midterm forecast models for the House, Senate, and Gubernatorial races sometime in the summer this year.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All-in-all, I’ve got a lot on my plate for 2022, but I’m confident that I’ll be able to tackle the challenges that come my way this year! My schedule is in a good place — full but manageable. As a preview of the next post I have scheduled, here’s a model I built to estimate Biden’s approval:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/biden_01.png&#34; /&gt;
&lt;img src=&#34;pics/biden_02.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Do Voters Want Democrats or Republicans in Congress?</title>
      <link>https://www.thedatadiary.net/blog/2021-12-14-do-voters-want-democrats-or-republicans-in-congress/</link>
      <pubDate>Tue, 14 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-12-14-do-voters-want-democrats-or-republicans-in-congress/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-12-14-do-voters-want-democrats-or-republicans-in-congress/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The 2022 midterms are still quite a ways away, however, in order to have a forecast ready in time, I need to start working on the model well in advance! One of the features I plan on using in the House Forecast is the &lt;a href=&#34;https://www.pewresearch.org/politics/2002/10/01/why-the-generic-ballot-test/&#34;&gt;generic congressional ballot&lt;/a&gt; average. Generic ballot polls ask respondents whether they intend to vote for either the Republican or Democratic candidate for the U.S. House of Representatives in their district. FiveThirtyEight provides a &lt;a href=&#34;https://projects.fivethirtyeight.com/congress-generic-ballot-polls/&#34;&gt;daily updating polling average&lt;/a&gt;, but in order to project beyond their current day’s average, I needed to build my own poll aggregator! Thankfully, they also are kind enough to provide the &lt;a href=&#34;https://github.com/fivethirtyeight/data/tree/master/polls&#34;&gt;underlying polling data&lt;/a&gt; as well.&lt;/p&gt;
&lt;p&gt;In this post, I’ll walk through the steps taken to build the generic ballot aggregator and explore the results.&lt;/p&gt;
&lt;div id=&#34;the-gameplan&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Gameplan&lt;/h3&gt;
&lt;p&gt;FiveThirtyEight’s data only goes back for two election cycles, which doesn’t provide enough data to build a poll aggregator from scratch. However, because they also provide their &lt;a href=&#34;https://projects.fivethirtyeight.com/generic-ballot-data/generic_ballot.csv&#34;&gt;historical trendline&lt;/a&gt;, I can build a poll aggregator that fits the polling results to their historical trend.&lt;/p&gt;
&lt;p&gt;In addition to the topline result, each poll in FiveThirtyEight’s repository includes quite a bit of additional meta-information about the poll: the pollster, sample size, survey methodology, and recipient population. Some pollsters, methodologies, and populations tend to be more accurate than others, so I’d be remiss to not include these features in the polling model. To incorporate all of this information, each feature will be weighted according to how well it fits FiveThirtyEight’s historical average. I’m sweeping a lot of programmatic detail under the rug here, but in general, features that fit FiveThirtyEight’s trendline well will have a higher weight and features that don’t fit the trendline so well will have a lower weight. Finally, since we know that some pollsters tend to favor one party, we’ll also create a “pollster offset” feature to shift each poll slightly and account for this partisan lean.&lt;/p&gt;
&lt;p&gt;Before digging any further, it may be beneficial to explore the polling repository a bit. Here’s a sample of the polling data used for the aggregator:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;cycle&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;pollster&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sample_size&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;population&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;methodology&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;end_date&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;dem&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;rep&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2018&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;HarrisX&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3861&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;lv&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Online&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2011-02-18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;48.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;42.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2018&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SurveyMonkey&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9532&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;a&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Online&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2010-07-18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;42.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2020&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;YouGov&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1200&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;rv&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Online&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2012-03-19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;47.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;41.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2018&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Ipsos&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;731&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;lv&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Online&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2009-06-18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;49.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;38.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2020&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;HarrisX&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3000&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;rv&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Online&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2010-07-19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;36.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2018&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Ipsos&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2383&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;a&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Online&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2009-01-17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;36.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;31.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2018&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Ipsos&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;663&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;lv&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Online&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2006-05-17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;38.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2018&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Morning Consult&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1992&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;rv&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Online&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2005-05-18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;42.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2018&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Ipsos&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1689&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;rv&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Online&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2007-03-18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;41.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;36.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2018&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Ipsos&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1907&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;a&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Online&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2004-02-18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;38.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30.3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The following pollsters conducted enough polls to warrant their own category - all other pollsters will be lumped together under the banner of “Other Pollster:”&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;pollster&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Ipsos&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1239&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;HarrisX&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;426&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Morning Consult&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;305&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;YouGov&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;195&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;USC Dornsife&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;71&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;The Winston Group&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;McLaughlin &amp;amp; Associates&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Firehouse Strategies/Øptimus&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Rasmussen Reports/Pulse Opinion Research&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Quinnipiac University&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here is a bit of expanded detail on the survey population categories:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;population&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;population_full&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;rv&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Registered Voters&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1510&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;lv&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Likely Voters&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;619&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;a&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Adults&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;602&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;v&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Voters&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Finally, the top survey methodologies considered are shown below. Similar to pollsters, methodologies that are not used enough are lumped into an “Other Methodology” category:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;methodology&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Online&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2425&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Live Phone&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;194&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unknown&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;62&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;IVR/Online&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;44&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The general plan of attack was to fit the polls to the trendline, measure the error, update the weights and offsets slightly, and repeat until the error is below an acceptable threshold. As time progresses, we make smaller and smaller updates to the weights and offsets we hone in on optimal values for minimizing error.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-the-results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploring the Results&lt;/h3&gt;
&lt;p&gt;First and foremost, I’m pleased to report that the model fit the historical data fairly well. The model’s trendline (in blue) reasonably matches FiveThirtyEight’s historical trendline (in red):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/final_fit.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, as the model trains the weights and offsets, the &lt;code&gt;rmse&lt;/code&gt; continues to shrink, and levels off just after 150 training rounds:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/rmse_tracker.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Some pollsters are better at fitting the trendline: YouGov polls end up with the highest weight in the model whereas Morning Consult polls are downweighted.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/pollster_weights.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What are the partisan leanings of each pollster? Each pollster offset is shown below - pollsters at the top are &lt;em&gt;generally&lt;/em&gt; more conservative leaning pollsters near the bottom are &lt;em&gt;generally&lt;/em&gt; more liberal leaning. Or rather, pollsters near the top fit the trend better when we adjust their results to favor democrats, and pollsters near the bottom fit better when adjusted in favor of republicans.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/pollster_offsets.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Some pollsters have fairly significant offsets! It’s reassuring to note, however, that these pollsters are downweighted and don’t make up the majority of polls recorded. The five pollsters with the most number of generic ballot polls conducted tend to fall fairly close to 0-offset.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/pollster_weights_summary_size.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although &lt;a href=&#34;https://www.nytimes.com/2019/07/02/upshot/online-polls-analyzing-reliability.html&#34;&gt;live phone polling has become more and more difficult&lt;/a&gt; in recent years, it still ended up being the top-weighted method for the generic ballot poll.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/methodology_weights.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, surveying a population of registered voters turned out to be best for matching polls to the trendline, whereas the “voters” population was filtered away entirely (though that may be because there were only two polls conducted with this population).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/population_weights.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-to-new-2022-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fitting to New 2022 Data&lt;/h3&gt;
&lt;p&gt;This exploration of how the model fits to the training data is useful, but how well does this model fit to new data? As it turns out, fairly well! When fit to data it wasn’t trained on, the model’s trendline (in blue) closely follows the target trendline (in red).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/fit_2022_comparison.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This also gives us the ability to project &lt;em&gt;beyond&lt;/em&gt; the current average! As we get further and further away from the current day, our confidence interval around the current average widens out.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/fit_2022_ed.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This behavior looks great! One area of concern, however, is the area on the left - on days where we already have data, we’re &lt;em&gt;way&lt;/em&gt; too confident in our estimate! On some days, the confidence interval only spans 0.5%, which simply isn’t realistic.&lt;/p&gt;
&lt;p&gt;To deal with this, we’ll create a new feature, &lt;code&gt;downweight&lt;/code&gt;, that we can train to widen our confidence interval to something more credible.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-more-round-of-training&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;One More Round of Training&lt;/h3&gt;
&lt;p&gt;Similar to training the generic ballot average, we can train the &lt;code&gt;downweight&lt;/code&gt; feature to match the model’s confidence interval to FiveThirtyEight’s confidence interval. The trained result isn’t a perfect fit, but this does give us a mathematically sound, consistent way to construct a more realistic confidence interval. Here’s how the model’s confidence interval compares to the training set:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/ci_fit_historical.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On new data, the model similarly provides an okay-enough fit for our purposes. The model may find it difficult to recreate FiveThirtyEight’s confidence interval due to differences in model construction. &lt;a href=&#34;https://fivethirtyeight.com/features/how-were-tracking-joe-bidens-approval-rating/&#34;&gt;According to FiveThirtyEight’s documentation&lt;/a&gt; on other polling averages, it looks like they use an ensemble of &lt;a href=&#34;https://en.wikipedia.org/wiki/Local_regression&#34;&gt;LOESS models&lt;/a&gt;, whereas I generate a single &lt;a href=&#34;https://en.wikipedia.org/wiki/Beta_distribution&#34;&gt;beta distribution&lt;/a&gt;. Without getting too far into the weeds, the stack of LOESS models results in a confidence interval that is less prone to quick growth.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/ci_fit_current.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-it-all-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wrapping It All Up&lt;/h3&gt;
&lt;p&gt;Now that we’ve defined a model, we can finally answer the question in the title: do voters want Democrats or Republicans in congress? According to this model, voters are as even split as you can get!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/generic_ballot_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even if there was a leader in the polls, the generic ballot’s predictive power this far out from the house elections is, to a first approximation, effectively useless. As we get closer to the election day, however, the polling model will become more and more useful. Additionally, the methods I used to generate the polling average can be replicated (with slight modifications) for other polling averages that will feed into the model, so stay tuned for more!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-programming-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some Programming Notes&lt;/h3&gt;
&lt;p&gt;I’ve skipped over quite a bit of programming detail. If you’re so inclined, you can read through the &lt;a href=&#34;https://github.com/markjrieke/electiondata/blob/main/scripts/generic_ballot_weighting.R&#34;&gt;script to build the model and generate plots&lt;/a&gt;. There’s a &lt;em&gt;lot&lt;/em&gt; going on there, but I’d like to highlight a few packages that were instrumental for putting this together:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/doParallel/index.html&#34;&gt;&lt;code&gt;doParallel&lt;/code&gt;&lt;/a&gt;: I frequently use &lt;code&gt;doParallel&lt;/code&gt;, which uses multiple cores running in parallel to speed up processes. Most of the time, loading &lt;code&gt;doParallel&lt;/code&gt; is just done for convenience’s sake, but it was absolutely necessary for this project — even with parallel processing, each round of model training was lengthy.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://furrr.futureverse.org/&#34;&gt;&lt;code&gt;furrr&lt;/code&gt;&lt;/a&gt;: the goal of &lt;code&gt;furrr&lt;/code&gt; is to combine &lt;code&gt;purrr&lt;/code&gt;’s family of mapping functions with &lt;code&gt;future&lt;/code&gt;’s parallel processing capabilities. Replacing my &lt;code&gt;purrr::map()&lt;/code&gt; functions with &lt;code&gt;furrr::future_map()&lt;/code&gt; functions literally cut training time in half.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/GuangchuangYu/shadowtext&#34;&gt;&lt;code&gt;shadowtext&lt;/code&gt;&lt;/a&gt;: &lt;code&gt;shadowtext&lt;/code&gt; is a small, niche package that allows you to add colored borders to &lt;code&gt;geom_text()&lt;/code&gt; objects in a ggplot. It’s not visible in today’s chart because there’s 100% overlap between democrats &amp;amp; republicans, but there’s a white border around the text in front of the trendline that just looks nice.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Technical Books!</title>
      <link>https://www.thedatadiary.net/blog/2021-11-28-technical-books/</link>
      <pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-11-28-technical-books/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-11-28-technical-books/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Happy (belated) Thanksgiving! This year, my family drove down to Houston for the holiday &amp;amp; I hosted Thanksgiving for the first time. We played lots of games and ate well - my fridge is &lt;em&gt;still&lt;/em&gt; stocked full of leftovers. Knowing we’d be busy with hosting, I planned ahead and scheduled a lighter post - this week, I thought I’d highlight some technical books that I’ve either referenced for modeling work, have been recommended to me, or I’ve heard about and would like to read:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf&#34;&gt;The Elements of Statistical Learning&lt;/a&gt;&lt;/strong&gt; is referenced as the Bible of Machine Learning by &lt;a href=&#34;https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw&#34;&gt;Josh Starmer&lt;/a&gt; and provides a robust and deeply technical foundation for a wide array of machine learning models. It’s considered a must-have among both machine learning theorists, who look for new model structures, and practitioners (like myself!).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://web.stanford.edu/~hastie/ISLR2/ISLRv2_website.pdf&#34;&gt;An Introduction to Statistical Learning with Applications in R&lt;/a&gt;&lt;/strong&gt; is a companion to &lt;em&gt;The Elements of Statistical Learning&lt;/em&gt;. &lt;em&gt;An Introduction to Statistical Learning&lt;/em&gt; arose as a broader and less technical treatment of the key topics discussed in &lt;em&gt;The Elements of Statistical Learning&lt;/em&gt;. Each section also includes learning-lab lessons walking through the implementation of the statistical learning method from that chapter (&lt;a href=&#34;https://www.emilhvitfeldt.com/&#34;&gt;Emil Hvitfeldt&lt;/a&gt; is also working on a &lt;a href=&#34;https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/&#34;&gt;companion site&lt;/a&gt; for completing the labs with tidymodels).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.tmwr.org/index.html&#34;&gt;Tidy Modeling with R&lt;/a&gt;&lt;/strong&gt; is a guide to using the tidymodel framework and has been an excellent reference in both personal and professional projects.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.tidytextmining.com/&#34;&gt;Text Mining with R: a Tidy Approach&lt;/a&gt;&lt;/strong&gt; serves as an introduction to text mining and other methods for dealing with unstructured, non-rectangular data. In my current role as a Consumer Experience Analyst, I have to interact with unstructured data (in the form of patient comments) daily - this book, along with the &lt;a href=&#34;https://juliasilge.github.io/tidytext/&#34;&gt;tidytext&lt;/a&gt; package, have been incredibly useful for analyzing and visualizing text data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://smltar.com/preface.html&#34;&gt;Supervised Machine Learning for Text Analysis in R&lt;/a&gt;&lt;/strong&gt; picks up where &lt;em&gt;Text Mining with R&lt;/em&gt; left off by exploring (as the title suggests) supervised machine learning methods with text data. While I haven’t done extensive text modeling, this is one area that I’d like to explore further in 2022.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;http://www.feat.engineering/&#34;&gt;Feature Engineering and Selection: A Practical Approach for Predictive Models&lt;/a&gt;&lt;/strong&gt; is a guidebook offering methods for feature engineering (transforming and creating new predictor variables to improve predictive model performance). While I’ve utilized some basic feature engineering in some of my work, I’m interested in adding more robust tools to my feature-engineering toolkit!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://drob.gumroad.com/l/empirical-bayes&#34;&gt;Introduction to Empirical Bayes&lt;/a&gt;&lt;/strong&gt; is &lt;a href=&#34;http://varianceexplained.org/about/&#34;&gt;David Robinson’s&lt;/a&gt; book coalescing a series of blog posts on Bayesian estimation, credible intervals, A/B testing, mixed models, and a host of other methods, all through the example of baseball batting averages.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://jnolis.com/book/&#34;&gt;Build a Career in Data Science&lt;/a&gt;&lt;/strong&gt; is, as the name suggests, a book about building a career in data science. I generally feel that most career-help books are too broad to be useful or offer non-novel information for those in the industry the book is written for. Given, however, that I don’t have an academic or professional background in the field and that I’d like to eventually move from analytics to data science, I’d like to add this to the collection to pick up on some best practices.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Diamonds are Forever: Feature Engineering with the Diamonds Dataset</title>
      <link>https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/</link>
      <pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Are y’all ready for some charts?? This week, I did a bit of machine learning practice with the &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/diamonds.html&#34;&gt;&lt;code&gt;diamonds dataset&lt;/code&gt;&lt;/a&gt;. This dataset is interesting and good for practice for a few reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there are lots of observations (50,000+);&lt;/li&gt;
&lt;li&gt;it includes a mix of numeric and categorical variables;&lt;/li&gt;
&lt;li&gt;there are some data oddities to deal with (log scales, interactions, non-linear relations)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ll be doing a bit of feature engineering prior to fitting an tuning a linear model that predicts the each diamond’s &lt;code&gt;price&lt;/code&gt; with the &lt;a href=&#34;https://glmnet.stanford.edu/index.html&#34;&gt;&lt;code&gt;glmnet&lt;/code&gt;&lt;/a&gt; package. This will give a good end-to-end glimpse into the data exploration and model fitting process! Before we get into that, let’s load some packages and get a preview of the dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidymodels)
library(vip)

theme_set(theme_minimal())

diamonds %&amp;gt;%
  slice_head(n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 10
##    carat cut       color clarity depth table price     x     y     z
##    &amp;lt;dbl&amp;gt; &amp;lt;ord&amp;gt;     &amp;lt;ord&amp;gt; &amp;lt;ord&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43
##  2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31
##  3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31
##  4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63
##  5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75
##  6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48
##  7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47
##  8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53
##  9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49
## 10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we’re predicting price, let’s look at its distribution first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = price)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We’re definitely gonna want to apply a transformation to the price when modeling - let’s look at the distribution on a log-10 scale.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = price)) +
  geom_histogram() +
  scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s a lot more evenly distributed, if not perfect. That’s a fine starting point, so now we’ll look through the rest of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = carat)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = cut,
             y = price)) +
  geom_boxplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  count(cut) %&amp;gt;%
  ggplot(aes(x = cut,
             y = n)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-3.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = color,
             y = price)) +
  geom_boxplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-4.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  count(color) %&amp;gt;%
  ggplot(aes(x = color,
             y = n)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-5.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = clarity,
             y = price)) +
  geom_boxplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-6.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  count(clarity) %&amp;gt;%
  ggplot(aes(x = clarity,
             y = n)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-7.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = depth)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-8.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = table)) +
  geom_histogram() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-9.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = x)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-10.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = y)) +
  geom_histogram() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-11.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;% 
  ggplot(aes(x = z)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-12.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like there may be a good opportunity to try out a few normalization and resampling techniques, but before we get into any of that, let’s build a baseline linear model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# splits
diamonds_split &amp;lt;- initial_split(diamonds)
diamonds_train &amp;lt;- training(diamonds_split)
diamonds_test &amp;lt;- testing(diamonds_split)

# resamples (don&amp;#39;t want to use testing data!)
diamonds_folds &amp;lt;- vfold_cv(diamonds_train)

# model spec
mod01 &amp;lt;-
  linear_reg() %&amp;gt;%
  set_engine(&amp;quot;lm&amp;quot;)

# recipe
rec01 &amp;lt;-
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_dummy(all_nominal_predictors())

# controls
ctrl_preds &amp;lt;- 
  control_resamples(save_pred = TRUE)

# create a wf
wf01 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod01) %&amp;gt;%
  add_recipe(rec01)

# parallel processing
doParallel::registerDoParallel()

# fit
rs01 &amp;lt;- 
  fit_resamples(
    wf01,
    diamonds_folds,
    control = ctrl_preds
  )

# metrics!
collect_metrics(rs01)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator     mean     n  std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   1129.       10 10.2     Preprocessor1_Model1
## 2 rsq     standard      0.920    10  0.00160 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And right off the bat, we can see a fairly high value for &lt;code&gt;rsq&lt;/code&gt;! However, &lt;code&gt;rsq&lt;/code&gt; doesn’t tell the whole story, so we should check our predictions and residuals plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;augment(rs01) %&amp;gt;%
  ggplot(aes(x = price,
             y = .pred)) +
  geom_point(alpha = 0.01) +
  geom_abline(linetype = &amp;quot;dashed&amp;quot;,
              size = 0.1,
              alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is &lt;em&gt;definitely&lt;/em&gt; not what we want to see! It looks like there’s an odd curve/structure to the graph and we’re actually predicting quite a few negative values. The residuals plot doesn’t look too great either.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;augment(rs01) %&amp;gt;%
  ggplot(aes(x = price,
             y = .resid)) +
  geom_point(alpha = 0.01) +
  geom_hline(yintercept = 0,
             linetype = &amp;quot;dashed&amp;quot;,
             alpha = 0.5,
             size = 0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What we’d like to see is a 0-correlation plot with errors normally distributed; what we’re seeing instead, however, is a ton of structure.&lt;/p&gt;
&lt;p&gt;That being said, that’s okay! we expected this first pass to be pretty rough! And the price is &lt;em&gt;clearly&lt;/em&gt; on a log-10 scale. To make apples-apples comparisons with models going forward, I’ll retrain this basic linear model to predict the &lt;code&gt;log10(price)&lt;/code&gt;. This’ll involve a bit of data re-manipulation!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# log transform price
diamonds_model &amp;lt;-
  diamonds %&amp;gt;%
  mutate(price = log10(price),
         across(cut:clarity, as.character))

# bad practice copy + paste lol

# splits
set.seed(999)
diamonds_split &amp;lt;- initial_split(diamonds_model)
diamonds_train &amp;lt;- training(diamonds_split)
diamonds_test &amp;lt;- testing(diamonds_split)

# resamples (don&amp;#39;t want to use testing data!)
set.seed(888)
diamonds_folds &amp;lt;- vfold_cv(diamonds_train)

# model spec
mod01 &amp;lt;-
  linear_reg() %&amp;gt;%
  set_engine(&amp;quot;lm&amp;quot;)

# recipe
rec01 &amp;lt;-
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_dummy(all_nominal_predictors())

# controls
ctrl_preds &amp;lt;- 
  control_resamples(save_pred = TRUE)

# create a wf
wf01 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod01) %&amp;gt;%
  add_recipe(rec01)

# parallel processing
doParallel::registerDoParallel()

# fit
set.seed(777)
rs01 &amp;lt;- 
  fit_resamples(
    wf01,
    diamonds_folds,
    control = ctrl_preds
  )

# metrics!
collect_metrics(rs01)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator   mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.0793    10 0.00557 Preprocessor1_Model1
## 2 rsq     standard   0.966     10 0.00494 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And wow, that &lt;em&gt;one&lt;/em&gt; transformation increased our &lt;code&gt;rsq&lt;/code&gt; to 0.96! Again, that’s not the whole story, and we’re going to be evaluating models based on the &lt;code&gt;rmse&lt;/code&gt;. Let’s look at how our prediction map has updated:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rs01 %&amp;gt;%
  augment() %&amp;gt;%
  ggplot(aes(x = price,
             y = .pred)) +
  geom_point(alpha = 0.01) +
  geom_abline(linetype = &amp;quot;dashed&amp;quot;,
              size = 0.1,
              alpha = 0.5) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now &lt;em&gt;that&lt;/em&gt; is a much better starting place to be at! Let’s look at our coefficients&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(666) # :thedevilisalive:
wf01 %&amp;gt;%
  fit(diamonds_train) %&amp;gt;%
  pull_workflow_fit() %&amp;gt;%
  vip::vi() %&amp;gt;%
  mutate(Variable = fct_reorder(Variable, Importance)) %&amp;gt;%
  ggplot(aes(x = Variable,
             y = Importance,
             fill = Sign)) +
  geom_col() +
  coord_flip() + 
  theme(plot.title.position = &amp;quot;plot&amp;quot;) +
  labs(x = NULL,
       y = NULL,
       title = &amp;quot;Diamonds are forever&amp;quot;,
       subtitle = &amp;quot;Variable importance plot of a basic linear regression predicting diamond price&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another way of looking at it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(666)
wf01 %&amp;gt;%
  fit(diamonds_train) %&amp;gt;%
  pull_workflow_fit() %&amp;gt;%
  vip::vi() %&amp;gt;%
  mutate(Importance = if_else(Sign == &amp;quot;NEG&amp;quot;, Importance * -1, Importance),
         Variable = fct_reorder(Variable, Importance)) %&amp;gt;%
  ggplot(aes(x = Variable,
             y = Importance,
             fill = Sign)) +
  geom_col() +
  coord_flip() +
  labs(title = &amp;quot;Diamonds are forever&amp;quot;,
       subtitle = &amp;quot;Variable importance plot of a basic linear regression predicting diamond price&amp;quot;,
       x = NULL,
       y = NULL) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a good, but definitely improvable, starting point. We can likely decrease our overall error with a bit of feature engineering and drop unimportant features by tuning a regularized model. There are some oddities in this initial model that will need to be improved upon; for one, we can definitively say that the &lt;code&gt;carat&lt;/code&gt; feature ought to be &lt;em&gt;positively&lt;/em&gt; associated with price&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  ggplot(aes(x = carat,
             y = price)) +
  geom_point(alpha = 0.01) +
  labs(title = &amp;quot;A clear positive (albeit nonlinear) relationship between `carat` and `price`&amp;quot;) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another few things that are interesting to note in this plot! It looks like there are clusterings of carat ratings around round-ish numbers. My hypothesis here is that carat ratings tend to get rounded up to the next size. There’s also a clear abscence of diamonds priced at $1,500 (~3.17 on the log10 scale). I suppose there is some industry-specific reason to avoid a diamond price of $,1500?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  ggplot(aes(x = carat,
             y = price)) +
  geom_point(alpha = 0.01) +
  labs(title = &amp;quot;A clear positive (albeit nonlinear) relationship between `carat` and `price`&amp;quot;) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;) +
  geom_hline(yintercept = log10(1500),
             linetype = &amp;quot;dashed&amp;quot;,
             size = 0.9,
             alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;How to address all these things? With some feature engineering! Firstly, let’s add some recipe steps to balance classes &amp;amp; normalize continuous variables.&lt;/p&gt;
&lt;p&gt;But before I get into &lt;em&gt;that&lt;/em&gt;, I’ll save the resample metrics so that we can compare models!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics &amp;lt;- collect_metrics(rs01) %&amp;gt;% mutate(model = &amp;quot;model01&amp;quot;)

metrics&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 7
##   .metric .estimator   mean     n std_err .config              model  
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt;  
## 1 rmse    standard   0.0793    10 0.00557 Preprocessor1_Model1 model01
## 2 rsq     standard   0.966     10 0.00494 Preprocessor1_Model1 model01&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# spec will be the same as model01
mod02 &amp;lt;- mod01

# recipe!
rec02 &amp;lt;- 
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_other(cut, color, clarity) %&amp;gt;% 
  step_dummy(all_nominal_predictors(), -cut) %&amp;gt;%
  
  # use smote resampling to balance classes
  themis::step_smote(cut) %&amp;gt;% 
    
  # normalize continuous vars
  bestNormalize::step_best_normalize(carat, depth, table, x, y, z)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s &lt;a href=&#34;https://recipes.tidymodels.org/reference/bake.html&#34;&gt;bake&lt;/a&gt; our recipe to verify that everything looks up-to-snuff in the preprocessed dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 &amp;lt;- 
  rec02 %&amp;gt;%
  prep() %&amp;gt;%
  bake(new_data = NULL)

baked_rec02&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 80,495 x 20
##      carat cut        depth  table       x       y      z price color_E color_F
##      &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 -0.706  Premium    0.138 -0.760 -0.709  -0.738  -0.695  3.01       0       1
##  2  0.356  Very Good  0.570  0.835  0.342   0.251   0.344  3.63       0       0
##  3  0.214  Premium   -0.308  0.835  0.293   0.263   0.166  3.58       0       0
##  4 -1.08   other      1.04  -0.310 -1.30   -1.40   -0.995  2.70       0       0
##  5 -0.641  Ideal     -0.602 -0.760 -0.595  -0.560  -0.622  2.97       0       0
##  6 -0.0759 Premium   -0.602  0.494 -0.0349 -0.0460 -0.114  3.38       0       0
##  7 -0.149  Premium   -1.16   0.103 -0.0565 -0.0842 -0.246  3.44       1       0
##  8  0.170  Very Good -0.371  0.494  0.178   0.313   0.130  3.56       0       1
##  9 -0.736  Ideal     -0.110 -0.760 -0.709  -0.738  -0.723  3.09       0       0
## 10  0.782  Ideal     -0.602 -0.310  0.819   0.846   0.732  4.02       0       0
## # ... with 80,485 more rows, and 10 more variables: color_G &amp;lt;dbl&amp;gt;,
## #   color_H &amp;lt;dbl&amp;gt;, color_I &amp;lt;dbl&amp;gt;, color_J &amp;lt;dbl&amp;gt;, clarity_SI2 &amp;lt;dbl&amp;gt;,
## #   clarity_VS1 &amp;lt;dbl&amp;gt;, clarity_VS2 &amp;lt;dbl&amp;gt;, clarity_VVS1 &amp;lt;dbl&amp;gt;,
## #   clarity_VVS2 &amp;lt;dbl&amp;gt;, clarity_other &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  count(cut) %&amp;gt;%
  ggplot(aes(x = cut,
             y = n)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  ggplot(aes(x = carat)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-2.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  ggplot(aes(x = depth)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-3.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  ggplot(aes(x = table)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-4.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  ggplot(aes(x = x)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-5.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  ggplot(aes(x = y)) +
  geom_histogram() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-6.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  ggplot(aes(x = z)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-7.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Everything looks alright with the exception of the &lt;code&gt;table&lt;/code&gt; predictor. I wonder if there are a lot of repeated values in the &lt;code&gt;table&lt;/code&gt; variable - that may be why we’re seeing a “chunky” histogram. Let’s check&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  count(table) %&amp;gt;%
  arrange(desc(n))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10,406 x 2
##     table     n
##     &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
##  1  0.103 12167
##  2 -0.310 11408
##  3 -0.760 11031
##  4  0.494  9406
##  5 -1.28   6726
##  6  0.835  6165
##  7  1.15   3810
##  8 -1.85   2789
##  9  1.42   2182
## 10  1.64    972
## # ... with 10,396 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ooh - okay yeah that’s definitely the issue! I’m not &lt;em&gt;quite&lt;/em&gt; sure how to deal with it, so we’re just going to ignore for now! Let’s add a new model &amp;amp; see how it compares against the baseline transformed model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wf02 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod02) %&amp;gt;%
  add_recipe(rec02)

# stop parallel to avoid error!
# need to replace with PSOCK clusters
# see github issue here: https://github.com/tidymodels/recipes/issues/847
foreach::registerDoSEQ()

set.seed(666) # spoopy
rs02 &amp;lt;-
  fit_resamples(
    wf02,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs02)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator  mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.115    10 0.00143 Preprocessor1_Model1
## 2 rsq     standard   0.932    10 0.00161 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Oof - that’s actually slightly worse than our baseline model!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rs02 %&amp;gt;%
  augment() %&amp;gt;%
  ggplot(aes(x = price,
             y = .pred)) +
  geom_point(alpha = 0.01) +
  geom_abline(linetype = &amp;quot;dashed&amp;quot;,
              size = 0.1,
              alpha = 0.5) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like we’ve introduced structure into the residual plot!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rs02 %&amp;gt;%
  augment() %&amp;gt;%
  ggplot(aes(x = price,
             y = .resid)) +
  geom_point(alpha = 0.01) +
  geom_hline(yintercept = 0,
             linetype = &amp;quot;dashed&amp;quot;,
             size = 0.1,
             alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yeah that’s fairly wonky! I’m wondering if it’s due to the SMOTE upsampling method we introduced? To counteract, I’ll build &amp;amp; train new models after each set of recipe steps (e.g., resampling, normalizing, interactions) to buil up a better performing model one step at a time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics &amp;lt;- 
  metrics %&amp;gt;%
  bind_rows(collect_metrics(rs02) %&amp;gt;% mutate(model = &amp;quot;model02&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# same model spec
mod03 &amp;lt;- mod02

# rebuild rec+wf &amp;amp; retrain
rec03 &amp;lt;- 
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_other(cut, color, clarity) %&amp;gt;%
  step_dummy(all_nominal_predictors(), -cut) %&amp;gt;%
  themis::step_smote(cut)

wf03 &amp;lt;- 
  workflow() %&amp;gt;%
  add_model(mod03) %&amp;gt;%
  add_recipe(rec03)

# do paralllel
doParallel::registerDoParallel()

# refit!
set.seed(123)
rs03 &amp;lt;-
  fit_resamples(
    wf03,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs03)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator   mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.0918    10 0.00502 Preprocessor1_Model1
## 2 rsq     standard   0.956     10 0.00502 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interesting! Improved relative to &lt;code&gt;rs02&lt;/code&gt;, but still not as good as our first model! Let’s try using &lt;code&gt;step_downsample()&lt;/code&gt; to balance classes &amp;amp; see how we fare.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# cleanup some large-ish items eating up memory
rm(mod01, mod02, rec01, rec02, wf01, wf02, rs01, rs02)

# save metrics
metrics &amp;lt;- 
  metrics %&amp;gt;%
  bind_rows(collect_metrics(rs03) %&amp;gt;% mutate(model = &amp;quot;model03&amp;quot;))

# new mod
mod04 &amp;lt;- mod03

# new rec
rec04 &amp;lt;-
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_other(cut, color, clarity) %&amp;gt;%
  step_dummy(all_nominal_predictors(), -cut) %&amp;gt;%
  themis::step_downsample(cut)

wf04 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod04) %&amp;gt;%
  add_recipe(rec04) 

set.seed(456) 
rs04 &amp;lt;-
  fit_resamples(
    wf04,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs04)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator  mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.116    10  0.0136 Preprocessor1_Model1
## 2 rsq     standard   0.927    10  0.0175 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wow - still a bit worse! I’ll try upsampling &amp;amp; if there is no improvement, we’ll move on without resampling!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics &amp;lt;-
  metrics %&amp;gt;%
  bind_rows(collect_metrics(rs04) %&amp;gt;% mutate(model = &amp;quot;model04&amp;quot;))

mod05 &amp;lt;- mod04

rec05 &amp;lt;-
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_other(cut, color, clarity) %&amp;gt;%
  step_dummy(all_nominal_predictors(), -cut) %&amp;gt;%
  themis::step_upsample(cut)

wf05 &amp;lt;- 
  workflow() %&amp;gt;%
  add_model(mod05) %&amp;gt;%
  add_recipe(rec05) 

set.seed(789)
rs05 &amp;lt;-
  fit_resamples(
    wf05,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs05)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator  mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.101    10 0.00499 Preprocessor1_Model1
## 2 rsq     standard   0.947    10 0.00536 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Okay - resampling gets stricken off our list of recipe steps! Let’s look at how the models compare so far&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics &amp;lt;-
  metrics %&amp;gt;%
  bind_rows(collect_metrics(rs05) %&amp;gt;% mutate(model = &amp;quot;model05&amp;quot;))

metrics %&amp;gt;%
  ggplot(aes(x = model)) +
  geom_point(aes(y = mean)) +
  geom_errorbar(aes(ymin = mean - std_err,
                    ymax = mean + std_err)) +
  facet_wrap(~.metric, scales = &amp;quot;free_y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The first simple linear model was the best as measured by both metrics! Let’s see if we can improve with some normalization of the continuous vars.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(mod03, mod04, rec03, rec04, rs03, rs04, wf03, wf04)

mod06 &amp;lt;- mod05

rec06 &amp;lt;-
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_other(cut, color, clarity) %&amp;gt;%
  bestNormalize::step_best_normalize(all_numeric_predictors()) %&amp;gt;%
  step_dummy(all_nominal_predictors())

wf06 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod06) %&amp;gt;%
  add_recipe(rec06)

foreach::registerDoSEQ()
set.seed(101112)
rs06 &amp;lt;-
  fit_resamples(
    wf06,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs06)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator  mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.127    10 0.00115 Preprocessor1_Model1
## 2 rsq     standard   0.916    10 0.00136 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well - that was quite a bit for no improvement! I guess that normalizing the continuous vars in this case isn’t helping. Moving on to adding some interactions - first let’s explore potential interactions a bit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics &amp;lt;-
  metrics %&amp;gt;% 
  bind_rows(collect_metrics(rs06) %&amp;gt;% mutate(model = &amp;quot;model06&amp;quot;))

diamonds_train %&amp;gt;%
  ggplot(aes(x = carat,
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) + 
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(splines)
diamonds_train %&amp;gt;%
  ggplot(aes(x = carat,
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = lm,
              formula = y ~ ns(x, df = 5),
              se = FALSE) +
  facet_wrap(~cut, scales = &amp;quot;free&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;5 spline terms might not be sufficient here - capturing the lower bound well but &lt;em&gt;really&lt;/em&gt; not doing well with the higher carat diamonds.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  ggplot(aes(x = carat,
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = lm,
              formula = y ~ ns(x, df = 10),
              se = FALSE) +
  facet_wrap(~cut, scales = &amp;quot;free&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hmmmm, 10 might be too many. It looks lie we’ll just lose a bit of confidence for the Premium &amp;amp; Very Good diamonds at higher carats. Relative to the total number, I’m not too concerned.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  ggplot(aes(x = carat,
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = lm,
              formula = y ~ ns(x, df = 7),
              se = FALSE) +
  facet_wrap(~cut, scales = &amp;quot;free&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;7 terms feels like the best we’re going to do here - I think this is tuneable, but we’ll leave as is (now &amp;amp; in the final model).&lt;/p&gt;
&lt;p&gt;Next, we’ll look at creating interactions between the &lt;code&gt;color&lt;/code&gt; and &lt;code&gt;carat&lt;/code&gt; variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  ggplot(aes(x = carat, 
             y = price,
             color = color)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = lm, 
              formula = y ~ ns(x, df = 15),
              se = FALSE) +
  facet_wrap(~color)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Adding interactive spline terms with &lt;code&gt;df&lt;/code&gt; of 15 seems to add some useful information!&lt;/p&gt;
&lt;p&gt;We have three shape parameters, &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, and &lt;code&gt;z&lt;/code&gt; - I wonder if creating a stand-in for volume by multiplying them all together will provide any useful information?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  mutate(volume_param = x * y * z) %&amp;gt;%
  ggplot(aes(x = volume_param,
             y = price)) +
  geom_point(alpha = 0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ooh, looks like we’re getting some good info here, but we may want to use &lt;code&gt;log10&lt;/code&gt; to scale this back.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  mutate(volume_param = log10(x * y * z)) %&amp;gt;%
  ggplot(aes(x = volume_param, 
             y = price)) +
  geom_point(alpha = 0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s see if this ought to interact with any other paramaters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  mutate(volume_param = log10(x * y * z)) %&amp;gt;%
  ggplot(aes(x = volume_param, 
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-35-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  mutate(volume_param = log10(x * y * z)) %&amp;gt;%
  ggplot(aes(x = volume_param, 
             y = price,
             color = color)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-35-2.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  mutate(volume_param = log10(x * y * z)) %&amp;gt;%
  ggplot(aes(x = volume_param, 
             y = price,
             color = clarity)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-35-3.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hmm, it doesn’t really look like we’re capturing too great of interactions, so I’ll leave out for now. It looks like the &lt;em&gt;size&lt;/em&gt; of the rock is more important than anything else! I could continue to dig further, but I’ll stop there. I’m likely getting diminishing returns, &amp;amp; I’d like to get back into modeling!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod07 &amp;lt;- mod06

rec07 &amp;lt;-
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_other(cut, color, clarity) %&amp;gt;%
  step_dummy(all_nominal_predictors()) %&amp;gt;%
  step_interact(~carat:starts_with(&amp;quot;cut_&amp;quot;)) %&amp;gt;%
  step_interact(~carat:starts_with(&amp;quot;color_&amp;quot;)) %&amp;gt;%
  step_mutate_at(c(x, y, z),
                 fn = ~if_else(.x == 0, mean(.x), .x)) %&amp;gt;%
  step_mutate(volume_param = log10(x * y * z)) %&amp;gt;%
  step_ns(starts_with(&amp;quot;carat_x_cut&amp;quot;), deg_free = 7) %&amp;gt;%
  step_ns(starts_with(&amp;quot;carat_x_color&amp;quot;), deg_free = 15) 

rec07&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          9
## 
## Operations:
## 
## Collapsing factor levels for cut, color, clarity
## Dummy variables from all_nominal_predictors()
## Interactions with carat:starts_with(&amp;quot;cut_&amp;quot;)
## Interactions with carat:starts_with(&amp;quot;color_&amp;quot;)
## Variable mutation for c(x, y, z)
## Variable mutation for log10(x * y * z)
## Natural splines on starts_with(&amp;quot;carat_x_cut&amp;quot;)
## Natural splines on starts_with(&amp;quot;carat_x_color&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wf07 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod07) %&amp;gt;%
  add_recipe(rec07)

doParallel::registerDoParallel()
set.seed(9876)
rs07 &amp;lt;-
  fit_resamples(
    wf07,
    diamonds_folds,
    control = ctrl_preds
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is definitely going to &lt;em&gt;way&lt;/em&gt; overfit our data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rs07 %&amp;gt;%
  collect_metrics()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator   mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.0750    10 0.00377 Preprocessor1_Model1
## 2 rsq     standard   0.970     10 0.00342 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well we (finally) made a modes improvement! Let’s see how the predictions/residuals plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rs07 %&amp;gt;%
  augment() %&amp;gt;%
  ggplot(aes(x = price,
             y = .pred)) +
  geom_point(alpha = 0.05) +
  geom_abline(linetype = &amp;quot;dashed&amp;quot;,
              alpha = 0.5,
              size = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-38-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s pretty good! We do have one value that’s &lt;strong&gt;&lt;em&gt;way&lt;/em&gt;&lt;/strong&gt; off, so let’s see if regulization can help. This will require setting a new baseline model, and we’ll tune our way to the best regularizaion parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics &amp;lt;- 
  rs07 %&amp;gt;%
  collect_metrics() %&amp;gt;%
  mutate(model = &amp;quot;model07&amp;quot;) %&amp;gt;%
  bind_rows(metrics)

# add normalization step
rec08 &amp;lt;- 
  rec07 %&amp;gt;% 
  step_zv(all_numeric_predictors()) %&amp;gt;%
  step_normalize(all_numeric_predictors(),
                 -cut_Ideal, -cut_Premium, -cut_Very.Good, -cut_other,
                 -color_E, -color_F, -color_G, -color_H, -color_I, -color_J,
                 -clarity_SI2, -clarity_VS1, -clarity_VS2, -clarity_VVS1, -clarity_VVS2, -clarity_other)

rm(mod05, mod06, mod07, rec05, rec06, rec07, wf05, wf06, wf07, rs05, rs06, rs07)

mod08 &amp;lt;-
  linear_reg(penalty = tune(), mixture = tune()) %&amp;gt;%
  set_engine(&amp;quot;glmnet&amp;quot;) %&amp;gt;%
  set_mode(&amp;quot;regression&amp;quot;) 

wf08 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod08) %&amp;gt;%
  add_recipe(rec08)

diamonds_grid &amp;lt;- 
  grid_regular(penalty(), mixture(), levels = 20)

doParallel::registerDoParallel()
set.seed(5831)
rs08 &amp;lt;-
  tune_grid(
    wf08,
    resamples = diamonds_folds,
    control = ctrl_preds,
    grid = diamonds_grid
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some notes but let’s explore our results…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rs08 %&amp;gt;%
  collect_metrics() %&amp;gt;%
  ggplot(aes(x = penalty,
             y = mean,
             color = as.character(mixture))) +
  geom_point() +
  geom_line(alpha = 0.75) +
  facet_wrap(~.metric, scales = &amp;quot;free&amp;quot;) +
  scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-40-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like we were performing pretty well with the unregularized model, oddly enough! Let’s select the best and finalize our workflow.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_metrics &amp;lt;- 
  rs08 %&amp;gt;%
  select_best(&amp;quot;rmse&amp;quot;)

wf_final &amp;lt;- 
  finalize_workflow(wf08, best_metrics)

rm(mod08, rec07, rec08, rs08, wf08)

set.seed(333)
final_fit &amp;lt;- 
  wf_final %&amp;gt;%
  fit(diamonds_train)

final_fit %&amp;gt;%
  predict(diamonds_test) %&amp;gt;%
  bind_cols(diamonds_test) %&amp;gt;%
  select(price, .pred) %&amp;gt;%
  ggplot(aes(x = price, 
             y = .pred)) +
  geom_point(alpha = 0.05) + 
  geom_abline(alpha = 0.5,
              linetype = &amp;quot;dashed&amp;quot;,
              size = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-41-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What are the most important variables in this regularized model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_fit %&amp;gt;%
  pull_workflow_fit() %&amp;gt;%
  vi(lambda = best_metrics$penalty) %&amp;gt;%
  mutate(Variable = fct_reorder(Variable, Importance)) %&amp;gt;%
  ggplot(aes(x = Variable,
             y = Importance, 
             fill = Sign)) +
  geom_col() +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-42-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, most of our terms get regularized away, which is what we want! Our chart is a little unreadable; let’s plot just the most important variables in a few ways:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_fit %&amp;gt;%
  pull_workflow_fit() %&amp;gt;%
  vi(lambda = best_metrics$penalty) %&amp;gt;%
  arrange(desc(Importance)) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  mutate(Variable = fct_reorder(Variable, Importance)) %&amp;gt;%
  ggplot(aes(x = Variable,
             y = Importance,
             fill = Sign)) +
  geom_col() +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-43-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_fit %&amp;gt;%
  pull_workflow_fit() %&amp;gt;%
  vi(lambda = best_metrics$penalty) %&amp;gt;%
  arrange(desc(Importance)) %&amp;gt;% 
  slice_head(n = 10) %&amp;gt;%
  mutate(Importance = if_else(Sign == &amp;quot;NEG&amp;quot;, -Importance, Importance),
         Variable = fct_reorder(Variable, Importance)) %&amp;gt;%
  ggplot(aes(x = Variable,
             y = Importance,
             fill = Sign)) + 
  geom_col() +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-44-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And look at that! Our most important variable was one that came from feature engineering! The size of the rock had the biggest impact on price.&lt;/p&gt;
&lt;p&gt;We’ve gone through a lot of steps, so it may be good to look back on what was done:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explored our dataset via some simple exploratory data analysis;&lt;/li&gt;
&lt;li&gt;Fit a simple linear model to predict the log-transform of price;&lt;/li&gt;
&lt;li&gt;Attempted (and failed) to improve upon the simple model with fancier normalization and resampling techniques;&lt;/li&gt;
&lt;li&gt;Explored the dataset further to find meaningful interactions and potential new features;&lt;/li&gt;
&lt;li&gt;Fit a new model with feature engineering;&lt;/li&gt;
&lt;li&gt;Tuned regularization parameters on our model with feature engineering to arrive at the final model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our models’ performances, ranked from best to worst, show that the final tuned model did indeed perform the best on the test dataset!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_preds &amp;lt;-
  final_fit %&amp;gt;%
  predict(diamonds_train) %&amp;gt;%
  bind_cols(diamonds_train) %&amp;gt;%
  select(price, .pred)

bind_rows(final_preds %&amp;gt;% rmse(price, .pred),
          final_preds %&amp;gt;% rsq(price, .pred)) %&amp;gt;%
  rename(mean = .estimate) %&amp;gt;%
  select(-.estimator) %&amp;gt;%
  mutate(model = &amp;quot;model_final&amp;quot;) %&amp;gt;%
  bind_rows(metrics %&amp;gt;% select(.metric, mean, model)) %&amp;gt;%
  pivot_wider(names_from = .metric,
              values_from = mean) %&amp;gt;%
  mutate(model = fct_reorder(model, desc(rmse))) %&amp;gt;%
  pivot_longer(rmse:rsq,
               names_to = &amp;quot;metric&amp;quot;,
               values_to = &amp;quot;value&amp;quot;) %&amp;gt;%
  ggplot(aes(x = model,
             y = value)) +
  geom_point() +
  facet_wrap(~metric, scales = &amp;quot;free&amp;quot;) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-45-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Election Night: Some Closing Thoughts on the VA Governor Race</title>
      <link>https://www.thedatadiary.net/blog/2021-11-02-election-night-some-closing-thoughts-on-the-va-governor-race/</link>
      <pubDate>Tue, 02 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-11-02-election-night-some-closing-thoughts-on-the-va-governor-race/</guid>
      <description>


&lt;p&gt;A few weeks ago I had &lt;a href=&#34;https://www.thedatadiary.net/blog/2021-10-19-polling-average-of-the-va-governor-s-race-using-purrr-map-functions/&#34;&gt;written about the VA Governor’s race&lt;/a&gt; before going on vacation - in that time it seems as though Terry McAuliffe’s campaign had lost a lot of steam and Youngkin made up a lot of ground in the final weeks of the campaign. At the time of this writing, it seems overwhelmingly likely that Glenn Youngkin will become the next governor of Virginia. To avoid some of the &lt;a href=&#34;https://twitter.com/rp_griffin/status/1455696452915802122&#34;&gt;galaxy-brain takes that will inevitabely wind up twitter&lt;/a&gt;, I thought I’d distract myself by following up on my previous post.&lt;/p&gt;
&lt;p&gt;Firstly, I should share the updated polling average. A few weeks ago, McAullife appeared to have a sizeable lead in the polls:&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;FiveThirtyEight&amp;#39;s poll tracker for the VA governor race (&lt;a href=&#34;https://t.co/HKVw7RcsJN&#34;&gt;https://t.co/HKVw7RcsJN&lt;/a&gt;) shows the moving averages - as a weekend coding project, I revamped with a weighted avg., 95% CI, and win probability based on an election day distribution📈 &lt;a href=&#34;https://t.co/hM6vzLUZnM&#34;&gt;pic.twitter.com/hM6vzLUZnM&lt;/a&gt;&lt;/p&gt;&amp;mdash; Mark Rieke (@markjrieke) &lt;a href=&#34;https://twitter.com/markjrieke/status/1444454926399254535?ref_src=twsrc%5Etfw&#34;&gt;October 3, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;However, as of election day, the race had significantly tightened to effectively a coin-toss:&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Final update - p_win ~ 55% McAuliffe; again, the win probability is just the portion of the polling avg. distribution above 50% for each candidate. For *actual* models, go check out the work by &lt;a href=&#34;https://twitter.com/lxeagle17?ref_src=twsrc%5Etfw&#34;&gt;@lxeagle17&lt;/a&gt;/&lt;a href=&#34;https://twitter.com/Thorongil16?ref_src=twsrc%5Etfw&#34;&gt;@Thorongil16&lt;/a&gt; or &lt;a href=&#34;https://twitter.com/jhkersting?ref_src=twsrc%5Etfw&#34;&gt;@jhkersting&lt;/a&gt;, among others &lt;a href=&#34;https://t.co/W9bDf1OumZ&#34;&gt;pic.twitter.com/W9bDf1OumZ&lt;/a&gt;&lt;/p&gt;&amp;mdash; Mark Rieke (@markjrieke) &lt;a href=&#34;https://twitter.com/markjrieke/status/1455164731749175304?ref_src=twsrc%5Etfw&#34;&gt;November 1, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;As I mention in the above tweet, the win probability isn’t a true forecast, just the portion of each candidate’s election day distribution &lt;em&gt;above 50%&lt;/em&gt;. That being said, actual forecasts similarly had the race down to a near 50-50 split as of this morning:&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Special for Twitter: Updated VA polling trends with the new Fox and Post/GMU polls &lt;a href=&#34;https://t.co/hLNa3pFCpR&#34;&gt;https://t.co/hLNa3pFCpR&lt;/a&gt; &lt;a href=&#34;https://t.co/6cz0JXa0xm&#34;&gt;pic.twitter.com/6cz0JXa0xm&lt;/a&gt;&lt;/p&gt;&amp;mdash; G. Elliott Morris (@gelliottmorris) &lt;a href=&#34;https://twitter.com/gelliottmorris/status/1454130028149620737?ref_src=twsrc%5Etfw&#34;&gt;October 29, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/FINAL?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#FINAL&lt;/a&gt; Virginia Gov. Projection.&lt;a href=&#34;https://twitter.com/GlennYoungkin?ref_src=twsrc%5Etfw&#34;&gt;@GlennYoungkin&lt;/a&gt; is ever so slightly favored in this tossup for the governorship.&lt;br&gt;&lt;br&gt;Full Projection&amp;gt;&amp;gt;&lt;a href=&#34;https://t.co/PcM2qSjUYK&#34;&gt;https://t.co/PcM2qSjUYK&lt;/a&gt; &lt;a href=&#34;https://t.co/NGCirjkkG4&#34;&gt;pic.twitter.com/NGCirjkkG4&lt;/a&gt;&lt;/p&gt;&amp;mdash; Jack Kersting (@jhkersting) &lt;a href=&#34;https://twitter.com/jhkersting/status/1455545666999042056?ref_src=twsrc%5Etfw&#34;&gt;November 2, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;Even the model most confident in McAuliffe built by &lt;a href=&#34;https://twitter.com/lxeagle17&#34;&gt;Lakshya Jain&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/Thorongil16&#34;&gt;Thorongil&lt;/a&gt; had dropped McAuliffe’s win probability from ~85% to 67% over the course of a few weeks:&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;🚨FINAL &lt;a href=&#34;https://twitter.com/hashtag/VAGov?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#VAGov&lt;/a&gt; FORECAST🚨&lt;br&gt;&lt;br&gt;At the end of a long campaign, here&amp;#39;s where the model made by me and &lt;a href=&#34;https://twitter.com/Thorongil16?ref_src=twsrc%5Etfw&#34;&gt;@Thorongil16&lt;/a&gt; stands. The race is rated as Lean Democratic, with a forecasted margin of D+3.6 and a win probability of 67%. An interactive map is over at &lt;a href=&#34;https://t.co/Kmvj6sRElC&#34;&gt;https://t.co/Kmvj6sRElC&lt;/a&gt;. &lt;a href=&#34;https://t.co/kuaJc82HML&#34;&gt;pic.twitter.com/kuaJc82HML&lt;/a&gt;&lt;/p&gt;&amp;mdash; Lakshya Jain (@lxeagle17) &lt;a href=&#34;https://twitter.com/lxeagle17/status/1455544487149592576?ref_src=twsrc%5Etfw&#34;&gt;November 2, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;While I definitely plan on utilizing a more scientific poll-weighting methodology in the future, I do find it interesting that even a simple averaging method can produce relatively accurate results in line with the majority of other forecasters.&lt;/p&gt;
&lt;p&gt;Regarding post-hoc analysis of &lt;em&gt;why&lt;/em&gt; McAuliffe lost, I won’t dredge up any of my own (partially because it’d be irresponsible &amp;amp; pundit-y to do so without referencing any data and partially because it’s getting late &amp;amp; I’m a bit tired), but I’ll point out a few tweets from &lt;a href=&#34;https://twitter.com/Nate_Cohn&#34;&gt;Nate Cohn&lt;/a&gt; that show that the results appear to show a near uniform shift across precincts and different voting groups. This would suggest that McAuliffe’s loss is tied more closely to the national environment, rather than shifts amongst specific groups/counties.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Update at nearly 1500 precincts: Youngkin is still on track for victory, running a ahead of what he needs across basically every dimension worth looking at.&lt;br&gt;But it&amp;#39;s worth noting just how close things are to &amp;#39;expectations,&amp;#39; which is simply shifting the 2020 result to the right &lt;a href=&#34;https://t.co/Mbe3pg7NFB&#34;&gt;pic.twitter.com/Mbe3pg7NFB&lt;/a&gt;&lt;/p&gt;&amp;mdash; Nate Cohn (@Nate_Cohn) &lt;a href=&#34;https://twitter.com/Nate_Cohn/status/1455693099062153217?ref_src=twsrc%5Etfw&#34;&gt;November 3, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;And btw, if you&amp;#39;re doing pundit things I&amp;#39;d look at that red column. McAuliffe fell short of expectations--a more-or-less uniform shift--by basically the same amount, just about everywhere&lt;/p&gt;&amp;mdash; Nate Cohn (@Nate_Cohn) &lt;a href=&#34;https://twitter.com/Nate_Cohn/status/1455717564772986884?ref_src=twsrc%5Etfw&#34;&gt;November 3, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;This won’t stop the networks from ascribing the win/loss to very specific campaign issues (I’ve already seen quite a few folks ascribe Youngkin’s win to education, race, suburban-reversion, etc., without any evidence to back up such claims). Until there are deep dives into data regarding the election, I’d treat any comments rom pundits with a hefty grain of salt.&lt;/p&gt;
&lt;div id=&#34;some-closing-thoughts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some closing thoughts&lt;/h3&gt;
&lt;p&gt;That’s all for me today! I’ll be back in a few weeks with some non-political content, looking at a machine learning model predicting the price of a diamond in the &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/diamonds.html&#34;&gt;diamonds dataset&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Polling Average of the VA Governor&#39;s Race using purrr::map functions</title>
      <link>https://www.thedatadiary.net/blog/2021-10-19-polling-average-of-the-va-governor-s-race-using-purrr-map-functions/</link>
      <pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-10-19-polling-average-of-the-va-governor-s-race-using-purrr-map-functions/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-10-19-polling-average-of-the-va-governor-s-race-using-purrr-map-functions/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Rolling poll averages can be misleading in the absence of errorbars or an expected distribution of outcomes. FiveThirtyEight is currently &lt;a href=&#34;https://projects.fivethirtyeight.com/polls/governor/virginia/&#34;&gt;tracking polls of Virginia’s Governor race&lt;/a&gt; slated for early November, and has kindly made their polls &lt;a href=&#34;https://projects.fivethirtyeight.com/polls-page/data/governor_polls.csv&#34;&gt;available to the public&lt;/a&gt;. Their current polling average, however, looks to be a simple rolling average and doesn’t include a confidence interval. I’ve attempted to improve upon their tracker here by providing a weighted polling average and a 95% confidence interval.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/race_results_2021-10-19.png&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;how-this-works&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How this works&lt;/h3&gt;
&lt;p&gt;Since we’re only considering the top candidates from each party, we’ll look at each candidate’s two-party voteshare of each poll. To get the two-party voteshare, third party or other minor candidates are removed from each poll and each candidate’s percentage is recalculated as if they were the only two options on the ballot (in practice, this only removes a tiny amount of undecideds and third party voters). Then, the daily polling average is calculated by weighting each poll by sample size and recency. Using &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes%27_theorem&#34;&gt;Bayes’ theorem&lt;/a&gt; and a weak &lt;a href=&#34;https://en.wikipedia.org/wiki/Continuous_uniform_distribution&#34;&gt;uniform prior&lt;/a&gt;, we can use the same method recalculate the polling average and confidence interval for each day between today and the election. Because polls are weighted by recency, as we look further and further into the future, our confidence in the polls decreases and the confidence interval around the polling average fans out. Each candidate’s probability of winning is the portion of the portion of the projected election-day polling distribution in their favor, based on that day’s polling average.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-caveats-worth-noting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some caveats worth noting&lt;/h3&gt;
&lt;p&gt;This is an inherently flawed method, and it’s worth pointing out a few of the flaws and shortcuts I used:&lt;/p&gt;
&lt;div id=&#34;the-functions-used-to-weight-polls-are-nowhere-near-perfect.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The functions used to weight polls are nowhere near perfect.&lt;/h4&gt;
&lt;p&gt;The original weighting functions (which I haven’t changed) were chosen somewhat arbitrarily. In hindsight, they’re probably placing too much emphasis on recency and the error bars ought to be larger. While I have received some &lt;a href=&#34;https://community.rstudio.com/t/is-it-possible-to-tune-arbitrary-parameters-w-tidymodels/117998/2&#34;&gt;advice on tuning arbitrary functions&lt;/a&gt; as a part of a larger model, I haven’t implemented here. It’s more prudent to think of this as an over-confident polling aggregate, rather than any sort of model. For a true projection model, I’d recommend looking at &lt;a href=&#34;https://projects.jhkforecasts.com/2022-midterms/virginia-governor&#34;&gt;Jack Kersting’s website&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-weighting-method-ignores-important-weighting-factors&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The weighting method ignores important weighting factors&lt;/h4&gt;
&lt;p&gt;This weighting method is super simple and ignores common weighting factors, like pollster and survey methodology. Other less-common poll weighting methods, like accounting for partisan non-response bias and and how the pollster weights their results (notably, whether or not the pollster weights by education) were similarly ignored. There is definitely a &lt;a href=&#34;https://gelliottmorris.substack.com/p/what-people-are-missing-about-the?justPublished=true&#34;&gt;strong argument&lt;/a&gt; for including these weighting factors, but for me, this exercise was more about learning to use &lt;code&gt;purrr::map()&lt;/code&gt; and other related functions when &lt;a href=&#34;https://github.com/markjrieke/thedatadiary/blob/main/2021.10.01-virginia_governors_race/scripts/va_poll_wrangle.R&#34;&gt;writing the script for this plot&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-pollsters-are-filtered-out-by-design&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Some pollsters are filtered out by design&lt;/h4&gt;
&lt;p&gt;I debated this for quite some time, but decided to add a filter to remove polls conducted by &lt;a href=&#34;https://www.rasmussenreports.com/&#34;&gt;Rasmussen&lt;/a&gt; and &lt;a href=&#34;https://www.thetrafalgargroup.org/&#34;&gt;Trafalgar&lt;/a&gt;. Trafalgar is excluded from any of the &lt;a href=&#34;https://docs.google.com/spreadsheets/d/e/2PACX-1vQ56fySJKLL18Lipu1_i3ID9JE06voJEz2EXm6JW4Vh11zmndyTwejMavuNntzIWLY0RyhA1UsVEen0/pub?gid=0&amp;amp;single=true&amp;amp;output=csv&#34;&gt;Economist’s polling databases&lt;/a&gt; for opaque yet clearly shoddy methodology and Rasmussen is &lt;a href=&#34;https://twitter.com/Rasmussen_Poll/status/1448819612242616324?ref_src=twsrc%5Etfw%7Ctwcamp%5Eembeddedtimeline%7Ctwterm%5Eprofile%3ARasmussen_Poll%7Ctwgr%5EeyJ0ZndfZXhwZXJpbWVudHNfY29va2llX2V4cGlyYXRpb24iOnsiYnVja2V0IjoxMjA5NjAwLCJ2ZXJzaW9uIjpudWxsfSwidGZ3X2hvcml6b25fdHdlZXRfZW1iZWRfOTU1NSI6eyJidWNrZXQiOiJodGUiLCJ2ZXJzaW9uIjpudWxsfSwidGZ3X3NwYWNlX2NhcmQiOnsiYnVja2V0Ijoib2ZmIiwidmVyc2lvbiI6bnVsbH19%7Ctwcon%5Etimelinechrome&amp;amp;ref_url=https%3A%2F%2Fwww.rasmussenreports.com%2F&#34;&gt;clearly partisan&lt;/a&gt;. Removing these from the average follows the general consensus on ET (though, to be transparent, ET does tend to slant far to the left). In future polling projects, I’d hope to develop some more robust methodology to programatically downweight problematic pollsters (how’s &lt;em&gt;that&lt;/em&gt; for a tongue twister?), but for now I’m just going to exclude.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;final-thoughts-on-polling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Final thoughts on polling&lt;/h3&gt;
&lt;p&gt;This methodology certainly has its flaws, but it &lt;em&gt;is&lt;/em&gt; &lt;a href=&#34;https://github.com/markjrieke/thedatadiary/blob/main/2021.10.01-virginia_governors_race/scripts/va_poll_wrangle.R&#34;&gt;transparent&lt;/a&gt;. I’ll continuously update this plot up until election day on my &lt;a href=&#34;https://github.com/markjrieke/thedatadiary/tree/main/2021.10.01-virginia_governors_race&#34;&gt;github&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-programming-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some programming notes&lt;/h3&gt;
&lt;p&gt;I’ve finished migrating my site to Netlify! I had originally planned to make the switch from Squarespace sometime early in 2022, but motivation struck me during a relatively light work week and I was able to rebuild the site using &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;blogdown&lt;/a&gt;. This allows for a lot more customization and control than was available with Squarespace, but the biggest upside is &lt;em&gt;definitely&lt;/em&gt; the ease of sharing in-line code, here’s a quick example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

# let&amp;#39;s put together a plot from the diamonds dataset
diamonds %&amp;gt;%
  ggplot(aes(x = carat,
             y = price,
             color = cut)) +
  geom_point(alpha = 0.1) +
  theme_minimal() +
  viridis::scale_color_viridis(discrete = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-10-19-polling-average-of-the-va-governor-s-race-using-purrr-map-functions/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This ease of use and visibility will make things more seamless for me and allow me to dig into more technical content in more detail in the future!&lt;/p&gt;
&lt;p&gt;I’ll be taking a (much needed) vacation next week, spending some time off the grid in the Grand Canyon and surrounding area with my family. I’ve got a short post lined up for early November when I return - see you then!&lt;/p&gt;
&lt;p&gt;As always, you can find the source files for &lt;a href=&#34;https://github.com/markjrieke/thedatadiary/tree/main/2021.10.01-virginia_governors_race&#34;&gt;the script to generate the polling average&lt;/a&gt; and for &lt;a href=&#34;https://github.com/markjrieke/thedatadiary.net&#34;&gt;this site&lt;/a&gt; on my github.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>RStudio&#39;s Call for Documentation</title>
      <link>https://www.thedatadiary.net/blog/2021-10-05-rstudio-s-call-for-documentation/</link>
      <pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-10-05-rstudio-s-call-for-documentation/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-10-05-rstudio-s-call-for-documentation/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In my job, I spend a good amount of time working in the platform setup by our survey vendor, &lt;a href=&#34;https://www.qualtrics.com/&#34;&gt;Qualtrics&lt;/a&gt;. There are some pre-formatted reports that we can send on a recurring basis through Qualtrics, but for one-off or custom reports, I can work with the raw data in R. A few weeks ago, however, I was asked to setup a recurring email to send a customized report to a group of hospital directors each week. While R makes generating the report simple, sending out each week was tedious, as each one needed to be sent separately. Since the reports contain patient information, I couldn’t automate via a third party server like &lt;a href=&#34;https://github.com/features/actions&#34;&gt;GitHub Actions&lt;/a&gt;. I needed a way to localize the automation to my computer.&lt;/p&gt;
&lt;p&gt;Luckily enough, I was able to work out a solution! I wrote about it in a submission to &lt;a href=&#34;https://community.rstudio.com/t/r-views-call-for-documentation-announcement/110579&#34;&gt;RStudio’s 2021 Call for Documentation&lt;/a&gt;. You can read my article on &lt;a href=&#34;https://community.rstudio.com/t/automated-email-reports-with-r-vba-and-the-task-scheduler-r-views-submission/115807/5&#34;&gt;Automated Email Reports with R, VBA, and the Task Scheduler&lt;/a&gt; for more detail.&lt;/p&gt;
&lt;div id=&#34;some-programming-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some programming notes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;I’ve been named one of RStudio Community’s New Users of the Month!&lt;/li&gt;
&lt;li&gt;I’ve started working in Rmarkdown for these posts (you can actually view the native file for this post here). This allows me to mix in code and prose in one document &amp;amp; should make the process a bit easier on my end.&lt;/li&gt;
&lt;li&gt;I’ve scheduled out posts every two weeks for the rest of the year. I wanted to give myself enough time to work on some longer term projects, so be on the lookout for more in store!&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Catching Up (again)</title>
      <link>https://www.thedatadiary.net/blog/2021-09-23-catching-up-again/</link>
      <pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-09-23-catching-up-again/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-09-23-catching-up-again/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Wow, it’s been quite a while! Once again, I’m catching up from a long hiatus (this time, there was about a month’s gap between posts). Although I haven’t been writing here, I have been keeping myself incredibly busy - for that reason, I think I’m going to switch up the format of this site. Here’s a few changes I’m planning on making, &amp;amp; also why I plan on making them.&lt;/p&gt;
&lt;div id=&#34;opening-up-topic-options&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Opening up topic options&lt;/h3&gt;
&lt;p&gt;Up until now, I’ve pretty much exclusively been looking at politically-related things from a data-centric point of view, partially because most news outlets are sensationalist and it annoys me, and partially because the 2020 election &amp;amp; aftermath was on my mind for most of early 2021 (also, wanting to understand the &lt;a href=&#34;https://projects.economist.com/us-2020-forecast/president/how-this-works&#34;&gt;Economist’s Election Forecast methodology&lt;/a&gt; was the spark that led to taking stats classes, learning R, and [eventually] landing a new job). While I still want to/expect that I will touch on political-esque topics, I want to expand into things I’m more generally interested in. I’ve spent a lot of time this year working on projects that don’t touch politics and it was difficult to spur the enthusiasm to work on a political project just to fill the site after having spent hours coding something unrelated. I’ve imposed this political filter on myself, &amp;amp; expanding the horizon to include any/all topics that interest me will allow me to more consistently write without overextending myself.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;moving-away-from-squarespace&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Moving away from Squarespace&lt;/h3&gt;
&lt;p&gt;Currently, this site is hosted by Squarespace. This was great for getting setup &amp;amp; used to working online, but after a year of working with it, I’ve realized that it isn’t the ideal platform for my use case. Early next year, hopefully, I’ll switch over to using rmarkdown, Hugo, and Netifly to build &amp;amp; deploy a site directly from R. This will make my life a lot easier, for a couple of reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I will have a lot more control over layouts and themes&lt;/li&gt;
&lt;li&gt;I’ll be able to more easily share code, images, and interactives(!)&lt;/li&gt;
&lt;li&gt;The code &amp;amp; the website will be housed under one directory (rather than existing as two separate entities)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, the platform I’m planning on moving to is more blog/personal-professional focused, rather than retail focused, like Squarespace. I may also be able to move towards having posts appear in emails, rather than having to link to the post itself.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;timeline&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Timeline&lt;/h3&gt;
&lt;p&gt;I haven’t yet settled on this yet, but I may move from a weekly schedule to a bi-weekly schedule (not that I ever really stuck to the weekly schedule). The projects I’ve been working on typically take quite a bit of time, so trying to churn one out each week means that I may have to suffer quality for a schedule, which I don’t want to do. I’d rather have enough time to fully devote to completing something.&lt;/p&gt;
&lt;p&gt;Anyways, that’s a lot of updates - I’ll hopefully check in within a few weeks time!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tidymodels and the Titanic</title>
      <link>https://www.thedatadiary.net/blog/2021-08-08-tidymodels-and-the-titanic/</link>
      <pubDate>Sun, 08 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-08-08-tidymodels-and-the-titanic/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-08-08-tidymodels-and-the-titanic/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This week, I thought I’d do something a bit different. I’ve been working with &amp;amp; getting used to &lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;tidymodels&lt;/a&gt;, a suite of R packages for building machine learning models with tidyverse principles (you can thank &lt;a href=&#34;https://juliasilge.com/blog/&#34;&gt;Julia Silge’s blog&lt;/a&gt; for providing a whole host of examples and walkthroughs). Using tidymodels and &lt;a href=&#34;https://www.kaggle.com/c/titanic&#34;&gt;Kaggle’s Titanic dataset&lt;/a&gt;, I created a few simple models to predict whether or not each passenger survived.&lt;/p&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploratory Data Analysis&lt;/h3&gt;
&lt;p&gt;First off, it’s important to get to know the dataset in a bit of detail before diving into model building. The Titanic dataset is relatively small, containing 12 columns of data on 891 passengers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PassengerId&lt;/strong&gt; : ordered number assigned to each passenger (1, 2, 3… etc.).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Survived&lt;/strong&gt; : indicates whether the passenger survived or not (imported as numeric, where a 1 means that the passenger survived and a 0 means that the passenger died.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PClass&lt;/strong&gt; : passenger class (1st, 2nd, or 3rd).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt; : passenger’s name.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sex&lt;/strong&gt; : passenger’s sex.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Age&lt;/strong&gt; : passenger’s age.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SibSp&lt;/strong&gt; : number of the passenger’s siblings and/or spouses aboard the ship.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parch&lt;/strong&gt; : number of the passenger’s parents and/or children aboard the ship.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ticket&lt;/strong&gt; : passenger’s ticket number.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fare&lt;/strong&gt; : price of the passenger’s ticket.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cabin&lt;/strong&gt; : passenger’s cabin number.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embarked&lt;/strong&gt; : port from which the passenger embarked.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The passenger’s name and ticket number are unique, and don’t offer any much predictive power (at least, not for the type we’ll be deploying), so I’ve removed those columns from the selection. The majority of passengers are missing Cabin information, so I’ve similarly removed that column. Let’s take a look at the age distribution by gender. Density plots are good for this sort of objective, but a quick note - density plots don’t show the counts (like a histogram), just the relative distribution of each group.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_01.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both men and women have similar age distributions, with the majority being adults roughly in their twenties. It’d be interesting to look at which age groups survived.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_02.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There’s a spike survivorship at a young age (as expected, due to the prioritization of children). Young adults make up the majority of both survivors and those who died, simply because young adults made up the majority of passengers. It may be more interesting to see which male/female age groups survived.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_03.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Interestingly, there’s a large portion of the women who died were children. To gain some more insight, however, it may be beneficial to bin the results. As mentioned above, density plots don’t show a count of each age, but the relative amounts instead. We can use a histogram to get a better idea of the actual amount in each group.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_04.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the histogram, we can see that there were far more male passengers than female passengers. Let’s setup age brackets and see how many of each age group survived.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_05.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Despite there being more male passengers, there were more female survivors in just about every age bracket. I had expected that women would have a greater percentage of survivorship, but was mildly surprised that the absolute number of female survivors was greater than the number of male survivors.&lt;/p&gt;
&lt;p&gt;Lets look at how survivorship is related to the number of family members each passenger was traveling with.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_06.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While there’s not huge variation between survivorship percentages across each class, we can see that the majority of passengers were traveling alone.&lt;/p&gt;
&lt;p&gt;Finally, I looked at the distribution of the fare price by survivorship. As expected, passengers who didn’t survive skewed towards low cost fares while survivors were more evenly distributed across fare prices.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_07.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting-and-evaluation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model Fitting and Evaluation&lt;/h3&gt;
&lt;p&gt;Now that we’ve explored the data a bit, we can get started on some actual model fitting and evaluation. I split the dataset into a training set and test set, and built two simple models - a &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34;&gt;logistic regression&lt;/a&gt; and a &lt;a href=&#34;https://en.wikipedia.org/wiki/Random_forest&#34;&gt;random forest model&lt;/a&gt;. In very light detail, a logistic regression is the most basic classification model based on the generalized linear model and a random forest model is a decision tree-based algorithm. With no &lt;a href=&#34;https://www.datarobot.com/wiki/tuning/&#34;&gt;tuning&lt;/a&gt; applied, neither model performed particularly well - the logistic regression and random forest models had accuracies of 85.5% and 83.8% on the test set, respectively. While the logistic regression had a slightly higher accuracy on the test set, the random forest performed better on the metric of &lt;a href=&#34;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&#34;&gt;area under the ROC (receiver-operator characteristic)&lt;/a&gt; curve - 0.899 compared to 0.888 (generally, binary classification systems with ROC AUC values close to 1 perform better). A comparison between the two ROC curves is shown below.&lt;/p&gt;
&lt;p&gt;It’s not perfect, but the purpose was to explore the tidymodels framework, and I’m pretty happy with the outcome/practice along the way.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/roc_auc.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-things-to-readwatchlisten-to&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some Things to Read/Watch/Listen to&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;There’s been a concerted effort by prominent conservatives to &lt;a href=&#34;https://twitter.com/gelliottmorris/status/1424524749875601408?s=21&#34;&gt;promote vaccination&lt;/a&gt; and &lt;a href=&#34;https://www.reviewjournal.com/opinion/letters/letter-dont-blame-the-gop-for-vaccine-hesitancy-2413731/amp/&#34;&gt;shift the blame for vaccine hesitancy to the Biden administration&lt;/a&gt;, pointing in particular to one of Kamala Harris’ comments during the Vice Presidential debate. This criticism falls fairly short, given that commentators like &lt;a href=&#34;https://www.youtube.com/watch?v=ocEFTIwnO6I&#34;&gt;Tucker Carlson&lt;/a&gt; and legislators like &lt;a href=&#34;https://www.cnn.com/2021/08/10/tech/twitter-marjorie-taylor-greene/index.html&#34;&gt;Marjorie Taylor Green&lt;/a&gt; continue to spread vaccine conspiracy theories. Further, a &lt;a href=&#34;https://www.economist.com/graphic-detail/2021/07/30/which-americans-are-against-the-jab&#34;&gt;statistical model by the Economist&lt;/a&gt; found that the strongest predictor of whether or not someone has been vaccinated is who they voted for in the 2020 presidential election.&lt;/li&gt;
&lt;li&gt;Julia Silge was recently featured as a guest on an excellent episode of the &lt;a href=&#34;https://www.youtube.com/watch?v=p7z8yZucwlM&#34;&gt;Not So Standard Podcast&lt;/a&gt;. Well worth a listen for R dorks like myself.&lt;/li&gt;
&lt;li&gt;Perry Bacon Jr wrote an &lt;a href=&#34;https://www.washingtonpost.com/opinions/2021/08/09/why-moderation-doesnt-guarantee-electoral-success-biden-democrats/&#34;&gt;informative piece&lt;/a&gt; on the misplaced importance on swing voters in election cycles.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Gas Price Fallacy</title>
      <link>https://www.thedatadiary.net/blog/2021-07-18-the-gas-price-fallacy/</link>
      <pubDate>Sun, 18 Jul 2021 22:16:52 -0500</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-07-18-the-gas-price-fallacy/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-07-18-the-gas-price-fallacy/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;As America has vaccinated its population, life has seemingly begun to return to the pre-pandemic normal. Businesses have been opening to higher levels of capacity, schools are planning for higher levels of in-person learning in the fall, and, notably, Americans have returned to &lt;a href=&#34;https://covid19.apple.com/mobility&#34;&gt;pre-pandemic levels of transit&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/existentialfish/status/1377779116414533638&#34;&gt;Conservative commentators&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/Jim_Jordan/status/1406966549714440196&#34;&gt;representatives&lt;/a&gt; have pointed out that prices of commodity goods - in particular, gasoline - have skyrocketed over the past year and attribute this increase to Biden’s presidency. Looking at retail gasoline prices, we can see a steady increase since Biden took office.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;plot_01.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What they fail to point out, however, is that the price of gasoline (and other commodities) significantly dropped during the pandemic, and that the increases are largely a return to pre-pandemic prices.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;plot_02.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While there are certainly opportunities to critique the current administration’s energy policy (&lt;a href=&#34;https://apnews.com/article/joe-biden-billings-a3a37acf2fce55449b704b01badc1f67&#34;&gt;banning new drilling leases on federal land/water&lt;/a&gt; and &lt;a href=&#34;https://apnews.com/article/donald-trump-joe-biden-keystone-pipeline-canada-environment-and-nature-141eabd7cca6449dfbd2dab8165812f2&#34;&gt;canceling the Keystone XL pipeline’s border-crossing permit&lt;/a&gt;, for example), attributing the recent return to pre-pandemic gasoline prices is a particularly weak and intentionally misleading avenue of attack.&lt;/p&gt;
&lt;p&gt;As always, you can find my work on &lt;a href=&#34;https://github.com/markjrieke/thedatadiary&#34;&gt;github&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;some-readingviewing-material&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some Reading/Viewing Material&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Economist wrote an &lt;a href=&#34;https://www.economist.com/graphic-detail/2021/07/10/in-person-voting-really-did-accelerate-covid-19s-spread-in-america&#34;&gt;interesting article&lt;/a&gt; finding a significant link between a county’s in-person voting rate and new COVID cases in November.&lt;/li&gt;
&lt;li&gt;Elliot Morris summarized the work of several political scientists in a recent article describing &lt;a href=&#34;https://gelliottmorris.substack.com/p/why-did-the-gop-slide-so-far-towards&#34;&gt;why the GOP slid so far towards authoritarianism&lt;/a&gt; in the past decade. The article is behind a subscriber paywall, but is well worth the read. In short, however, “ethnically antagonistic” voters are much more likely to agree with statements traditionally viewed as authoritarian. This group hadn’t coalesced under a single party until Trump brought these anti-racial voters into the party (recall his role in the &lt;a href=&#34;https://www.theatlantic.com/ideas/archive/2020/05/birtherism-and-trump/610978/&#34;&gt;birther conspiracy&lt;/a&gt; and the &lt;a href=&#34;https://www.cnn.com/2016/08/31/politics/2016-election-donald-trump-hillary-clinton-race/index.html&#34;&gt;racial antagonism surrounding his 2016 campaign&lt;/a&gt;), at which point democratic (note, small “d”) norms became a partisan issue.&lt;/li&gt;
&lt;li&gt;In my machine learning class, I’ve started learning about the implementation of neural networks. The course’s instruction style is highly technical, but Grant Sanderson’s &lt;a href=&#34;https://www.youtube.com/watch?t=1015s&amp;amp;v=aircAruvnKk&#34;&gt;series on neural networks&lt;/a&gt; has helped me align a technical and intuitive understanding of the topic.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Blexas?</title>
      <link>https://www.thedatadiary.net/blog/2021-07-11-blexas/</link>
      <pubDate>Sun, 11 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-07-11-blexas/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-07-11-blexas/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Last year, Donald Trump won Texas’ 38 electoral votes handily, &lt;a href=&#34;https://en.wikipedia.org/wiki/2020_United_States_presidential_election_in_Texas&#34;&gt;earning 52% of the vote compared to Biden’s 46% (R+6)&lt;/a&gt;. Taken on its own, this is hardly surprising - &lt;a href=&#34;https://www.270towin.com/states/Texas&#34;&gt;Texas has gone to the Republican presidential candidate in every election since 1980&lt;/a&gt;. Looking at the relative gains made in Texas over recent election cycles, however, paints a much more hopeful picture for Democrats hoping to flip the state &amp;amp; garner the nickname, “blexas.”&lt;/p&gt;
&lt;p&gt;On election day, FiveThirtyEight and the Economist gave Biden a &lt;a href=&#34;https://projects.fivethirtyeight.com/2020-election-forecast/texas/&#34;&gt;38%&lt;/a&gt; and &lt;a href=&#34;https://projects.economist.com/us-2020-forecast/president/texas&#34;&gt;30%&lt;/a&gt; chance of winning in Texas, respectively. While still bearish on Biden, this is far more bullish than 2016, when FiveThirtyEight’s model gave Clinton a mere &lt;a href=&#34;https://projects.fivethirtyeight.com/2016-election-forecast/texas/#plus&#34;&gt;5%&lt;/a&gt; chance of winning in Texas (in fact, both FiveThirtyEight and the Economist were more confident in Biden’s chance of winning Texas in 2020 than FiveThirtyEight was that Trump would win the presidency in 2016 (&lt;a href=&#34;https://projects.fivethirtyeight.com/2016-election-forecast/#plus&#34;&gt;28%&lt;/a&gt;)). Part of this shift can be attributed to Democratic gains nationally, but Texas Democrats have been steadily gaining more ground than can be explained by national swing.&lt;/p&gt;
&lt;p&gt;The Cook Political Report publishes its &lt;a href=&#34;https://cookpolitical.com/analysis/national/pvi/introducing-2021-cook-political-report-partisan-voter-index&#34;&gt;Partisan Voter Index&lt;/a&gt; (PVI) following each presidential election. PVI is a measure of how a state, district, or county votes relative to the national environment. For example, say a Democratic candidate wins 53% of the two-party voteshare nationally, but 51% in a given state. Despite the state going to the Democratic candidate, the state PVI would be R+2%, since the candidate performed 2% under the national vote (the actual PVI calculated in the Cook report is slightly more involved, but this basic understanding is sufficient for our purposes). Looking at Texas’ PVI over past elections, we see that Democrats have been making relative gains in every election since 2004, despite Texas still voting more Republican than the nation.&lt;/p&gt;
&lt;p&gt;Much of this can likely be attributed to demographic shifts drive by major Texas cities - &lt;a href=&#34;https://demographics.texas.gov/Resources/Presentations/OSD/2021/2021_01_15_LeadershipNorthTexas.pdf&#34;&gt;Dallas, Houston, Austin, and San Antonio have seen massive population increases&lt;/a&gt;, largely due to domestic and international migration (as opposed to natural changes - e.g., births). As Texas grows, it also continues to diversify. The non-hispanic white population in Texas dropped from 45% in 2010 to 41% in 2019, and non-white population groups have driven growth over the past 10 years.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/txdemographics.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;All that being said, while I expect that Texas will continue to experience demographic shifts that are favorable in the eyes of Democratic politicians, in the absence of any real modeling work, I’m hesitant to say that Texas will turn blue in the near term. Despite gains in the presidential results, Texas Democrats &lt;a href=&#34;https://en.wikipedia.org/wiki/2020_United_States_House_of_Representatives_elections_in_Texas#Results_summary&#34;&gt;didn’t outperform expectations in the house&lt;/a&gt;, nor did they even advance a candidate in the special election for TX-06 (two Republican candidates advanced to a runoff). Governor Abbott is also introducing a &lt;a href=&#34;https://www.nytimes.com/2021/07/07/us/politics/texas-abbott-voting-laws-transgender-rights.html&#34;&gt;special legislative session&lt;/a&gt; with one goal (of many) of making Texas a state with some of the most restrictive voting laws with targeted partisan effects. Looking to other states as a reference, North Carolina had also seen similar PVI shifts in the past without resulting in Democratic victories (though, to be fair, North Carolina hasn’t experienced the same level of demographic shift that Texas is undergoing).&lt;/p&gt;
&lt;p&gt;Perhaps there is some modeling work I can do to produce a more definitive stance, but until then, I’ll hold on making any bold predictions.&lt;/p&gt;
&lt;p&gt;As always, you can find my work on &lt;a href=&#34;https://github.com/markjrieke/thedatadiary&#34;&gt;github&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;general-updates&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;General Updates&lt;/h3&gt;
&lt;p&gt;It’s been quite a while, but it’s good to get back into writing again. I wrote the scripts/made the charts for this post about two weeks ago, but haven’t had the time (or rather, haven’t made the time) to write the post itself. While I feel like I’ve been saying this for three months now, I do hope to get back to a more regular schedule soon - perhaps biweekly, to avoid rushing projects. We’ll see - in any regard, here are some updates that I’m excited about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I started my new job a few weeks ago! It’s both very rewarding (I get to use R &amp;amp; work with large polling datasets daily!) and very demanding. I enjoy the challenge &amp;amp; am excited for upcoming projects that I’ve been tasked with.&lt;/li&gt;
&lt;li&gt;I’ve been steadily making headway against the Machine Learning course I’ve been taking - at this point, I’m about halfway finished.&lt;/li&gt;
&lt;li&gt;I’ve begun to poke around in the tidymodels framework. I’m interested in the standardization the packages supply for generating models in R.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;some-readingviewing-material&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Some reading/viewing material:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;I linked to it above, but it’s worth reading through this &lt;a href=&#34;https://www.nytimes.com/2021/07/07/us/politics/texas-abbott-voting-laws-transgender-rights.html&#34;&gt;NYT article on the special legislative session in Texas&lt;/a&gt;. I only touched on voting rights, but session is primed as another hot-button issue session following spring’s ultra-conservative agenda.&lt;/li&gt;
&lt;li&gt;Checks and Balance, the Economist’s weekly podcast, focused on Critical Race Theory &amp;amp; the troubled history of race in American education in their most recent podcast, “&lt;a href=&#34;https://open.spotify.com/show/4jjKHhNPHfkIZHssgrQavP&#34;&gt;History Test&lt;/a&gt;.” It’s well worth a listen, especially if you’re like me, and had never heard of CRT until recently.&lt;/li&gt;
&lt;li&gt;Just over 6 months ago on January 6th, violent Trump supporters stormed the U.S. capitol. With time and &lt;a href=&#34;https://www.msnbc.com/opinion/gop-lies-about-jan-6-are-getting-bolder-more-dangerous-n1267178&#34;&gt;overt lies&lt;/a&gt; being propagated by Fox News, OANN, and Newsmaxx, it’s easy to forget exactly what happened and how it felt on that day. This &lt;a href=&#34;https://www.youtube.com/watch?v=jWJVMoe7OY0&#34;&gt;NYT Visual Investigation&lt;/a&gt; does an excellent job of reconstructing the timeline of the events that day.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>COVID Cases Improve with Introduction of Vaccines</title>
      <link>https://www.thedatadiary.net/blog/2021-06-03-covid-cases-improve-with-introduction-of-vaccines/</link>
      <pubDate>Thu, 03 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-06-03-covid-cases-improve-with-introduction-of-vaccines/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-06-03-covid-cases-improve-with-introduction-of-vaccines/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;As vaccines have become widely available in the US, new COVID cases and deaths have dropped significantly from winter peak, with the 7-day average for both cases and deaths nearing the averages reported during the initial lockdown in the summer of last year (note that, in the chart for the 7-day average of deaths below, the scale is restricted to 15 deaths per million; this cuts off some of the state surges, but shows the US average better than a &lt;a href=&#34;https://github.com/markjrieke/thedatadiary/raw/main/2021.05.30-COVID/p3.png&#34;&gt;chart with an unedited scale&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_01.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_03.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;According to the US Vaccine Tracker, &lt;a href=&#34;https://twitter.com/USVaccineCount&#34;&gt;41% of Americans are fully vaccinated&lt;/a&gt; as of today. Even more heartening is the fact that the partisan divide regarding vaccine hesitancy, while still existent, &lt;a href=&#34;https://twitter.com/gelliottmorris/status/1397579777448550404&#34;&gt;is shrinking&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;While this is certainly good news about the country as a whole, individual states, counties, and cities may have varying levels of success in curbing the spread of the virus in the local community. The New York Times created a &lt;a href=&#34;https://www.nytimes.com/interactive/2021/us/covid-cases-deaths-tracker.html&#34;&gt;helpful dashboard&lt;/a&gt; that lets you look at the spread of the virus on a state, county, or metro area scale.&lt;/p&gt;
&lt;p&gt;All this is to say that, while the pandemic is still not (and possibly will never be) a thing of the past, the introduction of vaccines certainly has appeared to help curb the spread.&lt;/p&gt;
&lt;div id=&#34;catching-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Catching Up&lt;/h3&gt;
&lt;p&gt;It’s been a while since I’ve written, due to a flurry of weddings and trips. While I had originally hoped to get back onto a regular weekly writing schedule, I will again have to push out the next post until late June, due to some exciting personal news: June 21st, I’ll be starting a new job! I’ll be transitioning from engineering in the oil &amp;amp; gas industry to healthcare analytics, which I’m really excited about! In the meantime, however, there’s a flurry of work to do in my current job to ensure that the person backfilling me is prepared to pick up the projects I’m working on. Hopefully after that, I can get back to a regular schedule.&lt;/p&gt;
&lt;p&gt;As always, you can find my work on &lt;a href=&#34;https://github.com/markjrieke/thedatadiary/tree/main/2021.05.30-COVID&#34;&gt;github&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>President of the Polls</title>
      <link>https://www.thedatadiary.net/blog/2021-05-05-president-of-the-polls/</link>
      <pubDate>Wed, 05 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-05-05-president-of-the-polls/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-05-05-president-of-the-polls/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Elliot Morris wrote in his &lt;a href=&#34;https://gelliottmorris.substack.com/p/are-preferences-for-more-government&#34;&gt;newsletter last week&lt;/a&gt; about the increase in the public’s opinion that the government ought to be doing more. A summary of the last twenty years of polling by NBC shows that, while there seems to be a reactive effect based on the party of the president, support for increased government activity has generally been on the rise.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;govt.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The combination of Biden’s &lt;a href=&#34;https://www.pewresearch.org/politics/2021/03/11/biden-viewed-positively-on-many-issues-but-public-is-less-confident-he-can-unify-country/&#34;&gt;approval across several categories&lt;/a&gt;, &lt;a href=&#34;https://www.nytimes.com/2021/02/04/us/politics/biden-approval-rating-republicans.html&#34;&gt;public support of his proposals&lt;/a&gt; (with the exception of his original cap on refugees, which he then &lt;a href=&#34;https://www.cnn.com/2021/05/03/politics/refugee-cap/index.html&#34;&gt;raised after public outcry&lt;/a&gt;), and the public support of increased government activity gives Biden a strong argument in pressing congress to get his progressive policies pushed through to his desk. Despite the popularity, his $4 trillion &lt;a href=&#34;https://www.nytimes.com/2021/04/28/upshot/biden-families-plan-american-rescue-infrastructure.html&#34;&gt;infrastructure and families plan&lt;/a&gt; largely depends on what the senate parliamentarian will allow into a reconciliation bill (which only needs majority approval, rather than a filibuster-proof 60 votes) and the votes of a few moderate Democratic senators, since the bills will receive &lt;a href=&#34;https://nypost.com/2021/05/04/mcconnell-no-gop-senator-will-back-bidens-4-trillion-infrastructure-plan/&#34;&gt;no Republican support in the senate&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;outlook-for-the-coming-weeks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Outlook for the Coming Weeks&lt;/h3&gt;
&lt;p&gt;I’ll be taking a break for the next few weeks, due to a flurry of weekend trips/weddings now that I/most of my friends are fully vaccinated. I’ll continue to work on the database in the background (notably, I need to dive deep into the census bureau demographic data, which needs a lot of fixing). I’ll likely write a short update in either late May or early June.&lt;/p&gt;
&lt;p&gt;As always, you can find my work on &lt;a href=&#34;https://github.com/markjrieke/thedatadiary/tree/main/2021.05.03-misc_ish&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Lukewarm Case for DC Statehood</title>
      <link>https://www.thedatadiary.net/blog/2021-04-27-a-lukewarm-case-for-dc-statehood/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-04-27-a-lukewarm-case-for-dc-statehood/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-04-27-a-lukewarm-case-for-dc-statehood/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Once again, I’ll be keeping this very short, as I’ve continued to primarily focus on &lt;a href=&#34;https://github.com/markjrieke/electionsimulator/tree/main/notepad&#34;&gt;building out the demographic/economic database&lt;/a&gt;. I, did, however, sneak in some time to explore the &lt;a href=&#34;https://www.census.gov/data/tables/2020/dec/2020-apportionment-data.html&#34;&gt;2020 Census Apportionment Results&lt;/a&gt; that were released yesterday. Some surprises came out of the release: Texas and Florida underperformed expectations, only gaining two and one seat, respectively; Arizona didn’t gain a seat; and New York lost a seat but was apparently &lt;a href=&#34;https://www.nytimes.com/2021/04/26/nyregion/new-york-census-congress.html&#34;&gt;89 people short of retaining its seats&lt;/a&gt;. Given that Texas and Florida each gained one less seat than was generally expected, there’s been some speculation online that &lt;a href=&#34;https://www.washingtonpost.com/business/are-you-a-citizen-the-trump-census-controversy-explained/2021/04/26/1cfe6ed8-a6e1-11eb-a8a7-5f45ddcdf364_story.html&#34;&gt;Donald Trump’s attempt to undercount Hispanics&lt;/a&gt; was successful, but backfired. I’ll hold my judgments on this theory until I can read some more in-depth opinions (if they get written).&lt;/p&gt;
&lt;p&gt;With the new apportionment and population data, each individual’s representative power in congress has shifted. Including both Senators and House Representatives, states with smaller populations continue to have outsized representation in congress: Wyoming and Vermont have significantly more representatives per million residents than California or Texas, for example. Residents of Washington D.C., however, receive no representation in congress.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;plot.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;As a side note, the reps-per-million will be greater than the total number of congressional representatives in states with less than one million residents. This chart is really meant to compare the uneven representative power of each state.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If D.C. were to become a state, it would have outsized representative power, similar to Wyoming and Vermont, due to its size. That, however, isn’t really a justification to deny ~700,000 Americans national representation, and the arguments against D.C. statehood have been &lt;a href=&#34;https://www.washingtonpost.com/opinions/treating-fellow-citizens-as-landfill/2021/03/24/f4c16d34-8c1a-11eb-a6bd-0eb91c03305a_story.html&#34;&gt;weak at best&lt;/a&gt; and &lt;a href=&#34;https://www.cnn.com/2020/06/26/politics/tom-cotton-wyoming-dc-statehood/index.html&#34;&gt;implicitly racist at worst&lt;/a&gt;, given D.C.’s &lt;a href=&#34;https://en.wikipedia.org/wiki/Washington,_D.C.#Demographics&#34;&gt;majority-minority population&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Still, it’s virtually impossible that D.C. statehood receive the filibuster-proof 60 votes in the Senate (I’d doubt that even a compromise of &lt;a href=&#34;https://www.forbes.com/sites/andrewsolender/2021/04/22/gop-senator-introduces-bill-to-give-dc-to-maryland-as-statehood-bill-heads-to-senate/?sh=c66ab4c2ca30&#34;&gt;retrocession into Maryland&lt;/a&gt; would pass). For the time being, residents of D.C. will have to wait for representation until polarized ideological walls come down and Republicans vote for D.C. statehood, Democrats gain enough seats for a supermajority, or the filibuster is abolished.&lt;/p&gt;
&lt;p&gt;As always, you can find my work on &lt;a href=&#34;https://github.com/markjrieke/thedatadiary/tree/main/2021.04.27-bureau_results&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Catching Up</title>
      <link>https://www.thedatadiary.net/blog/2021-04-20-catching-up/</link>
      <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-04-20-catching-up/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-04-20-catching-up/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The past few weeks have been a bit lite (read::absent) in terms of posting updates. When not taking the machine learning course, I’ve been spending quite a bit of time working on &lt;a href=&#34;https://github.com/markjrieke/electionsimulator/tree/main/data/tidy&#34;&gt;building out a database&lt;/a&gt; of demographic data (from 1980 to 2019). The goal is to be able to build this database out once, then reference it repeatedly for future projects. If you’re so inclined, you can read my unedited ad-hoc thoughts while working on the database &lt;a href=&#34;https://github.com/markjrieke/electionsimulator/tree/main/notepad&#34;&gt;here&lt;/a&gt;. There’s lots of code interspersed with prose, so it’s a bit of a slog to read through, but gives a really detailed account of how I worked through the different problems that arose.&lt;/p&gt;
&lt;p&gt;That being said, I did have a bit of time to squeeze in a quick chart. YouGov recently conducted a &lt;a href=&#34;https://today.yougov.com/topics/politics/articles-reports/2021/04/14/best-and-worst-states-democrats-republicans&#34;&gt;poll&lt;/a&gt; on each state’s favorability rating according to Democrats and Republicans. Not surprisingly, state favorability is closely aligned with Biden’s voteshare in November.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;yougov.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I’ll keep this short and sweet. I may continue to write sparsely over the next month or so, due to a few trips (now that I have the COVID vaccine), but will continue to work on the database in the background.&lt;/p&gt;
&lt;p&gt;As always, you can find my work on &lt;a href=&#34;https://github.com/markjrieke/thedatadiary/tree/main/2021.04.18-misc&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Seasonality is a Weak Predictor of Border Crossings</title>
      <link>https://www.thedatadiary.net/blog/2021-04-05-seasonality-is-a-weak-predictor-of-border-crossings/</link>
      <pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-04-05-seasonality-is-a-weak-predictor-of-border-crossings/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-04-05-seasonality-is-a-weak-predictor-of-border-crossings/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Last week, the Washington Post published an &lt;a href=&#34;https://www.washingtonpost.com/politics/2021/03/23/theres-no-migrant-surge-us-southern-border-heres-data/&#34;&gt;article&lt;/a&gt; postulating that the recent increase in crossings at the border cannot be attributed to Biden administration policies, but rather is a function of seasonal patterns and pent up demand due to restricted travel in 2020. While the article is well intentioned, a simple linear model that predicts crossings based on month shows seasonality is a poor predictor (or, at least is a poor predictor on its own).&lt;/p&gt;
&lt;div id=&#34;examining-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Examining the Data&lt;/h3&gt;
&lt;p&gt;The US Border Patrol (USBP) publishes &lt;a href=&#34;https://www.cbp.gov/newsroom/stats/southwest-land-border-encounters&#34;&gt;monthly data&lt;/a&gt; summarizing the number of encounters/crossings at the Southern border. Since November of 2020, border crossings have increased monthly, from about 70,000 in November to a little over 100,000 in February.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_01.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the long term monthly data, however, reveals two key aspects:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;2019 was an exceptionally high year for crossings, and 2021 is on track to have a similar number of crossings;&lt;/li&gt;
&lt;li&gt;There seems to be a cyclical set of peaks and troughs in annual border crossing numbers.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_02.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the chart above, it’s difficult to determine which months correspond to peaks and troughs. Aligning by month offers a clearer picture of how monthly border crossings in each year compare to other years.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_03.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this chart, it does appear that there is some semblance of a monthly pattern, albeit with extreme outliers of 2019, 2020, and 2021. Appearances, however, can be deceiving, and a mathematical model can help distinguish signal from noise. Using month as a predictive input, we can create a model that estimates the expected number of crossings in said month. If we look at the residual error (the actual number of crossings minus the predicted number of crossings) over time, we can get an idea of how well the model is performing. If seasonality is a good predictor of the number of border crossings, the model will accurately account for seasonal shifts, and the residual error plot would be expected to remove or reduce the “waviness” that appears in the crossing plot and only show the long term trends.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_04.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Plotting the residual error, however, doesn’t filter out seasonal noise. In fact, apart from the y-axis shifting down, the residual plot is strikingly similar to the crossings plot. This means that the month offers little power on its own in predicting the number of border crossings.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;closing-remarks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Closing Remarks&lt;/h3&gt;
&lt;p&gt;The Washington Post article makes good points that aren’t discussed fully here, but are worth mentioning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Those who planned on crossing the border in 2020 but couldn’t due to travel restrictions may have simply waited until now to travel.&lt;/li&gt;
&lt;li&gt;Unaccompanied migrants are arriving at the border in rates that exclude the possibility of seasonal patterns.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also notable is that seasonal patterns - while not a good predictor of the number of border crossings on its own - may be more impactful in a more robust model. &lt;a href=&#34;https://www.nbcsandiego.com/news/local/professor-reacts-to-wapo-articles-claims-border-is-not-seeing-surge-or-crisis/2559256/&#34;&gt;As a professor from the University of San Diego explains&lt;/a&gt;, conditions in the migrant’s country of origin and the countries they pass through are among a host of variables that play a significant role in determining if migrants make the trip to the US border. While a more robust model may be able to accurately incorporate season into its predictions, simply using month as a predictor, as the Washington Post does in their article, does not explain the surge at the border (evidenced by the fact that this equally simple model refutes their finding).&lt;/p&gt;
&lt;p&gt;Finally, it should be noted that, above all else, there is a humanitarian crisis at the border. If you’d like to find a way to help or donate, &lt;a href=&#34;https://www.globalgiving.org/us-mexico-border-crisis/&#34;&gt;Global Giving&lt;/a&gt; is an excellent resource.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-other-updates&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some Other Updates&lt;/h3&gt;
&lt;p&gt;Here’s what I’ve been working on recently:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TidyTuesday&lt;/strong&gt;: I’ve made my first contribution to &lt;a href=&#34;https://twitter.com/search?q=%23tidytuesday&amp;amp;src=typed_query&#34;&gt;#tidytuesday&lt;/a&gt;! For those who are unaware, TidyTuesday is a weekly data project in R for the R4DS (R for data science) community. Each week, a public dataset is released on Monday, allowing users to explore and create interesting visualizations and analyses based on the dataset. This week, a &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-04-06/readme.md&#34;&gt;dataset&lt;/a&gt; based on global deforestation was released, but it came along with interesting information on the production of vegetable oil. I plotted the global annual vegetable oil production by crop type in the steamgraph below.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;pics/tidytuesday15.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Courses&lt;/strong&gt;: I’ve continued with the machine learning and Bayesian courses, though I’ve descoped to roughly two days each week, as my personal schedule has gotten quite hectic. The ML course is excellent for understanding the theory that goes into each algorithm, but in practice, I expect that I will likely make use of the tidymodels R package’s relatively simple interface.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rmd&lt;/strong&gt;: I’ve started writing my code in a R markdown document, which allows me to mix both code and prose. I mix in some off-the-rails/stream of consciousness commentary as I step through the process to get to a functional code. For regularly updated/critical code/projects, I’ll still stick to concise scripts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That’s all for this week - as always, you can read through the code for &lt;a href=&#34;https://github.com/markjrieke/thedatadiary/tree/main/2021.04.02-border_crossings&#34;&gt;this piece&lt;/a&gt; or for the &lt;a href=&#34;https://github.com/markjrieke/thedatadiary/blob/main/tidytuesday/2021.04.05-deforestation/tidydeforestation.md&#34;&gt;tidytuesday&lt;/a&gt; piece on github. See you next week.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Recent Works</title>
      <link>https://www.thedatadiary.net/blog/2021-03-15-recent-works/</link>
      <pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-03-15-recent-works/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-03-15-recent-works/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;One of the history teachers at my highschool was known for his prolific catchphrase, “give me the stuff, not the fluff,” often uttered to students attempting to submit history papers padded with superfluous words and sentences in order to meet a minimum word threshold. While I never took a class with this teacher, the phrase has stuck with me. With that in mind, I’ve reprioritized some of the things I’ve been working on. Here’s what I’ve been up to over the past week:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TX COVID Tracker&lt;/strong&gt;: Over the weekend, I put together and published my first Shiny application, a county-level &lt;a href=&#34;https://thedatadiary.net/texas-covid-tracker&#34;&gt;interactive Texas COVID tracker&lt;/a&gt;. Using &lt;a href=&#34;https://github.com/nytimes/covid-19-data&#34;&gt;data from the New York Times&lt;/a&gt;, the tracker lets the user view county-level historical data. The goal is to fill in the gaps of the &lt;a href=&#34;https://www.nytimes.com/interactive/2020/us/texas-coronavirus-cases.html&#34;&gt;Times’ Texas tracker&lt;/a&gt;, which shows historical state data and live county data, but doesn’t offer historical county data.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Classes&lt;/strong&gt;: I had previously spent evenings working on projects in R and had pushed off some of the statistics classes to focus on these projects. This past week, I’ve scheduled an hour each day for dedicated class time, alternating daily between a Bayesian inference course and a machine learning course.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;my-plans-for-the-upcoming-week&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;My plans for the upcoming week:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TX COVID Tracker&lt;/strong&gt;: While the interactive tracker is functional, it still leaves a lot to be desired. Being the first Shiny app I developed, I ran into a lot of learning curve issues but should be able to make updates more quickly. This upcoming week, I plan on converting to a &lt;a href=&#34;https://rstudio.github.io/shinythemes/&#34;&gt;bootstrap layout&lt;/a&gt;, adding in an interactive state map with &lt;a href=&#34;https://rstudio.github.io/leaflet/shiny.html&#34;&gt;leaflet&lt;/a&gt;, adding in statewide &lt;a href=&#34;https://dshs.texas.gov/coronavirus/AdditionalData.aspx&#34;&gt;hospitalization and vaccination information&lt;/a&gt;, adding a state and county level &lt;a href=&#34;https://www.youtube.com/watch?t=10s&amp;amp;v=54XLXg4fYsc&#34;&gt;log-log plot&lt;/a&gt;, making some formatting changes to help with mobile viewing, and updating themes/appearances. Eventually, I’d like to write a &lt;a href=&#34;https://cran.r-project.org/web/packages/taskscheduleR/vignettes/taskscheduleR.html&#34;&gt;scheduleR&lt;/a&gt; script to automatically update the data, but that may need to be put off to focus on the laundry list above.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Classes&lt;/strong&gt;: I’ll still continue with the daily schedule for coursework and may be able to finish the Bayesian inference course this week.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Between the COVID tracker and classes, I don’t think I’ll have too much time to work on anything else. That being said, I’ve got some projects/articles in the backlog that I’m looking forward to working on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Polarization - top down or bottom up?&lt;/strong&gt; It’s no secret that we are living in the &lt;a href=&#34;https://voteview.com/articles/party_polarization&#34;&gt;most polarized political landscape since the Civil War&lt;/a&gt;, but I’m interested in exploring where this polarization originates. Do elected politicians split the public by pushing increasingly divisive policies? Or do voters lead the polarization effort by increasingly self-segregating into isolated camps?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Two Party PVI (Partisan Voter Index)&lt;/strong&gt;: The &lt;a href=&#34;https://cookpolitical.com/pvi-0&#34;&gt;partisan voter index (PVI)&lt;/a&gt; is a measure of a state’s (or county’s) partisan preference relative to the national environment. There’s a story here about how the national environment is progressing - are our country voting preferences converging or diverging? What states are trending towards Democrats? What states are trending towards Republicans?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Gas Price Fallacy&lt;/strong&gt;: I’ve seen quite a few &lt;a href=&#34;https://twitter.com/ksorbs/status/1362540095241347073&#34;&gt;bad faith attempts to blame increasing gas prices on Joe Biden&lt;/a&gt; recently. Additionally, it’s particularly frustrating that I’ve seen a number of my colleagues in the oil &amp;amp; gas industry repeat this nonsense. As a working member of the oil &amp;amp; gas industry, I think I’m qualified and have a duty to explain why the administration change isn’t the cause of the rising gas prices (the TL;DR version is that we’re recovering from a global pandemic &amp;amp; oil oversupply, you can see that gas prices are just returning to pre-COVID Trump administration levels&lt;a href=&#34;https://www.gasbuddy.com/charts&#34;&gt;here&lt;/a&gt;.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>aRtwork!</title>
      <link>https://www.thedatadiary.net/blog/2021-03-07-artwork/</link>
      <pubDate>Sun, 07 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-03-07-artwork/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-03-07-artwork/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;img src=&#34;p_art.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This week, I did something a bit different - the artwork above was made in R! The code to create the graph is actually pretty short, only taking up 28 lines. The bulk of the work was spent writing the csv containing all the polygon points and colors. I spent most of the past week doing some back end work that will pay off in the future. Namely, I set up a local clone of my repository on github and figured out how to embed Shiny applications on Squarespace.&lt;/p&gt;
&lt;p&gt;I’d like to address something that has been nagging me for a bit. Last week, I wrote about Georgia’s special senate race. I spent a lot of time learning new packages and methods, and am happy with how the plots and majority of the post turned out. Near the end of the post, however, I made the conjecture that Doug Collins may have seen an opportunity to run against Kelly Loeffler because she was a polarizing candidate with a string of controversies surrounding her campaign. While I shared sources that generally support this position, I didn’t back it up with any sort of data or in-depth analysis. I’m not a political scientist nor am I an expert in election strategy, so in hindsight I think it was pretty irresponsible to add my uninformed opinion to the post. I’ll leave the post up unedited, since it’s important to keep a true record of what I’ve written, but moving forward, I’ll be better about writing the story the data reveals, rather than one I’d like to believe.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Doug Collins Saved Raphael Warnock&#39;s Senate Bid</title>
      <link>https://www.thedatadiary.net/blog/2021-02-28-doug-collins-saved-raphael-warnock-s-senate-bid/</link>
      <pubDate>Sun, 28 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-02-28-doug-collins-saved-raphael-warnock-s-senate-bid/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-02-28-doug-collins-saved-raphael-warnock-s-senate-bid/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I’ll be honest - prior to writing this post, I had never heard of Doug Collins (R), the third major candidate in the race for Georgia’s special senate election after Raphael Warnock (D) and Kelly Loeffler (R). I wasn’t particularly tuned in to the Georgia senate elections prior to the runoff (by which time, the special election race had narrowed to just Warnock and Loeffler) and most of the coverage I had seen prior to Nov. 3rd pitted Warnock against Leoffler, without considering Collins. Even Google search trends show that people were more interested in Loeffler than Collins leading up to the election.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_01.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Despite my ignorance, Collins was a major candidate in the special election, and ended up with a significant portion of the republican vote. It may be a bit obvious, but republicans splitting votes between Loeffler and Collins made Warnock a significantly more competitive candidate. However, the role of voter dropoff relative to the regular senate election is worth exploring in detail.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_02.png&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;comparing-results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparing Results&lt;/h3&gt;
&lt;p&gt;Georgia’s regular senate election was a much more typical election than the special election - a tightly contested election between two candidates, Jon Ossoff (D) and David Perdue (R). The county result map comparing the voteshare of the two major candidates shows democratic strongholds in urban areas and a republican lean in rural areas. The county result map of the special election, if comparing Warnock to both Loeffler and Collins, is noticeably redder.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_03.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_04.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we remove Collins, however, and just look at the county map comparing the top two candidates, the map shifts drastically in Warnock’s favor.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_04.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_05.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At face value, this explains how Warnock was able to advance to the runoff - split ticket votes aren’t usually good for the party with multiple major candidates. Voter dropoff between the regular and special election, however, shows just how much this pushed Warnock over the edge.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;voter-retention-a-tale-of-two-elections&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Voter Retention: a Tale of Two Elections&lt;/h3&gt;
&lt;p&gt;About 5 million Georgians voted in each of the senate elections in November. In the regular election, the major candidates, Ossoff and Perdue, accounted for about 98% of the votes, the rest going to Libertarian Shane Hazel or other write-in candidates. In the special election, on the other hand, a huge portion of Georgia voters didn’t vote for the major candidates - Warnock, Loeffler, and Collins only account for 79% of the votes! That’s about a 1 million voters who didn’t vote for their party’s major candidate (i.e., “dropped-off”).&lt;/p&gt;
&lt;p&gt;To get a clear grasp of what voter retention means in this context, let’s consider a hypothetical county with 100 voters, of which 60 voted for Perdue and 40 voted for Ossoff in the regular election. Let’s also say these 100 voters split their special election votes in the following way: 35 votes for Loeffler, 15 votes for Collins, 30 votes for Warnock, and 20 write-in votes. In this scenario, Warnock retained 75% of regular election votes (30 / 40 = 75%), republicans, collectively, retained 83% ([35 + 15] / 60 = 83%), and Loeffler retained 58% (35 / 60 = 58%). In this hypothetical county, republicans collectively improved relative to the regular election, since they had a greater vote retention, but Loeffler on her own worsened, despite winning more votes than Warnock.&lt;/p&gt;
&lt;p&gt;If we compare retentions for the entire state of Georgia, we can see that Loeffler and Collins were collectively better at retaining regular election votes than Warnock, retaining about 90% of Perdue votes compared to Warnock’s retention of about 68% of Ossoff votes. On her own, however, Loeffler was worse at retaining votes than Warnock, retaining a little over 50% of Perdue votes. This means that Warnock improved democratic performance relative to the regular election when compared against Loeffler directly.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_06.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Repeating this comparison across every county in Georgia shows that Loeffler and Collins, collectively, improved republican performance relative to the regular senate election across every single county. When, however, Collins is omitted, and Warnock and Loeffler are compared directly, democratic performance improves across the majority of counties.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_07.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;in-summary&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;In Summary&lt;/h3&gt;
&lt;p&gt;While it was obvious from the beginning that the split ticket hurt republicans in Georgia’s special senate election, it’s interesting to see just how much this split ticket helped Warnock. Collectively, republicans handily won the special election, but splitting votes between Loeffler and Collins meant that Warnock ended up winning a &lt;a href=&#34;https://en.wikipedia.org/wiki/Plurality_(voting)&#34;&gt;plurality&lt;/a&gt; of the votes.&lt;/p&gt;
&lt;p&gt;Political analysists are probably better than me at examining why multiple republicans ran as major contenders for the special senate election, but a quick take is that Kelly Loeffler wasn’t a particularly strong candidate. Although she was an incumbent, she wasn’t a senator any Georgian had ever voted for - &lt;a href=&#34;https://www.ajc.com/news/state--regional-govt--politics/inside-how-kemp-picked-loeffler-for-the-senate/ftgH6xSI2Lnl0QLgqSQu8O/&#34;&gt;she gained her senate seat through appointment&lt;/a&gt;. Loeffler &lt;a href=&#34;https://www.nytimes.com/2020/10/05/us/politics/kelly-loeffler-georgia.html&#34;&gt;came to office as a moderate&lt;/a&gt;, but quickly pivoted to a more &lt;a href=&#34;https://projects.fivethirtyeight.com/congress-trump-score/kelly-loeffler/&#34;&gt;Trumpy ideology&lt;/a&gt;, eventually defining herself as “more conservative than Atilla the Hun” in a campaign ad (I would normally just add a hyperlink, but this ad is just too weird not to link directly - I originally thought this was a bad parody).&lt;/p&gt;
&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/4pfvEFPvVGA&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Her campaign was also plagued by a few scandals, like &lt;a href=&#34;https://www.ibtimes.sg/kelly-loefflers-campaign-caught-darkening-skin-opponent-raphael-warnock-facebook-ad-54651&#34;&gt;darkening Warnock’s skin in a campaign ad&lt;/a&gt;, taking photos with &lt;a href=&#34;https://www.washingtonpost.com/nation/2020/12/14/loeffler-chester-doles-kkk-photo/&#34;&gt;known KKK members&lt;/a&gt;, and potentially &lt;a href=&#34;https://www.politifact.com/factchecks/2020/oct/31/raphael-warnock/fact-checking-raphael-warnocks-claim-georgia-sen-k/&#34;&gt;making stock trades based on COVID-19 information not-yet released to the public&lt;/a&gt;. It’s a bit of a conjecture, so take this with a grain of salt, but all of the above factors may have provided Collins an opportunity to run as a stronger &amp;amp; less divisive alternative to Loeffler.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-final-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some Final Notes&lt;/h3&gt;
&lt;p&gt;Choropleth charts (maps) can be misleading in the wrong context - it’s easy to subconsciously associate area with population. It wasn’t particularly relevant to the post above, but in spirit of the animation “&lt;a href=&#34;https://s3files.core77.com/blog/images/960537_81_90771_DrdO3qFgW.gif&#34;&gt;Land doesn’t vote, people do&lt;/a&gt;”, I added dot-plot maps where bubble size corresponds to total number of votes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_08.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_09.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As always, source data and code can be found on &lt;a href=&#34;https://github.com/markjrieke/thedatadiary/tree/main/2021.02.28-ticket_splitting&#34;&gt;github&lt;/a&gt;. I’m particularly happy with how this post turned out. I learned a lot of new things worth highlighting (some of this gets into technical mumbo jumbo):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learned how to use the maps, gganimate, and gifski packages;&lt;/li&gt;
&lt;li&gt;Picked up new method of piping objects into a ggplot object;&lt;/li&gt;
&lt;li&gt;Worked with geom_col and geom_poly for the first time;&lt;/li&gt;
&lt;li&gt;Used forcats for the first time (just to reorder a factor, but I’ll still count it as a win);&lt;/li&gt;
&lt;li&gt;Added new colors to the dd color palette;&lt;/li&gt;
&lt;li&gt;Created a reusable theme var to reduce code;&lt;/li&gt;
&lt;li&gt;Started using tibbles.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For this post, I had originally wanted to compare presidential vote to senate, governor, and house vote, but I had to pivot a bit to just the Georgia senate elections, for a few reasons. Firstly, the senate county dataset from Kaggle, for whatever reason, doesn’t include the winner of each county. Adding this by hand, just for Georgia’s 159 counties and two senate elections took hours. There’s probably a more efficient way to do this by scraping the data from online, but then I’d have to have learned a scraping package in addition to all of the other packages. I may have to hold on the original post idea for a bit while I pick up these skills.&lt;/p&gt;
&lt;p&gt;That being said, I’ve already started working on next week’s post. It’s a bit of a departure from what I’ve been doing, so I’m excited to see how it’ll turn out.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>(Kind of) Projecting the 2020 Election</title>
      <link>https://www.thedatadiary.net/blog/2021-02-21-kind-of-projecting-the-2020-election/</link>
      <pubDate>Sun, 21 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-02-21-kind-of-projecting-the-2020-election/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-02-21-kind-of-projecting-the-2020-election/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I’m about 4 months too late, but to practice forecasting, I made a modest projection for the 2020 national popular vote using &lt;a href=&#34;https://projects.fivethirtyeight.com/2020-general-data/presidential_polls_2020.csv&#34;&gt;polling data&lt;/a&gt; from the two weeks leading up to election day. In the weeks leading up to the election, aggregated polling was fairly stable, and the model projection was within 1.5% of the &lt;a href=&#34;https://en.wikipedia.org/wiki/2020_United_States_presidential_election&#34;&gt;actual outcome&lt;/a&gt; (both the model and actual outcome are adjusted to exclude third parties and instead show the two-party vote share).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_01.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The projected outcome of Biden’s vote share, 53.6%, is only the most likely outcome in a &lt;a href=&#34;https://en.wikipedia.org/wiki/Beta_distribution&#34;&gt;distribution of possible outcomes&lt;/a&gt; predicted by the model. Of the possible outcomes, Biden wins the popular vote about 70% of the time, according to the November 2nd projection.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_03.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Like the projected vote share, the probability of Biden winning the popular vote remained fairly constant in the two weeks leading up to the election.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_02.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you were like me, obsessing over the prominent forecast models prior to election day, you may notice that this projection is substantially less confident in the outcome than the leading forecasters. &lt;a href=&#34;https://projects.fivethirtyeight.com/2020-election-forecast/&#34;&gt;FiveThirtyEight&lt;/a&gt; and the &lt;a href=&#34;https://projects.economist.com/us-2020-forecast/president&#34;&gt;Economist&lt;/a&gt;, for example, both projected similar popular vote outcomes (within a percentage point of this forecast), but gave Biden at least a 97% of winning the popular vote! There are a couple reasons for this difference:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;This model is one-dimensional&lt;/strong&gt; : this is a pretty simple model built just to get practice with forecasting and some of the tools in R, so it only uses polls (and, at that, only a small subset of polls), whereas other forecast models used a wide variety of variables to inform the model (economic indicators, demographics, partisanship, etc.).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;This model doesn’t weight polls&lt;/strong&gt; : aside from the sample size of the poll, this model doesn’t apply any weights or corrections to the polling data. The polling method, date the poll was taken, and pollster &lt;a href=&#34;https://fivethirtyeight.com/features/calculating-house-effects-of-polling-firms/&#34;&gt;house effect&lt;/a&gt; (i.e., how partisan the pollster tends to be relative to the average) can be used to inflate or deflate the weight of each poll in the model. This simple model ignores all of that and treats every poll as equal.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;This model forces an uncertainty range&lt;/strong&gt; : unlike other models, which are a set of &lt;a href=&#34;https://en.wikipedia.org/wiki/Regression_analysis&#34;&gt;linear regressions&lt;/a&gt;, this model is a relatively simple &lt;a href=&#34;https://en.wikipedia.org/wiki/Beta_distribution&#34;&gt;beta distribution&lt;/a&gt; of the vote, with the sum of parameters manually set to 50. This is a bit of technical mumbo-jumbo, but the gist is that a beta distribution allows you to control its “peaky-ness,” and I did this manually, whereas other forecasters had the model do it for them. Increasing the sum of parameters increases how peaky the distribution looks, and a sum of 50 was used based on &lt;a href=&#34;http://florianmuellerklein.github.io/election/&#34;&gt;Florian Muellerklein’s Georgia runoff model&lt;/a&gt;, which also used a sum of 50.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;some-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some Notes&lt;/h3&gt;
&lt;p&gt;As always, you can find source data and code on &lt;a href=&#34;https://github.com/markjrieke/thedatadiary/tree/main/2021.02.21-simple_projection&#34;&gt;github&lt;/a&gt;. I’m pretty happy with how this turned out - I’ve been getting a bit more comfortable with R, and the tools used for this post were pretty intuitive to implement. I’m also happy with the color palette I selected (&lt;a href=&#34;https://htmlcolorcodes.com/color-picker/&#34;&gt;HTML Color Picker&lt;/a&gt; is a godsend). The only improvement is that I could/should have saved quite a bit of code by writing over one of the plot themes, rather than re-writing the theme for each plot. Something to remember going forward.&lt;/p&gt;
&lt;p&gt;Next week, I’ll dig into some of the county-level data from the election to see if there was any ticket splitting between the presidential election and the down-ballot races.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Scorecasting</title>
      <link>https://www.thedatadiary.net/blog/2021-02-14-scorecasting/</link>
      <pubDate>Sun, 14 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-02-14-scorecasting/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-02-14-scorecasting/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;A baseball forecast that correctly predicts the winner of the &lt;a href=&#34;https://www.mlb.com/schedule/2021-04-01&#34;&gt;15 opening day games&lt;/a&gt; could be a truly accurate model, or could just be getting lucky. Over the course of 6 months and &lt;a href=&#34;https://en.wikipedia.org/wiki/Major_League_Baseball_schedule&#34;&gt;2,430 regular season games&lt;/a&gt;, however, a great forecast will continue to shine whereas an initially lucky one will falter. In data rich environments like sports, there are lots of events (games) over the course of a season to judge how well a model is performing.&lt;/p&gt;
&lt;p&gt;When there aren’t a lot of events to forecast, like presidential elections that occur once every four years (quadrennially?), it’s more difficult to tell how well a forecast performs (a handful of correct/incorrect predictions could just be good/bad luck!), but forecasters still have tools available to evaluate their model. This week, I took a look at a few different methods of comparing different models.&lt;/p&gt;
&lt;div id=&#34;scoring-the-forecasters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Scoring the Forecasters&lt;/h3&gt;
&lt;p&gt;The most common scoring method, the Brier score, is a measurement of a probabilistic forecast’s accuracy based on the confidence of the prediction. Each event has its own score, and the average of all predicted events is the model’s Brier score. Highly confident predictions are highly rewarded/punished for their accuracy/inaccuracy, whereas timid predictions don’t move the needle too much, regardless of the outcome. Scores can range from 0.00 to 1.00, with 0.00 being a perfect score (you can read more about Brier scores &lt;a href=&#34;https://en.wikipedia.org/wiki/Brier_score&#34;&gt;here&lt;/a&gt;, but the gist is that the lower the score, the better). In presidential forecasts, each state can be considered an event.&lt;/p&gt;
&lt;p&gt;Oftentimes, presidential forecasters report Brier scores as weighted by each state’s number of electors. Correctly predicting Texas’ winner in this scoring system is far more important than correctly predicting Wyoming’s winner, given that Texas’ 38 electoral votes far overshadow Wyoming’s 3. I’m not quite convinced that this is the most meaningful way to evaluate models (California is pretty easy to predict and heavily weighted by its 55 electoral votes, but most of us were more concerned with Georgia’s outcome, despite being undervalued by this scoring method), but &lt;a href=&#34;https://projects.jhkforecasts.com/presidential-forecast/forecast-analysis&#34;&gt;forecasters do use this scoring method&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Finally, I came up with my own scoring method that makes a “bet” between $0 and $100 based on prediction confidence. 100% confidence would turn into a $100 bet, and 50% confidence (aka, a coin toss) would be a $0 bet. Each model’s average winnings (including losses) is reported as the score.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/pic_01.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I scored a few prominent presidential forecasts based on the above methodologies (you can read more about the &lt;a href=&#34;https://projects.jhkforecasts.com/presidential-forecast/&#34;&gt;JHK&lt;/a&gt;, &lt;a href=&#34;https://projects.economist.com/us-2020-forecast/president&#34;&gt;Economist&lt;/a&gt;, &lt;a href=&#34;https://projects.fivethirtyeight.com/2020-election-forecast/&#34;&gt;FiveThirtyEight&lt;/a&gt;, and &lt;a href=&#34;https://thecycle.news/news/september-2020-election-update&#34;&gt;Bitecofer&lt;/a&gt; forecasts at the links here). While all the scoring methods are similar - rewarding confidence in correct predictions and penalizing meek or incorrect predictions - each model’s performance is all over the map (with perhaps the exception of Bitecofer, which scores in the lower half of all methods). But does that mean these methods are useless? No! If anything, it highlights the importance that each forecast method’s performance should be scored across a wide variety of scoring methodologies. While it might not make a huge difference at the margins, it may separate some models as clearly ahead or behind the curve.&lt;/p&gt;
&lt;p&gt;So, how does each forecast model measure up? It depends on which yardstick you use!&lt;/p&gt;
&lt;p&gt;Here are some other things to consider with regards to each model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unlike the Economist, FiveThirtyEight, and Bitecofer models, which simulate each state’s outcome thousands of times and reports the confidence as the percentage of simulations won, the JHK forecast reports the percentage of winning results that fall within the 80% confidence interval around the expected vote. &lt;a href=&#34;https://en.wikipedia.org/wiki/Standard_deviation#/media/File:Standard_deviation_diagram.svg&#34;&gt;Lower confidence intervals result in tighter bands&lt;/a&gt;, so this relatively low confidence interval means that the model is allowed to make quite a few 100% confident predictions (which, luckily for the model, all came true in this case).&lt;/li&gt;
&lt;li&gt;The Bitecofer forecast is the only model that doesn’t utilize polls and instead uses an in-house developed negative-partisanship model.&lt;/li&gt;
&lt;li&gt;Based on conversations between Nate Silver of FiveThirtyEight and Elliott Morris of the Economist, it appears that &lt;a href=&#34;https://fivethirtyeight.com/features/our-election-forecast-didnt-say-what-i-thought-it-would/&#34;&gt;FiveThirtyEight added a bit of uncertainty ad-hoc&lt;/a&gt; to account for COVID, whereas &lt;a href=&#34;https://www.reddit.com/r/IAmA/comments/jjr0wk/im_elliott_morris_a_data_journalist_at_the/gaeefhv?context=3&amp;amp;utm_medium=web2x&amp;amp;utm_source=share&#34;&gt;the Economist did not&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As always, source files and code can be found on &lt;a href=&#34;https://github.com/markjrieke/thedatadiary/tree/main/2021.02.14-scorecasting&#34;&gt;github&lt;/a&gt;. Next week, I plan on making a relatively modest prediction of the election’s popular vote outcome based on polls from the final week.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some Worthwhile Links</title>
      <link>https://www.thedatadiary.net/blog/2021-02-08-some-worthwhile-links/</link>
      <pubDate>Mon, 08 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-02-08-some-worthwhile-links/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-02-08-some-worthwhile-links/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Rather than diving deep into a topic this week, I took a bit of a break to focus on playing catchup with the stats course I’m taking. Instead, I’ve listed out below a number of creators that I follow on various platforms. If you’re interested in data or critical thinking, each is well worth your time and attention.&lt;/p&gt;
&lt;div id=&#34;newsletters&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Newsletters&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://fivethirtyeight.com/&#34;&gt;FiveThirtyEight&lt;/a&gt; : While it’s not technically a newsletter, the FiveThirtyEight site, created by statistician Nate Silver prior to the 2008 presidential election, is the original source for data-driven news (or, at least, one of the first sites to popularize data as a news resource). Nate created the site specifically because the narrative created by pundits, that the 2008 election was super close, was pretty easily refutable when you looked at the polling data, which showed that Obama was going to win handily. Now, FiveThirtyEight is a powerhouse of data analysis for politics, sports, and science, and often serves as a good reality check against the narratives espoused by talking heads on the major news networks.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gelliottmorris.substack.com/&#34;&gt;G. Elliott Morris’ Newsletter&lt;/a&gt; : G. Elliott Morris is a data journalist for the Economist and created their forecast for the 2020 presidential election. In addition to regularly writing for the Economist, Elliott also writes a weekly newsletter in which he comments on polls that caught his eye (he also has a subscriber newsletter, for those who want to get his thoughts on even more topics). Elliott is also writing a book on the history of public polls, their limitations, and their future in American politics, which I am looking forward to reading when it releases later this year.
As an aside, Elliott and I are the same age, and he was a large part of the inspiration for me to start diving into statistics again (i.e., if he can do it, so can I). Elliot is pretty bearish on the future of American democracy, especially following Donald Trump’s repeated attempts to overturn the overwhelmingly clear and overwhelmingly fair results of the election and the attempted (but, thankfully, woefully unorganized) coup by insurrectionists on Jan. 6th. Despite all this, I’m a bit more hopeful for the future of democracy, and hope to be able to provide a more positive opinion alongside Elliott’s (provided it’s supported by the data!).&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://infinitemonkeys.substack.com/&#34;&gt;Infinite Monkeys&lt;/a&gt; : Started by a collection of college students who met via #ElectionTwitter, the Infinite Monkeys newsletter (named such from the theory that, an infinite collection of monkeys hitting keys on typewriters will eventually write the entirety of Shakespeare’s work by random chance) take a look at geographical trends and their relation to current political headlines. It’s a relatively recent startup, and I’m looking forward to the development of the newsletter &amp;amp; its coalition of authors over the coming years.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://visualnoise.substack.com/&#34;&gt;Visual in the Noise&lt;/a&gt; : The Visual in the Noise is a weekly newsletter focused on data visualization. Most often looking at sports (particularly, NBA) data, the Visual in the Noise is a great touch-point for the importance of visualization and how it can help make data more insightful.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;podcasts&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Podcasts&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nytimes.com/column/the-daily&#34;&gt;The Daily&lt;/a&gt; : The Daily is, appropriately, a daily (Mon. - Fri.) podcast hosted by the New York Times, covering important topics in the American landscape. The podcast generally focuses in on individuals, and how national stories can affect people personally (for example, touching base with a bar throughout the pandemic as they wade through the difficulties of diminished business, PPP applications, and unclear direction from the government). On Sundays, a guest reads an older, long form New York Times piece.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://theintelligence.economist.com/&#34;&gt;The Intelligence&lt;/a&gt; : The Intelligence, similarly to the Daily, is a daily weekday podcast covering important topics in the news, though typically has a more global focus than the Daily. Rather than following individuals, the Intelligence often brings in subject matter experts and local correspondents.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.economist.com/checks-and-balance-our-weekly-podcast-on-american-politics&#34;&gt;Checks and Balance&lt;/a&gt; : This weekly podcast by the Economist takes a deep dive into one big topic shaping American politics each week. Approximately 45 minutes per podcast, the Checks and Balance hosts take care to thoroughly explore each topic.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://fivethirtyeight.com/podcasts/&#34;&gt;FiveThirtyEight’s Politics Podcast&lt;/a&gt; : Every week, the FiveThirtyEight team covers the latest news in politics, utilizing polling data to guide their discussion. During election years, they also host intermittent “Model Talks,” where Nate Silver talks about into some of the intricacies of the site’s forecast models based on questions from listeners.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rationalreminder.ca/podcast-directory&#34;&gt;The Rational Reminder&lt;/a&gt; : A non-politics podcast, the Rational Reminder is a weekly podcast discussing index investing and rational decision making. Although the podcast is made by and for Canadians, most of the content is widely applicable, and the podcast has garnered an international audience.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;youtube&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;YouTube&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw&#34;&gt;3blue1brown&lt;/a&gt; : Many concepts in math can feel daunting and teaching methods are often unintuitive. Grant Sanderson’s channel attempts to introduce viewers to the beauty of math through intuitive visualizations and animations. As a fun fact, the animations are run via a python package, manim, developed on-the-fly by Grant himself!&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/user/sxephil&#34;&gt;Philip DeFranco&lt;/a&gt; : One of the original members of the YouTube community, Phil has grown from a weekly commentary on popular videos via a webcam in his bedroom to a daily rundown of the news backed by a full production staff. Phil does an excellent job of presenting the news whilst making it clear where the official reporting stops and his opinion starts.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCpa-Zb0ZcQjTCPP1Dx_1M8Q&#34;&gt;Legal Eagle&lt;/a&gt; : Dubbed “YouTube’s Lawyer,” Devin (DJ) Stone provides a perspective on the role the law plays in current events and controversies (as well as more fun videos, like reviewing a Spongebob episode for legal accuracy). I’m not sure how he manages to balance the two full time jobs of running channel with near-daily longform content and being a lawyer with active litigation, but I appreciate that he is able to find time for thoughtful (and often comedic) insight.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCDXTQ8nWmx_EhZ2v-kp7QxA&#34;&gt;Common Sense Investing&lt;/a&gt; : Ben Felix’s Common Sense Investing investigates the academic research supporting passive, rather than active, portfolio management (in summary: the data shows that passive index investing is overwhelmingly a more effective long term investment strategy than trusting an active portfolio manager with your money).&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/user/standupmaths&#34;&gt;Standup Maths&lt;/a&gt; : Mathematician, comedian, and Excel-enthusiast Matt Parker shares the joy that can be found in math by exploring topics in a comedic setting. Matt’s book, Humble Pi, explores some of history’s most famous mathematical blunders, and is coming up soon on my reading list.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/user/numberphile&#34;&gt;Numberphile&lt;/a&gt; : The Numberphile channel is a collection of interviews of prominent mathematicians explaining interesting historical math problems on trademark brown parchment paper. Grant Sanderson and Matt Parker make appearances on the channel a number of times (the infamous Parker square first made its appearance on Numberphile).&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/user/minutephysics/featured&#34;&gt;MinutePhysics&lt;/a&gt; : As Henry Reich, the channel owner, puts it, the channel is simply about “cool physics and other sweet science.” Henry’s videos explain concepts in physics via a whiteboard, expo markers, and a backdrop of jazzy standup bass.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;election-twitter&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Election Twitter&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;There are quite a few, so I’ll just highlight a few &amp;amp; link ot the rest&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/lxeagle17&#34;&gt;Lakshya Jain&lt;/a&gt; : Lakshya is a software engineer and self-described amateur elections mapper/analyst. He’s a very vocal (and self-labeled) partisan democrat, so I take his non-analytical posts with a grain of salt, but his analytical posts are very insightful.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/jhkersting&#34;&gt;Jack Kersting&lt;/a&gt; : A relative rarity on Election Twitter, Jack is a conservative forecaster. He’s not an ardent twitter user, but developed one of the most complete and thorough election forecasts I’ve seen outside of professional work.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/maxtmcc&#34;&gt;Max&lt;/a&gt; : A self described mapmaker, shitposter, and ardent supporter of Long Nebraska, Max is known for his oddball posts and lukewarm political takes.&lt;/li&gt;
&lt;li&gt;Some other folks I follow for election maps/data: &lt;a href=&#34;https://twitter.com/NateSilver538&#34;&gt;Nate Silver&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/gelliottmorris&#34;&gt;G. Elliott Morris&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/umichvoter99&#34;&gt;U Mich Voter&lt;/a&gt;, and &lt;a href=&#34;https://twitter.com/norwood270&#34;&gt;Sam&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Min-Wage Debate</title>
      <link>https://www.thedatadiary.net/blog/2021-01-31-the-min-wage-debate/</link>
      <pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-01-31-the-min-wage-debate/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-01-31-the-min-wage-debate/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;On January 26, House Democrats introduced a &lt;a href=&#34;https://twitter.com/nbcnews/status/1354102510101868547?s=21&#34;&gt;bill to raise the federal minimum wage&lt;/a&gt; from $7.25 per hour to $15 per hour by 2025. In the weeks leading up to the introduction, there’s been an influx on twitter of &lt;a href=&#34;https://twitter.com/laurenboebert/status/1353102326744166409?s=21&#34;&gt;bad faith attacks&lt;/a&gt;, outright &lt;a href=&#34;https://twitter.com/JoshuaConkel/status/1350228946361622528?s=20&#34;&gt;factually incorrect statements&lt;/a&gt;, and the type of fact-free arguments that &lt;a href=&#34;https://www.washingtonpost.com/news/wonk/wp/2012/10/10/in-excitable-pundits-vs-political-scientists-ill-take-political-scientists-every-time/&#34;&gt;pundits love&lt;/a&gt; (to be fair, my twitter timeline is biased towards my left-leaning friends, so the majority of what I see are poorly formulated left-leaning takes, but it’s pretty easy to find similarly &lt;a href=&#34;https://www.prageru.com/&#34;&gt;bad right-leaning takes&lt;/a&gt;). The slew of emotion-driven arguments muddies the water around the minimum wage discussion by avoiding references to data. In an effort to find the signal in all this noise, I dug into publicly available databases to hopefully provide at least one opinion grounded in empiricism, rather than emotion.&lt;/p&gt;
&lt;div id=&#34;my-priors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;My Priors&lt;/h3&gt;
&lt;p&gt;For transparency’s sake, prior to taking a look into the data behind the minimum wage debate, I was of the belief that the minimum wage should increase and that the proposed $15 per hour seemed reasonable. I didn’t have an empirically driven reason for this belief, just a vague sense that the minimum wage hadn’t risen in a while and had therefore effectively been deflating. Anecdotally, I also hadn’t seen a good defense of keeping the minimum wage static (in fact, most of the arguments against raising the minimum wage that I’d seen were &lt;a href=&#34;https://www.forbes.com/sites/laurashin/2013/07/18/why-mcdonalds-employee-budget-has-everyone-up-in-arms/?sh=4bb3106a5216&#34;&gt;laughably bad&lt;/a&gt;). My prior was built on little data, so the analysis I set out to do would either strongly confirm or refute it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-history-of-the-minimum-wage&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The History of the Minimum Wage&lt;/h3&gt;
&lt;p&gt;In 1947, the federal government introduced a minimum wage for hourly-compensated labor as part of an amendment to the Fair Labor Standards Act (FLSA). First set at $0.40 per hour, the minimum wage has risen periodically throughout its 80 year history - most recently rising to $7.25 per hour in 2009. Currently, &lt;a href=&#34;https://www.ncsl.org/research/labor-and-employment/state-minimum-wage-chart.aspx#:~:text=Saige%20Draeger-,Summary,wage%20below%20%247.25%20per%20hour.&#34;&gt;29 states and D.C.&lt;/a&gt; have state minimum wages greater than the minimum wage, and several state minimum wages are increasing in 2021 (either due to ballot initiatives or automatic increases based on cost of living).&lt;/p&gt;
&lt;p&gt;I’ve seen a theory float around that, &lt;a href=&#34;https://twitter.com/nytopinion/status/1215889657571028992&#34;&gt;had the minimum wage risen with inflation, it would be $22 per hour&lt;/a&gt; today, rather than $7.25 per hour. This is pretty overtly false.* Using the &lt;a href=&#34;https://fred.stlouisfed.org/series/CPIAUCSL&#34;&gt;consumer price index&lt;/a&gt; (CPI) as the standard measure of inflation, the nominal minimum wage throughout the years can be adjusted to real (aka, inflation-adjusted) dollars:&lt;/p&gt;
&lt;p&gt;In inflation-adjusted terms, the minimum wage peaked around 1968 at $12.20 per hour ($1.60 per hour, unadjusted). The current minimum wage of $7.25 per hour, while on the lower end of inflation-adjusted historical values, is not the lowest it’s ever been, and a $15 per hour minimum wage would set a new record for both adjusted and unadjusted minimum wage. Critics of the proposed increase may point to this as reason to keep the minimum wage at its current value - why increase the minimum wage beyond its historical high if it hasn’t even reached its historical low? Supporters, on the other hand, may argue that the minimum wage has never been enough.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-minimum-wages-dance-between-two-thresholds&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Minimum Wage’s Dance Between Two Thresholds&lt;/h3&gt;
&lt;p&gt;In 1978, the Census Bureau and Department of Health and Human Services set a baseline administrative threshold to determine eligibility for financial assistance from the federal government. Dubbed the “&lt;a href=&#34;https://www.census.gov/data/tables/time-series/demo/income-poverty/historical-poverty-thresholds.html&#34;&gt;poverty threshold&lt;/a&gt;,” this value was created based on an approximation of the annual budget various family units require to meet basic food &amp;amp; shelter needs (for example, a single adult with no children would need to earn less per hour to meet his needs than, say, a nuclear family of two working adults and two children). The Census Bureau updates the threshold annually by simply adjusting for inflation.&lt;/p&gt;
&lt;p&gt;Many groups, however, argue that the poverty threshold doesn’t meet basic needs and instead advocate for a &lt;a href=&#34;https://www.investopedia.com/terms/l/living_wage.asp&#34;&gt;living wage&lt;/a&gt;. The living wage is similar in concept to the poverty threshold, but generally much higher, as it includes an expanded food budget (the poverty threshold is based on &lt;a href=&#34;https://www.fns.usda.gov/cnpp/usda-food-plans-cost-food-reports-monthly-reports&#34;&gt;USDA’s “thrifty” food plan&lt;/a&gt;), a budget cap on rent (generally set at ~30% of a monthly budget), healthcare, and childcare, amongst other additions.&lt;/p&gt;
&lt;p&gt;Based on &lt;a href=&#34;https://livingwage.mit.edu/&#34;&gt;MIT Lab’s Living Wage Calculator&lt;/a&gt;, we can see that, for a family of four with two working adults, the minimum wage has historically been marginally greater than the poverty threshold, but never reaches (or approaches) the living wage:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_02.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 2, above, shows the national minimum and living wages, but these values vary by location. Figure 3 below shows the minimum and living wages for each state. Even states with minimum wages greater than the federal minimum do not meet the local living wage:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_03.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-proposed-increase&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Proposed Increase&lt;/h3&gt;
&lt;p&gt;So how does the proposed increase to the minimum wage fare against the living wage and poverty threshold in the future? If inflation continues to hold at about &lt;a href=&#34;https://www.pwlcapital.com/how-pwl-estimates-future-inflation/&#34;&gt;1.6%&lt;/a&gt;, the increased minimum wage will get significantly closer to, but not greater than, the living wage in 2025.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_04.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Is this enough? Arguably, no, as the proposed increase is less than the estimated living wage. But it is certainly a step in the right direction, and if passed, would likely keep the minimum wage above the poverty threshold for quite some (the poverty threshold rises roughly at a rate of $1 per hour every 10 years).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-posterior&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;My Posterior&lt;/h3&gt;
&lt;p&gt;In general, my prior was confirmed by the analysis - my belief that the minimum wage should be increased has strengthened, and there’s a believable argument that the $15 per hour increase could be raised even higher. There are some additional points that I didn’t discuss above, however, that are still worth noting:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are many different groups that have estimated living wages, and results can vary (I used MIT Lab’s data because it was the most readily available &amp;amp; also provided state, county, and metropolitan level data). In general, however, living wage estimates agree that the poverty threshold is far too low, and typically show living wages in excess of $15 per hour.&lt;/li&gt;
&lt;li&gt;The percentage of hourly workers earning the federal minimum wage (or less) has &lt;a href=&#34;https://usafacts.org/articles/minimum-wage-america-how-many-people-are-earning-725-hour/&#34;&gt;decreased over the years&lt;/a&gt; and currently rests at about 1.9%. This is in part due to many states having minimum wages greater than the federal minimum. If the minimum wage increases to $15 per hour, the total number of workers earning the federal minimum wage will likely increase (the linked chart shows spikes that correspond to minimum wage increases in the ‘90s and 2009).&lt;/li&gt;
&lt;li&gt;Some critics of the increased minimum wage argue that the minimum wage is only meant for teenagers working summer jobs. The data doesn’t support this argument - over &lt;a href=&#34;https://www.bls.gov/opub/reports/minimum-wage/2019/home.htm#cps_mw_whe_hist.f.1&#34;&gt;80% of federal minimum wage workers are 20+ years old&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;In 2019, the Congressional Budget Office released &lt;a href=&#34;https://www.cbo.gov/system/files/2019-07/CBO-55410-MinimumWage2019.pdf&#34;&gt;a report&lt;/a&gt; that estimated that job losses due to an increased minimum wage could range between 4.7 million and 0, with a median of 1.3 million lost jobs. The report also notes, however, that about 17 million workers would benefit from the wage increase.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;The linked NYT opinion article does give itself the caveat that the $22 per hour figure is based on “inflation and productivity,” though &lt;a href=&#34;https://twitter.com/JoshuaConkel/status/1350228946361622528?s=20&#34;&gt;others&lt;/a&gt; have repeated this $20+ per hour wage argument without this caveat. The &lt;a href=&#34;https://www.nytimes.com/2020/01/09/opinion/sunday/deaths-despair-poverty.html&#34;&gt;opinion article&lt;/a&gt; doesn’t mention any sources or databases, though I expect that the author is using GDP as a proxy for inflation - several articles point out that if the minimum wage had &lt;a href=&#34;https://www.counterpunch.org/2020/01/24/what-the-minimum-wage-would-be-if-it-kept-pace-with-productivity/&#34;&gt;kept pace with GDP growth since 1968&lt;/a&gt;, it’d be significantly greater than even the proposed $15 per hour minimum wage. Correlating this to inflation, however, is at best a significant oversight by the author/editor, and at worst intentionally deceptive.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-other-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some Other Notes&lt;/h3&gt;
&lt;p&gt;As always, source data for this post can be found on &lt;a href=&#34;https://github.com/markjrieke/thedatadiary/tree/main/2021.01.31-minwage&#34;&gt;github&lt;/a&gt;. I made an overt attempt to keep the code a bit cleaner this time around (having a “test area” in the code helped out). It’s still not perfect, and I suspect I’ll just naturally get better at writing neatly formatted scripts as the language becomes more intuitive.&lt;/p&gt;
&lt;p&gt;I think I’m going to take a break for a couple weeks from some of these longer posts - I’m currently finishing this a bit after midnight (which is very late for me!) - and between writing this &amp;amp; the Bayes’ Theorem post, I’ve had little time to dedicate to stats. Next week, I’ll put together a less intensive piece to give myself time to push forward with the stats class. Maybe after that I can write a quick post about the accuracy of prediction models, then get into another deep dive. We’ll see how I feel about it in the next couple weeks.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Your Doctor Probably Isn&#39;t an Idiot</title>
      <link>https://www.thedatadiary.net/blog/2021-01-24-your-doctor-probably-isn-t-an-idiot/</link>
      <pubDate>Sun, 24 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-01-24-your-doctor-probably-isn-t-an-idiot/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-01-24-your-doctor-probably-isn-t-an-idiot/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;There’s a great standup routine about doctors (which I spent about an hour trying to find online with no luck!) with a punchline to the effect of, “getting a positive cancer test then requesting a second opinion is basically the medical equivalent of telling your doctor you think they’re an idiot to their face.” The joke makes sense in context (and is a lot funnier to hear, rather than read), and is logical at a general level (why would a patient, who probably isn’t a medical expert, be able to say that a doctor’s assessment is wrong?), but there’s a hidden nugget of nuance about cancer screenings missed by the joke that make them such an interesting introduction to one of the foundational equations in statistics, &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes%27_theorem&#34;&gt;Bayes’ Theorem&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;what-is-bayes-theorem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What is Bayes’ Theorem?&lt;/h3&gt;
&lt;p&gt;To understand Bayes’ Theorem, it’s probably best to get a grasp on frequency-based probability, which is often considered the contrast of bayesian/uncertainty-based probability. Consider a perfectly ideal coin flip. I know that, for any given coin flip, the probabilities of landing either heads-up or tails-up are exactly the same: 50%. Moreover, this is true no matter how many times I flip the coin. If the coin lands heads-up on the first flip, I still have a 50% chance of the coin landing heads-up on the next flip (and the next flip, and the next flip, etc.).&lt;/p&gt;
&lt;p&gt;But what if I’m told the coin has a manufacturing defect, and is slightly heavier on one side (lets say, to favor tails-up)? I might assume, starting out, that the defect is small, and has a negligible effect on my odds of the coin landing heads-up. But every flip that lands tails-up changes my certainty about the true odds. That’s the heart of Bayes’ Theorem, and the biggest difference between bayesian statistics and frequentist statistics. With frequentist statistics, when the data updates, the uncertainty stays the same, whereas with bayesian statistics, when the data updates, the uncertainty updates as well.&lt;/p&gt;
&lt;p&gt;So why is this important? Well, at a base level, just about every forecasting model needs to make use of Bayes’ Theorem. So with the long term goal of building a predictive election model, it’s pretty dang important to have a firm understanding of the foundation the model will rest on. But in a more general sense, I think it’s important to understand how views can (and should) update in the light of new data. From what I can tell, the majority of opinions (especially political opinions…) are either based on assumptions or &lt;a href=&#34;https://fivethirtyeight.com/features/the-art-of-cherry-picking-polls/&#34;&gt;cherry-picked data points&lt;/a&gt;, and typically don’t update - even when presented with new evidence. In contrast, to think like a bayesian is to weigh the severity of new information against the history of data and update your views accordingly.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ugly-vs.-pretty-bayes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ugly vs. Pretty Bayes&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;pics/pic_01.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Equation 1 above shows Bayes’ Theory. As expressed in statistical language, it reads, “the probability of A, given event B, is equal to the probability of B, given event A, and the probability of A divided by the probability of B.” As written, Bayes’ Theorem is… unintuitive… Even reading the description back to myself stirs up memories of fumbling through my high school stats class. The way the equation is written doesn’t lend itself towards being accessible, but, &lt;a href=&#34;https://www.3blue1brown.com/videos-blog/bayes-theorem-and-making-probability-intuitive&#34;&gt;as Grant Sanderson points out in this video&lt;/a&gt;, reframing the equation into a geometry problem makes Bayes’ Theorem much easier to understand (much of the rest of this post is applying Bayes’ Theorem via Grant’s geometrical framing).&lt;/p&gt;
&lt;p&gt;To understand how a geometrical framing can help make sense of Bayes’ Theorem, it’s best to run through an example. So let’s get back to the doctor’s office &amp;amp; cancer screening.&lt;/p&gt;
&lt;p&gt;Let’s first consider a population group of 10,000, arranged on a grid of 100 x 100. If we know that 1% of the population has cancer, we can divide the group into two segments, cancer-free and cancerous, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/pic_02.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we avoid any compounding risk factors (like age, sex, diet, etc.) &amp;amp; assume that the likelihood of having cancer is the same for everyone, any person in the population can say that they are 99% certain that they do not have cancer. But how might this certainty update for someone who gets a positive result from a cancer screening test?&lt;/p&gt;
&lt;p&gt;The reliability of cancer screening tests are measured by their sensitivity &amp;amp; specificity. A test’s sensitivity is the the proportion of cancer-positive patients it correctly identifies (i.e., “true positives”). For example, if a screening test has a sensitivity of 90% and is used to test 100 cancerous patients, we’d expect 90 of these patients to receive a correct positive result and the remaining 10 patients to receive an incorrect negative result. The specificity, on the other hand, is the proportion of cancer-free patients the test correctly identifies. If the screening test’s specificity is 92% and is used to test 100 cancer-free patients, we’d expect 92 patients to correctly receive a negative result and 8 patients to incorrectly receive a positive result (a “false positive”). I tend to find it easier just to think about the positive cases, so from now on, I’ll refer to a test’s true positive rate and false positive rate, rather than the sensitivity &amp;amp; specificity.&lt;/p&gt;
&lt;p&gt;Let’s assume that a cancer screening test has the true positive &amp;amp; false positive rates above, 90% &amp;amp; 8%, respectively. How certain should someone who tests positive be that they truly have cancer? A test that gives a true positive 90% of the time seems pretty damning, but the 8% false positive rate isn’t insignificant. An important step is to realize that this person is no longer looking for the probability of having cancer, but instead looking for the probability of having cancer given that they have a positive test result.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/pic_03.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For someone who receives a positive test result, it may be helpful to think about the hypothetical question: “What if everyone took this test?” This reframing can help us think in terms of people, rather than probabilities.&lt;/p&gt;
&lt;p&gt;Given that 1% of the 10,000 person population has cancer, we can divide the group into two segments: 100 people with cancer, and 9,900 people without cancer.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/pic_04.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If everyone takes the screening test, we’d expect 90 true positives (90% x 100 = 90) and a whopping 792 false positives (8% x 9,900 = 792)!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/pic_05.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although the test is fairly accurate in terms of sensitivity &amp;amp; specificity (90% and 92% are both considered A’s, by most grading standards) the sheer number of non-cancerous people in the population results in a large number of false positives. Of all the 882 positive cases (792 + 90 = 882), only 90 are true positives, meaning that the probability of having cancer given a positive test result is about 10% (90 / 882 = 10.2%). While this is a significant increase from the prior assumption, 1%, it’s still far likelier that someone with one positive test result doesn’t have cancer. This is why it’s important to get a second opinion. It’s not that your first doctor is an idiot, just that your uncertainty has changed!&lt;/p&gt;
&lt;p&gt;Now what would happen if all the people who received a positive result took another test? Well, of this subset of 882 patients, we expect that 792 are cancer free and 90 are cancerous:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/pic_06.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When these 882 patients take the second test, we still expect the test to hold the same true positive &amp;amp; false positive rates - meaning we can expect 81 true positives (90% x 90 = 81) and about 63 false positives (8% x 792 = 63). This means that, of the 144 positive cases (81 + 63 = 144), 81 are true positives, and the probability of having cancer given two positive results is about 56% (81 / 144 = 56%).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/pic_07.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With this new round of data from test results, those who receive a second positive result once again update their prior assumption from 10% to 56%. In fact, every test result, positive or negative, should either support or refute the prior. Each positive test result, appropriately, increases your likelihood of actually having cancer. Similarly, each negative result decreases your likelihood.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/pic_08.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s the beauty of Bayes’ Theorem in a nutshell. Gathering new information allows you to update your prior belief!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;in-summary&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;In Summary…&lt;/h3&gt;
&lt;p&gt;I may have gotten too in the weeds with this post, though, to be fair, walking through Bayes’ Theorem in detail also helps my understanding. I likely wont dive as deep into stats topics in the future. In part, they’re a bit of a slog to read through if you’re not as excited as I am by this kind of stuff. But it also took me a good chunk of time to write this post. Going forward, I’d like to better balance my time between the three goals of coding, writing, and learning stats.&lt;/p&gt;
&lt;p&gt;As always, I’ve posted my work to &lt;a href=&#34;https://github.com/markjrieke/thedatadiary/tree/main/2021.01.24-bayes_theorem&#34;&gt;github&lt;/a&gt;, though this file just includes an excel workbook &amp;amp; a few pictures. Next week, I’ll dig more into plotting with ggplot - likely with a much shorter post!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R, ggplot2, &amp; plotly</title>
      <link>https://www.thedatadiary.net/blog/2021-01-17-r-ggplot2-plotly/</link>
      <pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-01-17-r-ggplot2-plotly/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-01-17-r-ggplot2-plotly/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Editor’s note: I’ve gone back &amp;amp; read my previous post, &amp;amp; have one general thought - yeeeesh… Everything is so… “matter of fact” in tone &amp;amp; doesn’t really sound like me. I’ll give myself a pass, since it was the first post, was written around midnight (which is very late for me!), and probably won’t ever be read by anyone besides me/my mom (hi mom!). That being said, I’m going to try to make a more conscious effort going forward of having my voice be expressed in my writing. This is my blog, after all, so it should sound like my voice when read back.&lt;/p&gt;
&lt;div id=&#34;plotting-in-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plotting in R&lt;/h3&gt;
&lt;p&gt;I’ve started getting more familiar with R &amp;amp; wrangling my way through a few plotting examples (Youtube university is, once again, my best friend), but I thought it would be worthwhile to work towards plotting my own dataset. Luckily enough, plotting with R is pretty intuitive, once you get ahold of the basic syntax. I had put together a &lt;a href=&#34;https://github.com/markjrieke/thedatadiary/tree/main/2021.01.17-r_ggplot2_plotly&#34;&gt;dataset&lt;/a&gt; on past US presidential elections for a separate personal project, and was able to convert to a .csv to use for some basic plotting practice.&lt;/p&gt;
&lt;p&gt;It’s generally well understood that the Republican party has a &lt;a href=&#34;https://www.vox.com/policy-and-politics/2019/9/17/20868790/republicans-lose-popular-vote-win-electoral-college&#34;&gt;structural advantage in presidential elections&lt;/a&gt; due to the winner-take-all nature of the electoral college, but I wanted to see if I could quantify this advantage in an understandable format using R. I used the ggplot2, dplyr, and plotly packages to put this together.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-together-the-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Putting Together the Plot&lt;/h3&gt;
&lt;p&gt;I imported the entire dataset and assigned it to a dataframe (R’s version of, say, an excel table). Using the base R plot() function, I plotted the entire dataframe:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_01.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Without specifying which variables I want to look at, plot() will output a set of summary plots, with every variable plotted against every other variable. This isn’t great for gathering any meaningful insight, but helps get a “lay of the land” view of the dataframe. In this case, I’m interested in how the percentage of the popular vote a candidate wins is related to the percentage of the electoral vote they win. Using plot(), I can graph popular_vote_pct and electoral_vote_pct variables from the dataframe:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_02.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 2 shows the basic data I want to represent, but there’s a lot of noise. The dataset includes every major candidate in every election since &lt;a href=&#34;https://en.wikipedia.org/wiki/1788%E2%80%9389_United_States_presidential_election&#34;&gt;Washington’s run for re-election in 1788&lt;/a&gt;. I’m really only interested in the modern two-party system, so I filtered out the elections prior to 1952, as well as any candidate that won 0 electoral votes (i.e., third parties). The new, filtered plot is shown in Figure 3, below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_03.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, with all the data manipulation squared away, I can start with the fun part: making it look good! ggplot’s base plot is, right off the bat, just a bit nicer looking than base R’s plot() function:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_04.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;ggplot color maps very easily, and with a little googling (aka - jumping through multiple color converters online to convert colors from hexadecimal to rgb to hcl), I was able to make some formatting changes that give the below static chart:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_05.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Despite some frustrations (more on that, below), I was able to add a simple linear regression to the plot, with the shaded bands representing the 95% confidence interval for each regression. Since the Republican regression is above the Democratic regression, the chart implies that, for a given share of popular vote, Republican candidates on average win a larger share of the electoral college than their Democratic counterparts.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_06.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This finding matches the general consensus and my prior expectations, but I also found something mildly surprising. Ideally, I would imagine, we’d want a candidate’s share of the electoral vote to match their share of the popular vote. However, if I add a reference line of y = x, we can see that both parties tend to underperform in the electoral college when they lose the popular vote, but overperform in the electoral college when they win the popular vote!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_07.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I plotted the absolute vote share from published election results, rather than the two-party vote share (2PV) (i.e., the percentage of vote won if only the top two candidates are considered). Converting to the 2PV would shift the plot to the right, but keep the same overall trend, as Democrats and Republicans have won 100% of the electoral college since 1952. This exercise was more about exploring R’s ggplot() function, so going back into the dataset to account for this marginal change doesn’t seem like an efficient use of time, but I did want to make a note of it, for transparency’s sake.&lt;/p&gt;
&lt;p&gt;Finally, I used the ggplotly() to convert from a static ggplot to an interactive plotly, and exported as an html file. For reasons I’ve yet to figure out, the custom textbox formatting did not want to play nice with the linear regression lines, so I had to take those out. I plan on figuring this wrinkle out eventually, but for now I’m happy with the final plot. You can hover over individual datapoints below to see more details!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is an archive of a post previously hosted on Squarespace. You can view the original interactive content &lt;a href=&#34;https://plotly.com/~markjrieke/1/&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some Notes&lt;/h3&gt;
&lt;p&gt;I’ve published all of my work to &lt;a href=&#34;https://github.com/markjrieke/thedatadiary&#34;&gt;github&lt;/a&gt;, and I’ll continue to do so in the future. The files I uploaded are, to say the least, in pretty rough shape. That being said, I don’t really plan on tweaking them for this post - I’ve used them as training tools, and it might be interesting in the future to look at the progress I’ve made.&lt;/p&gt;
&lt;p&gt;Also, I just realized that the interactive plotly chart looks like junk when viewed via mobile (the data points and labels don’t resize automatically). I’ll have to figure out a way to fix this in the future… Maybe I can set the point size as a ratio of the plot width? It’s something to think about, but again, I’m happy with my first run using ggplot &amp;amp; plotly.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Baby Steps</title>
      <link>https://www.thedatadiary.net/blog/2021-01-10-baby-steps/</link>
      <pubDate>Sun, 10 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-01-10-baby-steps/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-01-10-baby-steps/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;an-uphill-battle&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;An Uphill Battle&lt;/h3&gt;
&lt;p&gt;Today marks the beginning of what I imagine will be a long journey, with a significant number of hiccups &amp;amp; frustrations along the way. The end goal I have is to build out an election forecast model (in the spirit of &lt;a href=&#34;https://projects.fivethirtyeight.com/2020-election-forecast/&#34;&gt;FiveThirtyEight&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://projects.economist.com/us-2020-forecast/president&#34;&gt;The Economist&lt;/a&gt;), but along the way, I plan on getting better at a few things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Writing&lt;/strong&gt; : while I’ve never been a poor writer, I’ve always gravitated more toward math &amp;amp; science, &amp;amp; in my career as an engineer, a good amount of my communication has been visual. If possible, I’ve always preferred to leave words off the page &amp;amp; have a chart speak for me. While I still think that, in general, its better the explain a concept visually (a picture vs. a thousand words, and all), I also believe supplementing a visual with my a well written analysis &amp;amp; opinion can improve its reception.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stats&lt;/strong&gt; : in my current job, almost everything I model is deterministic, rather than probabilistic (physical systems tend to behave the way that natural laws expect them to). It has been a long time since I’ve done any sort of stats work (&amp;amp; it was all basic introductory analyses), so I expect an uphill battle with getting my head wrapped around Bayes’ Theorem, Monte Carlo simulations, Brier’s Tests, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Coding&lt;/strong&gt; : most of my experience in coding is with VBA - typically just for manipulating data in Excel. I’ve built out a good number of forms using VBA, but don’t typically tend to dig into anything far beyond simple manipulation. The front end of Excel is effectively setup as a visual programming tool, so there’s not a great incentive to do much array manipulation outside of the spreadsheet. Excel is a wonderful tool, &amp;amp; is sufficient for my current job, but I’ve definitely found where its limits lie. I’m excited to dig into R, the stats based program that seems almost to be designed specifically for what I’d like to do.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-first-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The First Plot&lt;/h3&gt;
&lt;p&gt;R has a few sample datasets, and r:base includes a pretty basic plot function. With the sample set, cars, I made a quick plot of the stopping distance vs. the speed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(cars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-01-10-baby-steps/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hello World!</title>
      <link>https://www.thedatadiary.net/blog/2021-01-09-hello-world/</link>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-01-09-hello-world/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-01-09-hello-world/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I’m currently setting up the website &amp;amp; will have an official post soon(ish).&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
