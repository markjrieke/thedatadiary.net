<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tidymodels on the data diary</title>
    <link>https://www.thedatadiary.net/categories/tidymodels/</link>
    <description>Recent content in tidymodels on the data diary</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Mark Rieke {year}</copyright>
    <lastBuildDate>Tue, 08 Feb 2022 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://www.thedatadiary.net/categories/tidymodels/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Pull Yourself Up by Your Bootstraps</title>
      <link>https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/</link>
      <pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Statistical modeling sometimes presents conflicting goals. Oftentimes, building a model involves a mix of objectives that don’t necessarily mesh well together: super-accurate point predictions, explainability, fast performance, or an expression of confidence in the prediction. In my work as an analyst, I generally am focused on how explainable the model is while being able to express a confidence interval around each prediction. For that, simple linear models do the trick. If, however, I want to regularize via &lt;code&gt;{glmnet}&lt;/code&gt; (which — with good reason — &lt;a href=&#34;https://stats.stackexchange.com/questions/224796/why-are-confidence-intervals-and-p-values-not-reported-as-default-for-penalized&#34;&gt;doesn’t provide confidence intervals&lt;/a&gt;) or use a non-linear model like &lt;code&gt;{xgboost}&lt;/code&gt;, I have to drop the confidence interval around predictions. Or so I had previously thought! As it turns out, building a series of models from bootstrap resamples provides an alternative method of generating a confidence interval around a prediction.&lt;/p&gt;
&lt;div id=&#34;setting-a-baseline-with-penguins&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting a baseline with penguins&lt;/h3&gt;
&lt;p&gt;First, let’s build out a baseline linear model with the Palmer Penguins dataset. This dataset contains information on 344 penguins across three species types and three islands. For this example, we’ll use the penguin information to predict &lt;code&gt;body_mass_g&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the data in from the tidytuesdayR package
penguins_src &amp;lt;- tidytuesdayR::tt_load(2020, week = 31)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Downloading file 1 of 2: `penguins.csv`
##  Downloading file 2 of 2: `penguins_raw.csv`&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract out the penguins dataset
penguins &amp;lt;- penguins_src$penguins
rm(penguins_src)

penguins&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 344 x 8
##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
##  1 Adelie  Torgersen           39.1          18.7               181        3750
##  2 Adelie  Torgersen           39.5          17.4               186        3800
##  3 Adelie  Torgersen           40.3          18                 195        3250
##  4 Adelie  Torgersen           NA            NA                  NA          NA
##  5 Adelie  Torgersen           36.7          19.3               193        3450
##  6 Adelie  Torgersen           39.3          20.6               190        3650
##  7 Adelie  Torgersen           38.9          17.8               181        3625
##  8 Adelie  Torgersen           39.2          19.6               195        4675
##  9 Adelie  Torgersen           34.1          18.1               193        3475
## 10 Adelie  Torgersen           42            20.2               190        4250
## # ... with 334 more rows, and 2 more variables: sex &amp;lt;chr&amp;gt;, year &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll need to do some lite preprocessing before we start modeling — it looks like there are some &lt;code&gt;NAs&lt;/code&gt; in &lt;code&gt;body_mass_g&lt;/code&gt; and in &lt;code&gt;sex&lt;/code&gt;. If I were creating a more serious model, I might keep the rows with &lt;code&gt;NAs&lt;/code&gt; for &lt;code&gt;sex&lt;/code&gt;, but since there are so few and this is an explainer, I’ll just filter them out.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# remove NA from body_mass_g and sex
penguins &amp;lt;- 
  penguins %&amp;gt;%
  filter(!is.na(body_mass_g),
         !is.na(sex))

penguins&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 333 x 8
##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
##  1 Adelie  Torgersen           39.1          18.7               181        3750
##  2 Adelie  Torgersen           39.5          17.4               186        3800
##  3 Adelie  Torgersen           40.3          18                 195        3250
##  4 Adelie  Torgersen           36.7          19.3               193        3450
##  5 Adelie  Torgersen           39.3          20.6               190        3650
##  6 Adelie  Torgersen           38.9          17.8               181        3625
##  7 Adelie  Torgersen           39.2          19.6               195        4675
##  8 Adelie  Torgersen           41.1          17.6               182        3200
##  9 Adelie  Torgersen           38.6          21.2               191        3800
## 10 Adelie  Torgersen           34.6          21.1               198        4400
## # ... with 323 more rows, and 2 more variables: sex &amp;lt;chr&amp;gt;, year &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s always good practice to explore the dataset prior to fitting a model, so let’s jump into some good ol’ fashioned EDA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how are species/island related to body mass?
penguins %&amp;gt;%
  ggplot(aes(x = species,
             y = body_mass_g,
             color = species)) +
  geom_boxplot() +
  geom_point(alpha = 0.25,
             position = position_jitter()) +
  facet_wrap(~island)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Interesting! It looks like the Gentoo and Chinstrap species are only found on the Biscoe and Dream islands, respectively, whereas the Adelie species can be found on all three islands. At first glance, there’s not a meaningful difference that Island has on the weight of the Adelie penguins, so I think we’re safe to toss out the &lt;code&gt;island&lt;/code&gt; feature and just keep &lt;code&gt;species&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how does sex relate to body mass?
penguins %&amp;gt;%
  ggplot(aes(x = sex,
             y = body_mass_g,
             color = sex)) +
  geom_boxplot() +
  geom_point(alpha = 0.25,
             position = position_jitter())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unsurprisingly, male penguins are typically heavier than female penguins.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# are penguins getting heavier or lighter as years progress?
penguins %&amp;gt;%
  mutate(year = as.character(year)) %&amp;gt;%
  ggplot(aes(x = year,
             y = body_mass_g)) +
  geom_boxplot() +
  geom_point(alpha = 0.25,
             position = position_jitter())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It doesn’t look like there is significant signal being drawn from &lt;code&gt;year&lt;/code&gt;, so we’ll toss that out as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how do other body measurements compare with the total body mass?
penguins %&amp;gt;%
  select(bill_length_mm:body_mass_g) %&amp;gt;%
  pivot_longer(ends_with(&amp;quot;mm&amp;quot;),
               names_to = &amp;quot;measurement&amp;quot;,
               values_to = &amp;quot;value&amp;quot;) %&amp;gt;%
  ggplot(aes(x = value,
             y = body_mass_g,
             color = measurement)) +
  geom_point(alpha = 0.5) + 
  facet_wrap(~measurement, scales = &amp;quot;free_x&amp;quot;) +
  geom_smooth(method = &amp;quot;lm&amp;quot;,
              se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For bill and flipper length, there’s a pretty clear relationship, but it looks like bill depth has a &lt;em&gt;classic&lt;/em&gt; case of &lt;a href=&#34;https://en.wikipedia.org/wiki/Simpson%27s_paradox&#34;&gt;Simpson’s paradox&lt;/a&gt;. Let’s explore that further to find a meaningful interaction to apply.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# which feature interacts with bill depth to produce simpson&amp;#39;s pardox?
penguins %&amp;gt;%
  ggplot(aes(x = bill_depth_mm,
             y = body_mass_g,
             color = species)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = &amp;quot;lm&amp;quot;,
              se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So, very clearly, the Gentoo species has a very different relationship between bill depth and body mass than the Adelie/Chinstrap species. We’ll add this as an interactive feature to the model.&lt;/p&gt;
&lt;p&gt;With all that completed, let’s (finally) setup and build the baseline linear model with confidence intervals around the prediction!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# remove features
penguins &amp;lt;- 
  penguins %&amp;gt;%
  select(-island, -year)

# split into testing and training datasets
set.seed(123)
penguins_split &amp;lt;- initial_split(penguins)
penguins_test &amp;lt;- testing(penguins_split)
penguins_train &amp;lt;- training(penguins_split)

# setup a pre-processing recipe
penguins_rec &amp;lt;- 
  recipe(body_mass_g ~ ., data = penguins_train) %&amp;gt;%
  step_dummy(all_nominal()) %&amp;gt;% 
  step_interact(~starts_with(&amp;quot;species&amp;quot;):bill_depth_mm)

# fit a workflow
penguins_lm &amp;lt;- 
  workflow() %&amp;gt;%
  add_recipe(penguins_rec) %&amp;gt;%
  add_model(linear_reg() %&amp;gt;% set_engine(&amp;quot;lm&amp;quot;)) %&amp;gt;%
  fit(penguins_train)

# predict on training data with confidence intervals
bind_cols(penguins_lm %&amp;gt;% predict(penguins_train),
          penguins_lm %&amp;gt;% predict(penguins_train, type = &amp;quot;conf_int&amp;quot;, level = 0.95),
          penguins_train) %&amp;gt;%
  ggplot(aes(x = body_mass_g,
             y = .pred)) +
  geom_point(alpha = 0.5) +
  geom_segment(aes(x = body_mass_g,
                   xend = body_mass_g,
                   y = .pred_lower,
                   yend = .pred_upper),
               alpha = 0.25) +
  labs(title = &amp;quot;Predicting the Palmer Penguins - Training&amp;quot;,
       subtitle = &amp;quot;Linear model predicting a penguin&amp;#39;s weight in grams&amp;quot;,
       x = &amp;quot;Actual weight (g)&amp;quot;,
       y = &amp;quot;Predicted weight (g)&amp;quot;,
       caption = &amp;quot;Errorbars represent the a 95% confidence interval&amp;quot;) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This model does generally okay, but the confidence interval around each prediction is pretty &lt;a href=&#34;https://mc-stan.org/rstanarm/articles/rstanarm.html&#34;&gt;clearly too confident&lt;/a&gt;! Let’s solve this with bootstrapping.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;whats-a-bootstrap&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What’s a bootstrap?&lt;/h3&gt;
&lt;p&gt;Before progressing any further, it’s probably important to define what exactly a bootstrap is/what bootstrapping is. Bootstrapping is a resampling method that lets us take one dataset and turn it into many datasets. Bootstrapping accomplishes this by repeatedly pulling a random row from the source dataset and, importantly, bootstrapping allows for rows to be repeated! Let’s look at an example for a bit more clarity.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;rowid&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;104&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;102&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;79&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let’s say we want to make bootstrap resamples of this dataset. We’ll draw five random rows from the dataset and, sometimes, we’ll have the same row show up in our new bootstrapped dataset multiple times:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;rowid&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;104&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;102&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Another bootstrap dataset might look like this:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;rowid&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;102&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;79&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;79&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Bootstrap datasets allow us to create many datasets from the original dataset and evaluate models across these bootstraps. Models that are well informed will give similar outputs across each dataset, despite of the randomness within each dataset, whereas less confident models will have a wider variation across the bootstrapped datasets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generating-some-confident-penguins&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Generating some confident penguins&lt;/h3&gt;
&lt;p&gt;Let’s say we want to use &lt;code&gt;{xgboost}&lt;/code&gt; to predict penguin weight and we’ll use bootstrapping to generate a confidence interval. Firstly, we’ll create the bootstrap datasets from our training set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;penguins_boot &amp;lt;- penguins_train %&amp;gt;% bootstraps()

penguins_boot&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # Bootstrap sampling 
## # A tibble: 25 x 2
##    splits           id         
##    &amp;lt;list&amp;gt;           &amp;lt;chr&amp;gt;      
##  1 &amp;lt;split [249/92]&amp;gt; Bootstrap01
##  2 &amp;lt;split [249/90]&amp;gt; Bootstrap02
##  3 &amp;lt;split [249/91]&amp;gt; Bootstrap03
##  4 &amp;lt;split [249/87]&amp;gt; Bootstrap04
##  5 &amp;lt;split [249/98]&amp;gt; Bootstrap05
##  6 &amp;lt;split [249/84]&amp;gt; Bootstrap06
##  7 &amp;lt;split [249/91]&amp;gt; Bootstrap07
##  8 &amp;lt;split [249/95]&amp;gt; Bootstrap08
##  9 &amp;lt;split [249/94]&amp;gt; Bootstrap09
## 10 &amp;lt;split [249/86]&amp;gt; Bootstrap10
## # ... with 15 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, the &lt;code&gt;bootstraps()&lt;/code&gt; function will create 25 bootstrap datasets, but we could theoretically create as many as we want. Now that we have our bootstraps, let’s create a function that will fit a model to each of the bootstraps and save to disk. We’ll use the default parameters for our &lt;code&gt;{xgboost}&lt;/code&gt; model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define a basic xgboost model
penguins_xgb &amp;lt;-
  boost_tree() %&amp;gt;%
  set_mode(&amp;quot;regression&amp;quot;) %&amp;gt;%
  set_engine(&amp;quot;xgboost&amp;quot;)

# function that will fit a model and save to a folder
fit_bootstrap &amp;lt;- function(index) {
  
  # pull out individual bootstrap to fit
  xgb_boot &amp;lt;- penguins_boot$splits[[index]] %&amp;gt;% training()
  
  # fit to a workflow
  workflow() %&amp;gt;%
    add_recipe(penguins_rec) %&amp;gt;%
    add_model(penguins_xgb) %&amp;gt;%
    fit(xgb_boot) %&amp;gt;%
    write_rds(paste0(&amp;quot;models/model_&amp;quot;, index, &amp;quot;.rds&amp;quot;))
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function will create a new model for each bootstrap, so we’ll end up with 25 separate models. Let’s fit!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fit to 25 bootstrapped datasets
for (i in 1:25) {
  
  fit_bootstrap(i)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s define a function that will predict based on these 25 bootstrapped models, then predict on our training data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_bootstrap &amp;lt;- function(new_data, index){
  
  read_rds(paste0(&amp;quot;models/model_&amp;quot;, index, &amp;quot;.rds&amp;quot;)) %&amp;gt;%
    predict(new_data) %&amp;gt;%
    rename(!!sym(paste0(&amp;quot;pred_&amp;quot;, index)) := .pred)
  
}

# predict!
training_preds &amp;lt;- 
  seq(1, 25) %&amp;gt;%
  map_dfc(~predict_bootstrap(penguins_train, .x))

training_preds&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 249 x 25
##    pred_1 pred_2 pred_3 pred_4 pred_5 pred_6 pred_7 pred_8 pred_9 pred_10
##     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1  5552.  5638.  5555.  5703.  5726.  5783.  5404.  5566.  5493.   5547.
##  2  3470.  3340.  3334.  3350.  3311.  3303.  3315.  3421.  3692.   3436.
##  3  5309.  5274.  5241.  5286.  5206.  5084.  5506.  5531.  5274.   5309.
##  4  4160.  4013.  3988.  4111.  4075.  4073.  4284.  4050.  4033.   4033.
##  5  4003.  3931.  4096.  3968.  4008.  3918.  3941.  4093.  3941.   3880.
##  6  3967.  4039.  4095.  4047.  4021.  4055.  3980.  4115.  4067.   4084.
##  7  4647.  4551.  4750.  4555.  4690.  4396.  4235.  4686.  4764.   4659.
##  8  5240.  5288.  5291.  5276.  5308.  5508.  5570.  5375.  5340.   5268.
##  9  4138.  4111.  4106.  4236.  4135.  4219.  4218.  4211.  4160.   4071.
## 10  4728.  4723.  4715.  4823.  4765.  4727.  4836.  4777.  4765.   4633.
## # ... with 239 more rows, and 15 more variables: pred_11 &amp;lt;dbl&amp;gt;, pred_12 &amp;lt;dbl&amp;gt;,
## #   pred_13 &amp;lt;dbl&amp;gt;, pred_14 &amp;lt;dbl&amp;gt;, pred_15 &amp;lt;dbl&amp;gt;, pred_16 &amp;lt;dbl&amp;gt;, pred_17 &amp;lt;dbl&amp;gt;,
## #   pred_18 &amp;lt;dbl&amp;gt;, pred_19 &amp;lt;dbl&amp;gt;, pred_20 &amp;lt;dbl&amp;gt;, pred_21 &amp;lt;dbl&amp;gt;, pred_22 &amp;lt;dbl&amp;gt;,
## #   pred_23 &amp;lt;dbl&amp;gt;, pred_24 &amp;lt;dbl&amp;gt;, pred_25 &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a column of predictions for each model — we can summarise our point prediction for each row with the average across all models and set the confidence interval based on the standard deviation of the predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_preds %&amp;gt;%
  bind_cols(penguins_train) %&amp;gt;%
  rowid_to_column() %&amp;gt;%
  pivot_longer(starts_with(&amp;quot;pred_&amp;quot;),
               names_to = &amp;quot;model&amp;quot;,
               values_to = &amp;quot;.pred&amp;quot;) %&amp;gt;%
  group_by(rowid) %&amp;gt;%
  summarise(body_mass_g = max(body_mass_g),
            .pred_mean = mean(.pred),
            std_dev = sd(.pred)) %&amp;gt;%
  riekelib::normal_interval(.pred_mean, std_dev) %&amp;gt;%
  ggplot(aes(x = body_mass_g,
             y = .pred_mean)) +
  geom_point(alpha = 0.5) +
  geom_segment(aes(x = body_mass_g, 
                   xend = body_mass_g,
                   y = ci_lower,
                   yend = ci_upper),
               alpha = 0.25) +
  labs(title = &amp;quot;Predicting the Palmer Penguins - Training&amp;quot;,
       subtitle = &amp;quot;XGBoost model predicting a penguin&amp;#39;s weight in grams&amp;quot;,
       x = &amp;quot;Actual weight (g)&amp;quot;,
       y = &amp;quot;Predicted weight (g)&amp;quot;,
       caption = &amp;quot;Errorbars represent the a 95% confidence interval&amp;quot;) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And just like that, we’ve trained a series of models with &lt;code&gt;{xgboost}&lt;/code&gt; that let us apply a confidence interval around a point prediction! Now that we’ve done so on the training set, let’s look at performance on the test set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(1, 25) %&amp;gt;%
  map_dfc(~predict_bootstrap(penguins_test, .x)) %&amp;gt;%
  bind_cols(penguins_test) %&amp;gt;%
  rowid_to_column() %&amp;gt;%
  pivot_longer(starts_with(&amp;quot;pred_&amp;quot;),
               names_to = &amp;quot;model&amp;quot;,
               values_to = &amp;quot;.pred&amp;quot;) %&amp;gt;%
  group_by(rowid) %&amp;gt;%
  summarise(body_mass_g = max(body_mass_g),
            .pred_mean = mean(.pred),
            std_dev = sd(.pred)) %&amp;gt;%
  riekelib::normal_interval(.pred_mean, std_dev) %&amp;gt;%
  ggplot(aes(x = body_mass_g,
             y = .pred_mean)) +
  geom_point(alpha = 0.5) +
  geom_segment(aes(x = body_mass_g, 
                   xend = body_mass_g,
                   y = ci_lower,
                   yend = ci_upper),
               alpha = 0.25) +
  labs(title = &amp;quot;Predicting the Palmer Penguins - Testing&amp;quot;,
       subtitle = &amp;quot;XGBoost model predicting a penguin&amp;#39;s weight in grams&amp;quot;,
       x = &amp;quot;Actual weight (g)&amp;quot;,
       y = &amp;quot;Predicted weight (g)&amp;quot;,
       caption = &amp;quot;Errorbars represent the a 95% confidence interval&amp;quot;) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The performance on the test data is slightly less accurate than on the training data, but that is to be expected. Importantly, we’ve used bootstrap resampling to generate a confidence interval from a model that otherwise normally returns a simple point prediction.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-noteworthy-caveats&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some noteworthy caveats&lt;/h3&gt;
&lt;p&gt;The prediction interval above is all well and good, but it comes with some &lt;em&gt;hefty&lt;/em&gt; caveats. Firstly, the confidence interval in the Testing plot is generated from the mean and standard deviation from each prediction. This assumes that the predictions are distributed normally, which may not necessarily be the case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_preds %&amp;gt;%
  slice_head(n = 1) %&amp;gt;%
  pivot_longer(starts_with(&amp;quot;pred&amp;quot;)) %&amp;gt;%
  ggplot(aes(x = value)) +
  geom_density()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This density plot for one of the predictions shows that there’s definitely some non-normal behavior! There’s a few ways of addressing this.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create many, many, more bootstraps and models so that the prediction distribution approaches normality (with only 25 points, we really shouldn’t even expect normality from this example).&lt;/li&gt;
&lt;li&gt;Report out the actual values of the percentiles in the distribution (e.g., the 2.5% percentile is below X, 97.5% is above Y, and the mean is at Z).&lt;/li&gt;
&lt;li&gt;Report out the actual distribution as the result.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Ideally, you should do all three.&lt;/p&gt;
&lt;p&gt;The second major caveat is that this is not one model, but a whole host of models and these take up a large amount of disk space. In this example, our 25 models take up 25 times more space than our original model and it takes some time to read in, fit, and wrangle the results. We can trade disk space for computation time by writing a function that fits and predicts without saving a model, but again, that’s a tradeoff between speed and space. For linear models, it may be a better route to have STAN simulate thousands of results via &lt;code&gt;{rstanarm}&lt;/code&gt; or &lt;code&gt;{brms}&lt;/code&gt;, but for non-linear models, boostrapping is the best way to go for now!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polling-bites&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Polling Bites&lt;/h3&gt;
&lt;p&gt;Currently, the Generic Ballot is holding steady with a slight sliver more Americans wanting Republicans in Congress than Democrats (&lt;strong&gt;50.7%&lt;/strong&gt; to &lt;strong&gt;49.3%&lt;/strong&gt;, respectively). Joe Biden’s net approval continues to slide, currently sitting at &lt;strong&gt;-11.4%&lt;/strong&gt; (&lt;strong&gt;41.8%&lt;/strong&gt; approve, &lt;strong&gt;53.1%&lt;/strong&gt; disapprove).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/generic_ballot_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/approval_disapproval_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/net_approval_current.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Diamonds are Forever: Feature Engineering with the Diamonds Dataset</title>
      <link>https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/</link>
      <pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Are y’all ready for some charts?? This week, I did a bit of machine learning practice with the &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/diamonds.html&#34;&gt;&lt;code&gt;diamonds dataset&lt;/code&gt;&lt;/a&gt;. This dataset is interesting and good for practice for a few reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there are lots of observations (50,000+);&lt;/li&gt;
&lt;li&gt;it includes a mix of numeric and categorical variables;&lt;/li&gt;
&lt;li&gt;there are some data oddities to deal with (log scales, interactions, non-linear relations)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ll be doing a bit of feature engineering prior to fitting an tuning a linear model that predicts the each diamond’s &lt;code&gt;price&lt;/code&gt; with the &lt;a href=&#34;https://glmnet.stanford.edu/index.html&#34;&gt;&lt;code&gt;glmnet&lt;/code&gt;&lt;/a&gt; package. This will give a good end-to-end glimpse into the data exploration and model fitting process! Before we get into that, let’s load some packages and get a preview of the dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidymodels)
library(vip)

theme_set(theme_minimal())

diamonds %&amp;gt;%
  slice_head(n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 10
##    carat cut       color clarity depth table price     x     y     z
##    &amp;lt;dbl&amp;gt; &amp;lt;ord&amp;gt;     &amp;lt;ord&amp;gt; &amp;lt;ord&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43
##  2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31
##  3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31
##  4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63
##  5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75
##  6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48
##  7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47
##  8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53
##  9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49
## 10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we’re predicting price, let’s look at its distribution first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = price)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We’re definitely gonna want to apply a transformation to the price when modeling - let’s look at the distribution on a log-10 scale.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = price)) +
  geom_histogram() +
  scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s a lot more evenly distributed, if not perfect. That’s a fine starting point, so now we’ll look through the rest of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = carat)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = cut,
             y = price)) +
  geom_boxplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  count(cut) %&amp;gt;%
  ggplot(aes(x = cut,
             y = n)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-3.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = color,
             y = price)) +
  geom_boxplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-4.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  count(color) %&amp;gt;%
  ggplot(aes(x = color,
             y = n)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-5.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = clarity,
             y = price)) +
  geom_boxplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-6.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  count(clarity) %&amp;gt;%
  ggplot(aes(x = clarity,
             y = n)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-7.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = depth)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-8.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = table)) +
  geom_histogram() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-9.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = x)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-10.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = y)) +
  geom_histogram() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-11.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;% 
  ggplot(aes(x = z)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-12.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like there may be a good opportunity to try out a few normalization and resampling techniques, but before we get into any of that, let’s build a baseline linear model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# splits
diamonds_split &amp;lt;- initial_split(diamonds)
diamonds_train &amp;lt;- training(diamonds_split)
diamonds_test &amp;lt;- testing(diamonds_split)

# resamples (don&amp;#39;t want to use testing data!)
diamonds_folds &amp;lt;- vfold_cv(diamonds_train)

# model spec
mod01 &amp;lt;-
  linear_reg() %&amp;gt;%
  set_engine(&amp;quot;lm&amp;quot;)

# recipe
rec01 &amp;lt;-
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_dummy(all_nominal_predictors())

# controls
ctrl_preds &amp;lt;- 
  control_resamples(save_pred = TRUE)

# create a wf
wf01 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod01) %&amp;gt;%
  add_recipe(rec01)

# parallel processing
doParallel::registerDoParallel()

# fit
rs01 &amp;lt;- 
  fit_resamples(
    wf01,
    diamonds_folds,
    control = ctrl_preds
  )

# metrics!
collect_metrics(rs01)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator     mean     n  std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   1120.       10 12.0     Preprocessor1_Model1
## 2 rsq     standard      0.921    10  0.00118 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And right off the bat, we can see a fairly high value for &lt;code&gt;rsq&lt;/code&gt;! However, &lt;code&gt;rsq&lt;/code&gt; doesn’t tell the whole story, so we should check our predictions and residuals plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;augment(rs01) %&amp;gt;%
  ggplot(aes(x = price,
             y = .pred)) +
  geom_point(alpha = 0.01) +
  geom_abline(linetype = &amp;quot;dashed&amp;quot;,
              size = 0.1,
              alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is &lt;em&gt;definitely&lt;/em&gt; not what we want to see! It looks like there’s an odd curve/structure to the graph and we’re actually predicting quite a few negative values. The residuals plot doesn’t look too great either.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;augment(rs01) %&amp;gt;%
  ggplot(aes(x = price,
             y = .resid)) +
  geom_point(alpha = 0.01) +
  geom_hline(yintercept = 0,
             linetype = &amp;quot;dashed&amp;quot;,
             alpha = 0.5,
             size = 0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What we’d like to see is a 0-correlation plot with errors normally distributed; what we’re seeing instead, however, is a ton of structure.&lt;/p&gt;
&lt;p&gt;That being said, that’s okay! we expected this first pass to be pretty rough! And the price is &lt;em&gt;clearly&lt;/em&gt; on a log-10 scale. To make apples-apples comparisons with models going forward, I’ll retrain this basic linear model to predict the &lt;code&gt;log10(price)&lt;/code&gt;. This’ll involve a bit of data re-manipulation!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# log transform price
diamonds_model &amp;lt;-
  diamonds %&amp;gt;%
  mutate(price = log10(price),
         across(cut:clarity, as.character))

# bad practice copy + paste lol

# splits
set.seed(999)
diamonds_split &amp;lt;- initial_split(diamonds_model)
diamonds_train &amp;lt;- training(diamonds_split)
diamonds_test &amp;lt;- testing(diamonds_split)

# resamples (don&amp;#39;t want to use testing data!)
set.seed(888)
diamonds_folds &amp;lt;- vfold_cv(diamonds_train)

# model spec
mod01 &amp;lt;-
  linear_reg() %&amp;gt;%
  set_engine(&amp;quot;lm&amp;quot;)

# recipe
rec01 &amp;lt;-
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_dummy(all_nominal_predictors())

# controls
ctrl_preds &amp;lt;- 
  control_resamples(save_pred = TRUE)

# create a wf
wf01 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod01) %&amp;gt;%
  add_recipe(rec01)

# parallel processing
doParallel::registerDoParallel()

# fit
set.seed(777)
rs01 &amp;lt;- 
  fit_resamples(
    wf01,
    diamonds_folds,
    control = ctrl_preds
  )

# metrics!
collect_metrics(rs01)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator   mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.0793    10 0.00557 Preprocessor1_Model1
## 2 rsq     standard   0.966     10 0.00494 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And wow, that &lt;em&gt;one&lt;/em&gt; transformation increased our &lt;code&gt;rsq&lt;/code&gt; to 0.96! Again, that’s not the whole story, and we’re going to be evaluating models based on the &lt;code&gt;rmse&lt;/code&gt;. Let’s look at how our prediction map has updated:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rs01 %&amp;gt;%
  augment() %&amp;gt;%
  ggplot(aes(x = price,
             y = .pred)) +
  geom_point(alpha = 0.01) +
  geom_abline(linetype = &amp;quot;dashed&amp;quot;,
              size = 0.1,
              alpha = 0.5) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now &lt;em&gt;that&lt;/em&gt; is a much better starting place to be at! Let’s look at our coefficients&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(666) # :thedevilisalive:
wf01 %&amp;gt;%
  fit(diamonds_train) %&amp;gt;%
  pull_workflow_fit() %&amp;gt;%
  vip::vi() %&amp;gt;%
  mutate(Variable = fct_reorder(Variable, Importance)) %&amp;gt;%
  ggplot(aes(x = Variable,
             y = Importance,
             fill = Sign)) +
  geom_col() +
  coord_flip() + 
  theme(plot.title.position = &amp;quot;plot&amp;quot;) +
  labs(x = NULL,
       y = NULL,
       title = &amp;quot;Diamonds are forever&amp;quot;,
       subtitle = &amp;quot;Variable importance plot of a basic linear regression predicting diamond price&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another way of looking at it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(666)
wf01 %&amp;gt;%
  fit(diamonds_train) %&amp;gt;%
  pull_workflow_fit() %&amp;gt;%
  vip::vi() %&amp;gt;%
  mutate(Importance = if_else(Sign == &amp;quot;NEG&amp;quot;, Importance * -1, Importance),
         Variable = fct_reorder(Variable, Importance)) %&amp;gt;%
  ggplot(aes(x = Variable,
             y = Importance,
             fill = Sign)) +
  geom_col() +
  coord_flip() +
  labs(title = &amp;quot;Diamonds are forever&amp;quot;,
       subtitle = &amp;quot;Variable importance plot of a basic linear regression predicting diamond price&amp;quot;,
       x = NULL,
       y = NULL) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a good, but definitely improvable, starting point. We can likely decrease our overall error with a bit of feature engineering and drop unimportant features by tuning a regularized model. There are some oddities in this initial model that will need to be improved upon; for one, we can definitively say that the &lt;code&gt;carat&lt;/code&gt; feature ought to be &lt;em&gt;positively&lt;/em&gt; associated with price&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  ggplot(aes(x = carat,
             y = price)) +
  geom_point(alpha = 0.01) +
  labs(title = &amp;quot;A clear positive (albeit nonlinear) relationship between `carat` and `price`&amp;quot;) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another few things that are interesting to note in this plot! It looks like there are clusterings of carat ratings around round-ish numbers. My hypothesis here is that carat ratings tend to get rounded up to the next size. There’s also a clear abscence of diamonds priced at $1,500 (~3.17 on the log10 scale). I suppose there is some industry-specific reason to avoid a diamond price of $,1500?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  ggplot(aes(x = carat,
             y = price)) +
  geom_point(alpha = 0.01) +
  labs(title = &amp;quot;A clear positive (albeit nonlinear) relationship between `carat` and `price`&amp;quot;) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;) +
  geom_hline(yintercept = log10(1500),
             linetype = &amp;quot;dashed&amp;quot;,
             size = 0.9,
             alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;How to address all these things? With some feature engineering! Firstly, let’s add some recipe steps to balance classes &amp;amp; normalize continuous variables.&lt;/p&gt;
&lt;p&gt;But before I get into &lt;em&gt;that&lt;/em&gt;, I’ll save the resample metrics so that we can compare models!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics &amp;lt;- collect_metrics(rs01) %&amp;gt;% mutate(model = &amp;quot;model01&amp;quot;)

metrics&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 7
##   .metric .estimator   mean     n std_err .config              model  
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt;  
## 1 rmse    standard   0.0793    10 0.00557 Preprocessor1_Model1 model01
## 2 rsq     standard   0.966     10 0.00494 Preprocessor1_Model1 model01&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# spec will be the same as model01
mod02 &amp;lt;- mod01

# recipe!
rec02 &amp;lt;- 
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_other(cut, color, clarity) %&amp;gt;% 
  step_dummy(all_nominal_predictors(), -cut) %&amp;gt;%
  
  # use smote resampling to balance classes
  themis::step_smote(cut) %&amp;gt;% 
    
  # normalize continuous vars
  bestNormalize::step_best_normalize(carat, depth, table, x, y, z)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s &lt;a href=&#34;https://recipes.tidymodels.org/reference/bake.html&#34;&gt;bake&lt;/a&gt; our recipe to verify that everything looks up-to-snuff in the preprocessed dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 &amp;lt;- 
  rec02 %&amp;gt;%
  prep() %&amp;gt;%
  bake(new_data = NULL)

baked_rec02&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 80,495 x 20
##      carat cut        depth  table       x       y      z price color_E color_F
##      &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 -0.706  Premium    0.138 -0.760 -0.709  -0.738  -0.695  3.01       0       1
##  2  0.356  Very Good  0.570  0.835  0.342   0.251   0.344  3.63       0       0
##  3  0.214  Premium   -0.308  0.835  0.293   0.263   0.166  3.58       0       0
##  4 -1.08   other      1.04  -0.310 -1.30   -1.40   -0.995  2.70       0       0
##  5 -0.641  Ideal     -0.602 -0.760 -0.595  -0.560  -0.622  2.97       0       0
##  6 -0.0759 Premium   -0.602  0.494 -0.0349 -0.0460 -0.114  3.38       0       0
##  7 -0.149  Premium   -1.16   0.103 -0.0565 -0.0842 -0.246  3.44       1       0
##  8  0.170  Very Good -0.371  0.494  0.178   0.313   0.130  3.56       0       1
##  9 -0.736  Ideal     -0.110 -0.760 -0.709  -0.738  -0.723  3.09       0       0
## 10  0.782  Ideal     -0.602 -0.310  0.819   0.846   0.732  4.02       0       0
## # ... with 80,485 more rows, and 10 more variables: color_G &amp;lt;dbl&amp;gt;,
## #   color_H &amp;lt;dbl&amp;gt;, color_I &amp;lt;dbl&amp;gt;, color_J &amp;lt;dbl&amp;gt;, clarity_SI2 &amp;lt;dbl&amp;gt;,
## #   clarity_VS1 &amp;lt;dbl&amp;gt;, clarity_VS2 &amp;lt;dbl&amp;gt;, clarity_VVS1 &amp;lt;dbl&amp;gt;,
## #   clarity_VVS2 &amp;lt;dbl&amp;gt;, clarity_other &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  count(cut) %&amp;gt;%
  ggplot(aes(x = cut,
             y = n)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  ggplot(aes(x = carat)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-2.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  ggplot(aes(x = depth)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-3.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  ggplot(aes(x = table)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-4.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  ggplot(aes(x = x)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-5.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  ggplot(aes(x = y)) +
  geom_histogram() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-6.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  ggplot(aes(x = z)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-7.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Everything looks alright with the exception of the &lt;code&gt;table&lt;/code&gt; predictor. I wonder if there are a lot of repeated values in the &lt;code&gt;table&lt;/code&gt; variable - that may be why we’re seeing a “chunky” histogram. Let’s check&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  count(table) %&amp;gt;%
  arrange(desc(n))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10,406 x 2
##     table     n
##     &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
##  1  0.103 12167
##  2 -0.310 11408
##  3 -0.760 11031
##  4  0.494  9406
##  5 -1.28   6726
##  6  0.835  6165
##  7  1.15   3810
##  8 -1.85   2789
##  9  1.42   2182
## 10  1.64    972
## # ... with 10,396 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ooh - okay yeah that’s definitely the issue! I’m not &lt;em&gt;quite&lt;/em&gt; sure how to deal with it, so we’re just going to ignore for now! Let’s add a new model &amp;amp; see how it compares against the baseline transformed model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wf02 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod02) %&amp;gt;%
  add_recipe(rec02)

# stop parallel to avoid error!
# need to replace with PSOCK clusters
# see github issue here: https://github.com/tidymodels/recipes/issues/847
foreach::registerDoSEQ()

set.seed(666) # spoopy
rs02 &amp;lt;-
  fit_resamples(
    wf02,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs02)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator  mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.115    10 0.00141 Preprocessor1_Model1
## 2 rsq     standard   0.932    10 0.00158 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Oof - that’s actually slightly worse than our baseline model!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rs02 %&amp;gt;%
  augment() %&amp;gt;%
  ggplot(aes(x = price,
             y = .pred)) +
  geom_point(alpha = 0.01) +
  geom_abline(linetype = &amp;quot;dashed&amp;quot;,
              size = 0.1,
              alpha = 0.5) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like we’ve introduced structure into the residual plot!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rs02 %&amp;gt;%
  augment() %&amp;gt;%
  ggplot(aes(x = price,
             y = .resid)) +
  geom_point(alpha = 0.01) +
  geom_hline(yintercept = 0,
             linetype = &amp;quot;dashed&amp;quot;,
             size = 0.1,
             alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yeah that’s fairly wonky! I’m wondering if it’s due to the SMOTE upsampling method we introduced? To counteract, I’ll build &amp;amp; train new models after each set of recipe steps (e.g., resampling, normalizing, interactions) to buil up a better performing model one step at a time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics &amp;lt;- 
  metrics %&amp;gt;%
  bind_rows(collect_metrics(rs02) %&amp;gt;% mutate(model = &amp;quot;model02&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# same model spec
mod03 &amp;lt;- mod02

# rebuild rec+wf &amp;amp; retrain
rec03 &amp;lt;- 
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_other(cut, color, clarity) %&amp;gt;%
  step_dummy(all_nominal_predictors(), -cut) %&amp;gt;%
  themis::step_smote(cut)

wf03 &amp;lt;- 
  workflow() %&amp;gt;%
  add_model(mod03) %&amp;gt;%
  add_recipe(rec03)

# do paralllel
doParallel::registerDoParallel()

# refit!
set.seed(123)
rs03 &amp;lt;-
  fit_resamples(
    wf03,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs03)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator   mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.0918    10 0.00502 Preprocessor1_Model1
## 2 rsq     standard   0.956     10 0.00502 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interesting! Improved relative to &lt;code&gt;rs02&lt;/code&gt;, but still not as good as our first model! Let’s try using &lt;code&gt;step_downsample()&lt;/code&gt; to balance classes &amp;amp; see how we fare.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# cleanup some large-ish items eating up memory
rm(mod01, mod02, rec01, rec02, wf01, wf02, rs01, rs02)

# save metrics
metrics &amp;lt;- 
  metrics %&amp;gt;%
  bind_rows(collect_metrics(rs03) %&amp;gt;% mutate(model = &amp;quot;model03&amp;quot;))

# new mod
mod04 &amp;lt;- mod03

# new rec
rec04 &amp;lt;-
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_other(cut, color, clarity) %&amp;gt;%
  step_dummy(all_nominal_predictors(), -cut) %&amp;gt;%
  themis::step_downsample(cut)

wf04 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod04) %&amp;gt;%
  add_recipe(rec04) 

set.seed(456) 
rs04 &amp;lt;-
  fit_resamples(
    wf04,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs04)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator  mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.120    10  0.0150 Preprocessor1_Model1
## 2 rsq     standard   0.921    10  0.0194 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wow - still a bit worse! I’ll try upsampling &amp;amp; if there is no improvement, we’ll move on without resampling!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics &amp;lt;-
  metrics %&amp;gt;%
  bind_rows(collect_metrics(rs04) %&amp;gt;% mutate(model = &amp;quot;model04&amp;quot;))

mod05 &amp;lt;- mod04

rec05 &amp;lt;-
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_other(cut, color, clarity) %&amp;gt;%
  step_dummy(all_nominal_predictors(), -cut) %&amp;gt;%
  themis::step_upsample(cut)

wf05 &amp;lt;- 
  workflow() %&amp;gt;%
  add_model(mod05) %&amp;gt;%
  add_recipe(rec05) 

set.seed(789)
rs05 &amp;lt;-
  fit_resamples(
    wf05,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs05)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator  mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.106    10 0.00618 Preprocessor1_Model1
## 2 rsq     standard   0.941    10 0.00714 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Okay - resampling gets stricken off our list of recipe steps! Let’s look at how the models compare so far&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics &amp;lt;-
  metrics %&amp;gt;%
  bind_rows(collect_metrics(rs05) %&amp;gt;% mutate(model = &amp;quot;model05&amp;quot;))

metrics %&amp;gt;%
  ggplot(aes(x = model)) +
  geom_point(aes(y = mean)) +
  geom_errorbar(aes(ymin = mean - std_err,
                    ymax = mean + std_err)) +
  facet_wrap(~.metric, scales = &amp;quot;free_y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The first simple linear model was the best as measured by both metrics! Let’s see if we can improve with some normalization of the continuous vars.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(mod03, mod04, rec03, rec04, rs03, rs04, wf03, wf04)

mod06 &amp;lt;- mod05

rec06 &amp;lt;-
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_other(cut, color, clarity) %&amp;gt;%
  bestNormalize::step_best_normalize(all_numeric_predictors()) %&amp;gt;%
  step_dummy(all_nominal_predictors())

wf06 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod06) %&amp;gt;%
  add_recipe(rec06)

foreach::registerDoSEQ()
set.seed(101112)
rs06 &amp;lt;-
  fit_resamples(
    wf06,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs06)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator  mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.127    10 0.00115 Preprocessor1_Model1
## 2 rsq     standard   0.916    10 0.00136 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well - that was quite a bit for no improvement! I guess that normalizing the continuous vars in this case isn’t helping. Moving on to adding some interactions - first let’s explore potential interactions a bit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics &amp;lt;-
  metrics %&amp;gt;% 
  bind_rows(collect_metrics(rs06) %&amp;gt;% mutate(model = &amp;quot;model06&amp;quot;))

diamonds_train %&amp;gt;%
  ggplot(aes(x = carat,
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) + 
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(splines)
diamonds_train %&amp;gt;%
  ggplot(aes(x = carat,
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = lm,
              formula = y ~ ns(x, df = 5),
              se = FALSE) +
  facet_wrap(~cut, scales = &amp;quot;free&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;5 spline terms might not be sufficient here - capturing the lower bound well but &lt;em&gt;really&lt;/em&gt; not doing well with the higher carat diamonds.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  ggplot(aes(x = carat,
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = lm,
              formula = y ~ ns(x, df = 10),
              se = FALSE) +
  facet_wrap(~cut, scales = &amp;quot;free&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hmmmm, 10 might be too many. It looks lie we’ll just lose a bit of confidence for the Premium &amp;amp; Very Good diamonds at higher carats. Relative to the total number, I’m not too concerned.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  ggplot(aes(x = carat,
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = lm,
              formula = y ~ ns(x, df = 7),
              se = FALSE) +
  facet_wrap(~cut, scales = &amp;quot;free&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;7 terms feels like the best we’re going to do here - I think this is tuneable, but we’ll leave as is (now &amp;amp; in the final model).&lt;/p&gt;
&lt;p&gt;Next, we’ll look at creating interactions between the &lt;code&gt;color&lt;/code&gt; and &lt;code&gt;carat&lt;/code&gt; variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  ggplot(aes(x = carat, 
             y = price,
             color = color)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = lm, 
              formula = y ~ ns(x, df = 15),
              se = FALSE) +
  facet_wrap(~color)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Adding interactive spline terms with &lt;code&gt;df&lt;/code&gt; of 15 seems to add some useful information!&lt;/p&gt;
&lt;p&gt;We have three shape parameters, &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, and &lt;code&gt;z&lt;/code&gt; - I wonder if creating a stand-in for volume by multiplying them all together will provide any useful information?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  mutate(volume_param = x * y * z) %&amp;gt;%
  ggplot(aes(x = volume_param,
             y = price)) +
  geom_point(alpha = 0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ooh, looks like we’re getting some good info here, but we may want to use &lt;code&gt;log10&lt;/code&gt; to scale this back.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  mutate(volume_param = log10(x * y * z)) %&amp;gt;%
  ggplot(aes(x = volume_param, 
             y = price)) +
  geom_point(alpha = 0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s see if this ought to interact with any other paramaters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  mutate(volume_param = log10(x * y * z)) %&amp;gt;%
  ggplot(aes(x = volume_param, 
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-35-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  mutate(volume_param = log10(x * y * z)) %&amp;gt;%
  ggplot(aes(x = volume_param, 
             y = price,
             color = color)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-35-2.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  mutate(volume_param = log10(x * y * z)) %&amp;gt;%
  ggplot(aes(x = volume_param, 
             y = price,
             color = clarity)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-35-3.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hmm, it doesn’t really look like we’re capturing too great of interactions, so I’ll leave out for now. It looks like the &lt;em&gt;size&lt;/em&gt; of the rock is more important than anything else! I could continue to dig further, but I’ll stop there. I’m likely getting diminishing returns, &amp;amp; I’d like to get back into modeling!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod07 &amp;lt;- mod06

rec07 &amp;lt;-
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_other(cut, color, clarity) %&amp;gt;%
  step_dummy(all_nominal_predictors()) %&amp;gt;%
  step_interact(~carat:starts_with(&amp;quot;cut_&amp;quot;)) %&amp;gt;%
  step_interact(~carat:starts_with(&amp;quot;color_&amp;quot;)) %&amp;gt;%
  step_mutate_at(c(x, y, z),
                 fn = ~if_else(.x == 0, mean(.x), .x)) %&amp;gt;%
  step_mutate(volume_param = log10(x * y * z)) %&amp;gt;%
  step_ns(starts_with(&amp;quot;carat_x_cut&amp;quot;), deg_free = 7) %&amp;gt;%
  step_ns(starts_with(&amp;quot;carat_x_color&amp;quot;), deg_free = 15) 

rec07&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          9
## 
## Operations:
## 
## Collapsing factor levels for cut, color, clarity
## Dummy variables from all_nominal_predictors()
## Interactions with carat:starts_with(&amp;quot;cut_&amp;quot;)
## Interactions with carat:starts_with(&amp;quot;color_&amp;quot;)
## Variable mutation for c(x, y, z)
## Variable mutation for volume_param
## Natural Splines on starts_with(&amp;quot;carat_x_cut&amp;quot;)
## Natural Splines on starts_with(&amp;quot;carat_x_color&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wf07 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod07) %&amp;gt;%
  add_recipe(rec07)

doParallel::registerDoParallel()
set.seed(9876)
rs07 &amp;lt;-
  fit_resamples(
    wf07,
    diamonds_folds,
    control = ctrl_preds
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is definitely going to &lt;em&gt;way&lt;/em&gt; overfit our data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rs07 %&amp;gt;%
  collect_metrics()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator   mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.0750    10 0.00377 Preprocessor1_Model1
## 2 rsq     standard   0.970     10 0.00342 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well we (finally) made a modes improvement! Let’s see how the predictions/residuals plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rs07 %&amp;gt;%
  augment() %&amp;gt;%
  ggplot(aes(x = price,
             y = .pred)) +
  geom_point(alpha = 0.05) +
  geom_abline(linetype = &amp;quot;dashed&amp;quot;,
              alpha = 0.5,
              size = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-38-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s pretty good! We do have one value that’s &lt;strong&gt;&lt;em&gt;way&lt;/em&gt;&lt;/strong&gt; off, so let’s see if regulization can help. This will require setting a new baseline model, and we’ll tune our way to the best regularizaion parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics &amp;lt;- 
  rs07 %&amp;gt;%
  collect_metrics() %&amp;gt;%
  mutate(model = &amp;quot;model07&amp;quot;) %&amp;gt;%
  bind_rows(metrics)

# add normalization step
rec08 &amp;lt;- 
  rec07 %&amp;gt;% 
  step_zv(all_numeric_predictors()) %&amp;gt;%
  step_normalize(all_numeric_predictors(),
                 -cut_Ideal, -cut_Premium, -cut_Very.Good, -cut_other,
                 -color_E, -color_F, -color_G, -color_H, -color_I, -color_J,
                 -clarity_SI2, -clarity_VS1, -clarity_VS2, -clarity_VVS1, -clarity_VVS2, -clarity_other)

rm(mod05, mod06, mod07, rec05, rec06, rec07, wf05, wf06, wf07, rs05, rs06, rs07)

mod08 &amp;lt;-
  linear_reg(penalty = tune(), mixture = tune()) %&amp;gt;%
  set_engine(&amp;quot;glmnet&amp;quot;) %&amp;gt;%
  set_mode(&amp;quot;regression&amp;quot;) 

wf08 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod08) %&amp;gt;%
  add_recipe(rec08)

diamonds_grid &amp;lt;- 
  grid_regular(penalty(), mixture(), levels = 20)

doParallel::registerDoParallel()
set.seed(5831)
rs08 &amp;lt;-
  tune_grid(
    wf08,
    resamples = diamonds_folds,
    control = ctrl_preds,
    grid = diamonds_grid
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some notes but let’s explore our results…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rs08 %&amp;gt;%
  collect_metrics() %&amp;gt;%
  ggplot(aes(x = penalty,
             y = mean,
             color = as.character(mixture))) +
  geom_point() +
  geom_line(alpha = 0.75) +
  facet_wrap(~.metric, scales = &amp;quot;free&amp;quot;) +
  scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-40-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like we were performing pretty well with the unregularized model, oddly enough! Let’s select the best and finalize our workflow.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_metrics &amp;lt;- 
  rs08 %&amp;gt;%
  select_best(&amp;quot;rmse&amp;quot;)

wf_final &amp;lt;- 
  finalize_workflow(wf08, best_metrics)

rm(mod08, rec07, rec08, rs08, wf08)

set.seed(333)
final_fit &amp;lt;- 
  wf_final %&amp;gt;%
  fit(diamonds_train)

final_fit %&amp;gt;%
  predict(diamonds_test) %&amp;gt;%
  bind_cols(diamonds_test) %&amp;gt;%
  select(price, .pred) %&amp;gt;%
  ggplot(aes(x = price, 
             y = .pred)) +
  geom_point(alpha = 0.05) + 
  geom_abline(alpha = 0.5,
              linetype = &amp;quot;dashed&amp;quot;,
              size = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-41-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What are the most important variables in this regularized model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_fit %&amp;gt;%
  pull_workflow_fit() %&amp;gt;%
  vi(lambda = best_metrics$penalty) %&amp;gt;%
  mutate(Variable = fct_reorder(Variable, Importance)) %&amp;gt;%
  ggplot(aes(x = Variable,
             y = Importance, 
             fill = Sign)) +
  geom_col() +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-42-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, most of our terms get regularized away, which is what we want! Our chart is a little unreadable; let’s plot just the most important variables in a few ways:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_fit %&amp;gt;%
  pull_workflow_fit() %&amp;gt;%
  vi(lambda = best_metrics$penalty) %&amp;gt;%
  arrange(desc(Importance)) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  mutate(Variable = fct_reorder(Variable, Importance)) %&amp;gt;%
  ggplot(aes(x = Variable,
             y = Importance,
             fill = Sign)) +
  geom_col() +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-43-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_fit %&amp;gt;%
  pull_workflow_fit() %&amp;gt;%
  vi(lambda = best_metrics$penalty) %&amp;gt;%
  arrange(desc(Importance)) %&amp;gt;% 
  slice_head(n = 10) %&amp;gt;%
  mutate(Importance = if_else(Sign == &amp;quot;NEG&amp;quot;, -Importance, Importance),
         Variable = fct_reorder(Variable, Importance)) %&amp;gt;%
  ggplot(aes(x = Variable,
             y = Importance,
             fill = Sign)) + 
  geom_col() +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-44-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And look at that! Our most important variable was one that came from feature engineering! The size of the rock had the biggest impact on price.&lt;/p&gt;
&lt;p&gt;We’ve gone through a lot of steps, so it may be good to look back on what was done:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explored our dataset via some simple exploratory data analysis;&lt;/li&gt;
&lt;li&gt;Fit a simple linear model to predict the log-transform of price;&lt;/li&gt;
&lt;li&gt;Attempted (and failed) to improve upon the simple model with fancier normalization and resampling techniques;&lt;/li&gt;
&lt;li&gt;Explored the dataset further to find meaningful interactions and potential new features;&lt;/li&gt;
&lt;li&gt;Fit a new model with feature engineering;&lt;/li&gt;
&lt;li&gt;Tuned regularization parameters on our model with feature engineering to arrive at the final model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our models’ performances, ranked from best to worst, show that the final tuned model did indeed perform the best on the test dataset!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics %&amp;gt;%
  select(.metric, mean, model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 14 x 3
##    .metric   mean model  
##    &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  
##  1 rmse    0.0750 model07
##  2 rsq     0.970  model07
##  3 rmse    0.0793 model01
##  4 rsq     0.966  model01
##  5 rmse    0.115  model02
##  6 rsq     0.932  model02
##  7 rmse    0.0918 model03
##  8 rsq     0.956  model03
##  9 rmse    0.120  model04
## 10 rsq     0.921  model04
## 11 rmse    0.106  model05
## 12 rsq     0.941  model05
## 13 rmse    0.127  model06
## 14 rsq     0.916  model06&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_preds &amp;lt;-
  final_fit %&amp;gt;%
  predict(diamonds_train) %&amp;gt;%
  bind_cols(diamonds_train) %&amp;gt;%
  select(price, .pred)

bind_rows(final_preds %&amp;gt;% rmse(price, .pred),
          final_preds %&amp;gt;% rsq(price, .pred)) %&amp;gt;%
  rename(mean = .estimate) %&amp;gt;%
  select(-.estimator) %&amp;gt;%
  mutate(model = &amp;quot;model_final&amp;quot;) %&amp;gt;%
  bind_rows(metrics %&amp;gt;% select(.metric, mean, model)) %&amp;gt;%
  pivot_wider(names_from = .metric,
              values_from = mean) %&amp;gt;%
  mutate(model = fct_reorder(model, desc(rmse))) %&amp;gt;%
  pivot_longer(rmse:rsq,
               names_to = &amp;quot;metric&amp;quot;,
               values_to = &amp;quot;value&amp;quot;) %&amp;gt;%
  ggplot(aes(x = model,
             y = value)) +
  geom_point() +
  facet_wrap(~metric, scales = &amp;quot;free&amp;quot;) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-45-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
