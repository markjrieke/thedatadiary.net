<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tidymodels on the data diary</title>
    <link>https://www.thedatadiary.net/categories/tidymodels/</link>
    <description>Recent content in tidymodels on the data diary</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Mark Rieke {year}</copyright>
    <lastBuildDate>Sun, 14 Nov 2021 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://www.thedatadiary.net/categories/tidymodels/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Diamonds are Forever: Feature Engineering with the Diamonds Dataset</title>
      <link>https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/</link>
      <pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Are y’all ready for some charts?? This week, I did a bit of machine learning practice with the &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/diamonds.html&#34;&gt;&lt;code&gt;diamonds dataset&lt;/code&gt;&lt;/a&gt;. This dataset is interesting and good for practice for a few reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there are lots of observations (50,000+);&lt;/li&gt;
&lt;li&gt;it includes a mix of numeric and categorical variables;&lt;/li&gt;
&lt;li&gt;there are some data oddities to deal with (log scales, interactions, non-linear relations)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ll be doing a bit of feature engineering prior to fitting an tuning a linear model that predicts the each diamond’s &lt;code&gt;price&lt;/code&gt; with the &lt;a href=&#34;https://glmnet.stanford.edu/index.html&#34;&gt;&lt;code&gt;glmnet&lt;/code&gt;&lt;/a&gt; package. This will give a good end-to-end glimpse into the data exploration and model fitting process! Before we get into that, let’s load some packages and get a preview of the dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidymodels)
library(vip)

theme_set(theme_minimal())

diamonds %&amp;gt;%
  slice_head(n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 10
##    carat cut       color clarity depth table price     x     y     z
##    &amp;lt;dbl&amp;gt; &amp;lt;ord&amp;gt;     &amp;lt;ord&amp;gt; &amp;lt;ord&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43
##  2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31
##  3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31
##  4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63
##  5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75
##  6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48
##  7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47
##  8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53
##  9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49
## 10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we’re predicting price, let’s look at its distribution first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = price)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We’re definitely gonna want to apply a transformation to the price when modeling - let’s look at the distribution on a log-10 scale.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = price)) +
  geom_histogram() +
  scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s a lot more evenly distributed, if not perfect. That’s a fine starting point, so now we’ll look through the rest of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = carat)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = cut,
             y = price)) +
  geom_boxplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  count(cut) %&amp;gt;%
  ggplot(aes(x = cut,
             y = n)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-3.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = color,
             y = price)) +
  geom_boxplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-4.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  count(color) %&amp;gt;%
  ggplot(aes(x = color,
             y = n)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-5.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = clarity,
             y = price)) +
  geom_boxplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-6.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  count(clarity) %&amp;gt;%
  ggplot(aes(x = clarity,
             y = n)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-7.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = depth)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-8.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = table)) +
  geom_histogram() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-9.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = x)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-10.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;%
  ggplot(aes(x = y)) +
  geom_histogram() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-11.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds %&amp;gt;% 
  ggplot(aes(x = z)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-4-12.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like there may be a good opportunity to try out a few normalization and resampling techniques, but before we get into any of that, let’s build a baseline linear model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# splits
diamonds_split &amp;lt;- initial_split(diamonds)
diamonds_train &amp;lt;- training(diamonds_split)
diamonds_test &amp;lt;- testing(diamonds_split)

# resamples (don&amp;#39;t want to use testing data!)
diamonds_folds &amp;lt;- vfold_cv(diamonds_train)

# model spec
mod01 &amp;lt;-
  linear_reg() %&amp;gt;%
  set_engine(&amp;quot;lm&amp;quot;)

# recipe
rec01 &amp;lt;-
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_dummy(all_nominal_predictors())

# controls
ctrl_preds &amp;lt;- 
  control_resamples(save_pred = TRUE)

# create a wf
wf01 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod01) %&amp;gt;%
  add_recipe(rec01)

# parallel processing
doParallel::registerDoParallel()

# fit
rs01 &amp;lt;- 
  fit_resamples(
    wf01,
    diamonds_folds,
    control = ctrl_preds
  )

# metrics!
collect_metrics(rs01)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator     mean     n  std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   1120.       10 12.0     Preprocessor1_Model1
## 2 rsq     standard      0.921    10  0.00118 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And right off the bat, we can see a fairly high value for &lt;code&gt;rsq&lt;/code&gt;! However, &lt;code&gt;rsq&lt;/code&gt; doesn’t tell the whole story, so we should check our predictions and residuals plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;augment(rs01) %&amp;gt;%
  ggplot(aes(x = price,
             y = .pred)) +
  geom_point(alpha = 0.01) +
  geom_abline(linetype = &amp;quot;dashed&amp;quot;,
              size = 0.1,
              alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is &lt;em&gt;definitely&lt;/em&gt; not what we want to see! It looks like there’s an odd curve/structure to the graph and we’re actually predicting quite a few negative values. The residuals plot doesn’t look too great either.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;augment(rs01) %&amp;gt;%
  ggplot(aes(x = price,
             y = .resid)) +
  geom_point(alpha = 0.01) +
  geom_hline(yintercept = 0,
             linetype = &amp;quot;dashed&amp;quot;,
             alpha = 0.5,
             size = 0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What we’d like to see is a 0-correlation plot with errors normally distributed; what we’re seeing instead, however, is a ton of structure.&lt;/p&gt;
&lt;p&gt;That being said, that’s okay! we expected this first pass to be pretty rough! And the price is &lt;em&gt;clearly&lt;/em&gt; on a log-10 scale. To make apples-apples comparisons with models going forward, I’ll retrain this basic linear model to predict the &lt;code&gt;log10(price)&lt;/code&gt;. This’ll involve a bit of data re-manipulation!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# log transform price
diamonds_model &amp;lt;-
  diamonds %&amp;gt;%
  mutate(price = log10(price),
         across(cut:clarity, as.character))

# bad practice copy + paste lol

# splits
set.seed(999)
diamonds_split &amp;lt;- initial_split(diamonds_model)
diamonds_train &amp;lt;- training(diamonds_split)
diamonds_test &amp;lt;- testing(diamonds_split)

# resamples (don&amp;#39;t want to use testing data!)
set.seed(888)
diamonds_folds &amp;lt;- vfold_cv(diamonds_train)

# model spec
mod01 &amp;lt;-
  linear_reg() %&amp;gt;%
  set_engine(&amp;quot;lm&amp;quot;)

# recipe
rec01 &amp;lt;-
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_dummy(all_nominal_predictors())

# controls
ctrl_preds &amp;lt;- 
  control_resamples(save_pred = TRUE)

# create a wf
wf01 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod01) %&amp;gt;%
  add_recipe(rec01)

# parallel processing
doParallel::registerDoParallel()

# fit
set.seed(777)
rs01 &amp;lt;- 
  fit_resamples(
    wf01,
    diamonds_folds,
    control = ctrl_preds
  )

# metrics!
collect_metrics(rs01)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator   mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.0793    10 0.00557 Preprocessor1_Model1
## 2 rsq     standard   0.966     10 0.00494 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And wow, that &lt;em&gt;one&lt;/em&gt; transformation increased our &lt;code&gt;rsq&lt;/code&gt; to 0.96! Again, that’s not the whole story, and we’re going to be evaluating models based on the &lt;code&gt;rmse&lt;/code&gt;. Let’s look at how our prediction map has updated:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rs01 %&amp;gt;%
  augment() %&amp;gt;%
  ggplot(aes(x = price,
             y = .pred)) +
  geom_point(alpha = 0.01) +
  geom_abline(linetype = &amp;quot;dashed&amp;quot;,
              size = 0.1,
              alpha = 0.5) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now &lt;em&gt;that&lt;/em&gt; is a much better starting place to be at! Let’s look at our coefficients&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(666) # :thedevilisalive:
wf01 %&amp;gt;%
  fit(diamonds_train) %&amp;gt;%
  pull_workflow_fit() %&amp;gt;%
  vip::vi() %&amp;gt;%
  mutate(Variable = fct_reorder(Variable, Importance)) %&amp;gt;%
  ggplot(aes(x = Variable,
             y = Importance,
             fill = Sign)) +
  geom_col() +
  coord_flip() + 
  theme(plot.title.position = &amp;quot;plot&amp;quot;) +
  labs(x = NULL,
       y = NULL,
       title = &amp;quot;Diamonds are forever&amp;quot;,
       subtitle = &amp;quot;Variable importance plot of a basic linear regression predicting diamond price&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another way of looking at it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(666)
wf01 %&amp;gt;%
  fit(diamonds_train) %&amp;gt;%
  pull_workflow_fit() %&amp;gt;%
  vip::vi() %&amp;gt;%
  mutate(Importance = if_else(Sign == &amp;quot;NEG&amp;quot;, Importance * -1, Importance),
         Variable = fct_reorder(Variable, Importance)) %&amp;gt;%
  ggplot(aes(x = Variable,
             y = Importance,
             fill = Sign)) +
  geom_col() +
  coord_flip() +
  labs(title = &amp;quot;Diamonds are forever&amp;quot;,
       subtitle = &amp;quot;Variable importance plot of a basic linear regression predicting diamond price&amp;quot;,
       x = NULL,
       y = NULL) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a good, but definitely improvable, starting point. We can likely decrease our overall error with a bit of feature engineering and drop unimportant features by tuning a regularized model. There are some oddities in this initial model that will need to be improved upon; for one, we can definitively say that the &lt;code&gt;carat&lt;/code&gt; feature ought to be &lt;em&gt;positively&lt;/em&gt; associated with price&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  ggplot(aes(x = carat,
             y = price)) +
  geom_point(alpha = 0.01) +
  labs(title = &amp;quot;A clear positive (albeit nonlinear) relationship between `carat` and `price`&amp;quot;) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another few things that are interesting to note in this plot! It looks like there are clusterings of carat ratings around round-ish numbers. My hypothesis here is that carat ratings tend to get rounded up to the next size. There’s also a clear abscence of diamonds priced at $1,500 (~3.17 on the log10 scale). I suppose there is some industry-specific reason to avoid a diamond price of $,1500?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  ggplot(aes(x = carat,
             y = price)) +
  geom_point(alpha = 0.01) +
  labs(title = &amp;quot;A clear positive (albeit nonlinear) relationship between `carat` and `price`&amp;quot;) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;) +
  geom_hline(yintercept = log10(1500),
             linetype = &amp;quot;dashed&amp;quot;,
             size = 0.9,
             alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;How to address all these things? With some feature engineering! Firstly, let’s add some recipe steps to balance classes &amp;amp; normalize continuous variables.&lt;/p&gt;
&lt;p&gt;But before I get into &lt;em&gt;that&lt;/em&gt;, I’ll save the resample metrics so that we can compare models!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics &amp;lt;- collect_metrics(rs01) %&amp;gt;% mutate(model = &amp;quot;model01&amp;quot;)

metrics&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 7
##   .metric .estimator   mean     n std_err .config              model  
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt;  
## 1 rmse    standard   0.0793    10 0.00557 Preprocessor1_Model1 model01
## 2 rsq     standard   0.966     10 0.00494 Preprocessor1_Model1 model01&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# spec will be the same as model01
mod02 &amp;lt;- mod01

# recipe!
rec02 &amp;lt;- 
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_other(cut, color, clarity) %&amp;gt;% 
  step_dummy(all_nominal_predictors(), -cut) %&amp;gt;%
  
  # use smote resampling to balance classes
  themis::step_smote(cut) %&amp;gt;% 
    
  # normalize continuous vars
  bestNormalize::step_best_normalize(carat, depth, table, x, y, z)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s &lt;a href=&#34;https://recipes.tidymodels.org/reference/bake.html&#34;&gt;bake&lt;/a&gt; our recipe to verify that everything looks up-to-snuff in the preprocessed dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 &amp;lt;- 
  rec02 %&amp;gt;%
  prep() %&amp;gt;%
  bake(new_data = NULL)

baked_rec02&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 80,495 x 20
##      carat cut        depth  table       x       y      z price color_E color_F
##      &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 -0.706  Premium    0.138 -0.760 -0.709  -0.738  -0.695  3.01       0       1
##  2  0.356  Very Good  0.570  0.835  0.342   0.251   0.344  3.63       0       0
##  3  0.214  Premium   -0.308  0.835  0.293   0.263   0.166  3.58       0       0
##  4 -1.08   other      1.04  -0.310 -1.30   -1.40   -0.995  2.70       0       0
##  5 -0.641  Ideal     -0.602 -0.760 -0.595  -0.560  -0.622  2.97       0       0
##  6 -0.0759 Premium   -0.602  0.494 -0.0349 -0.0460 -0.114  3.38       0       0
##  7 -0.149  Premium   -1.16   0.103 -0.0565 -0.0842 -0.246  3.44       1       0
##  8  0.170  Very Good -0.371  0.494  0.178   0.313   0.130  3.56       0       1
##  9 -0.736  Ideal     -0.110 -0.760 -0.709  -0.738  -0.723  3.09       0       0
## 10  0.782  Ideal     -0.602 -0.310  0.819   0.846   0.732  4.02       0       0
## # ... with 80,485 more rows, and 10 more variables: color_G &amp;lt;dbl&amp;gt;,
## #   color_H &amp;lt;dbl&amp;gt;, color_I &amp;lt;dbl&amp;gt;, color_J &amp;lt;dbl&amp;gt;, clarity_SI2 &amp;lt;dbl&amp;gt;,
## #   clarity_VS1 &amp;lt;dbl&amp;gt;, clarity_VS2 &amp;lt;dbl&amp;gt;, clarity_VVS1 &amp;lt;dbl&amp;gt;,
## #   clarity_VVS2 &amp;lt;dbl&amp;gt;, clarity_other &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  count(cut) %&amp;gt;%
  ggplot(aes(x = cut,
             y = n)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  ggplot(aes(x = carat)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-2.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  ggplot(aes(x = depth)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-3.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  ggplot(aes(x = table)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-4.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  ggplot(aes(x = x)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-5.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  ggplot(aes(x = y)) +
  geom_histogram() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-6.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  ggplot(aes(x = z)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-17-7.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Everything looks alright with the exception of the &lt;code&gt;table&lt;/code&gt; predictor. I wonder if there are a lot of repeated values in the &lt;code&gt;table&lt;/code&gt; variable - that may be why we’re seeing a “chunky” histogram. Let’s check&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baked_rec02 %&amp;gt;%
  count(table) %&amp;gt;%
  arrange(desc(n))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10,406 x 2
##     table     n
##     &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
##  1  0.103 12167
##  2 -0.310 11408
##  3 -0.760 11031
##  4  0.494  9406
##  5 -1.28   6726
##  6  0.835  6165
##  7  1.15   3810
##  8 -1.85   2789
##  9  1.42   2182
## 10  1.64    972
## # ... with 10,396 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ooh - okay yeah that’s definitely the issue! I’m not &lt;em&gt;quite&lt;/em&gt; sure how to deal with it, so we’re just going to ignore for now! Let’s add a new model &amp;amp; see how it compares against the baseline transformed model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wf02 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod02) %&amp;gt;%
  add_recipe(rec02)

# stop parallel to avoid error!
# need to replace with PSOCK clusters
# see github issue here: https://github.com/tidymodels/recipes/issues/847
foreach::registerDoSEQ()

set.seed(666) # spoopy
rs02 &amp;lt;-
  fit_resamples(
    wf02,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs02)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator  mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.115    10 0.00141 Preprocessor1_Model1
## 2 rsq     standard   0.932    10 0.00158 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Oof - that’s actually slightly worse than our baseline model!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rs02 %&amp;gt;%
  augment() %&amp;gt;%
  ggplot(aes(x = price,
             y = .pred)) +
  geom_point(alpha = 0.01) +
  geom_abline(linetype = &amp;quot;dashed&amp;quot;,
              size = 0.1,
              alpha = 0.5) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like we’ve introduced structure into the residual plot!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rs02 %&amp;gt;%
  augment() %&amp;gt;%
  ggplot(aes(x = price,
             y = .resid)) +
  geom_point(alpha = 0.01) +
  geom_hline(yintercept = 0,
             linetype = &amp;quot;dashed&amp;quot;,
             size = 0.1,
             alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yeah that’s fairly wonky! I’m wondering if it’s due to the SMOTE upsampling method we introduced? To counteract, I’ll build &amp;amp; train new models after each set of recipe steps (e.g., resampling, normalizing, interactions) to buil up a better performing model one step at a time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics &amp;lt;- 
  metrics %&amp;gt;%
  bind_rows(collect_metrics(rs02) %&amp;gt;% mutate(model = &amp;quot;model02&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# same model spec
mod03 &amp;lt;- mod02

# rebuild rec+wf &amp;amp; retrain
rec03 &amp;lt;- 
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_other(cut, color, clarity) %&amp;gt;%
  step_dummy(all_nominal_predictors(), -cut) %&amp;gt;%
  themis::step_smote(cut)

wf03 &amp;lt;- 
  workflow() %&amp;gt;%
  add_model(mod03) %&amp;gt;%
  add_recipe(rec03)

# do paralllel
doParallel::registerDoParallel()

# refit!
set.seed(123)
rs03 &amp;lt;-
  fit_resamples(
    wf03,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs03)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator   mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.0918    10 0.00502 Preprocessor1_Model1
## 2 rsq     standard   0.956     10 0.00502 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interesting! Improved relative to &lt;code&gt;rs02&lt;/code&gt;, but still not as good as our first model! Let’s try using &lt;code&gt;step_downsample()&lt;/code&gt; to balance classes &amp;amp; see how we fare.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# cleanup some large-ish items eating up memory
rm(mod01, mod02, rec01, rec02, wf01, wf02, rs01, rs02)

# save metrics
metrics &amp;lt;- 
  metrics %&amp;gt;%
  bind_rows(collect_metrics(rs03) %&amp;gt;% mutate(model = &amp;quot;model03&amp;quot;))

# new mod
mod04 &amp;lt;- mod03

# new rec
rec04 &amp;lt;-
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_other(cut, color, clarity) %&amp;gt;%
  step_dummy(all_nominal_predictors(), -cut) %&amp;gt;%
  themis::step_downsample(cut)

wf04 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod04) %&amp;gt;%
  add_recipe(rec04) 

set.seed(456) 
rs04 &amp;lt;-
  fit_resamples(
    wf04,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs04)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator  mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.120    10  0.0150 Preprocessor1_Model1
## 2 rsq     standard   0.921    10  0.0194 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wow - still a bit worse! I’ll try upsampling &amp;amp; if there is no improvement, we’ll move on without resampling!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics &amp;lt;-
  metrics %&amp;gt;%
  bind_rows(collect_metrics(rs04) %&amp;gt;% mutate(model = &amp;quot;model04&amp;quot;))

mod05 &amp;lt;- mod04

rec05 &amp;lt;-
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_other(cut, color, clarity) %&amp;gt;%
  step_dummy(all_nominal_predictors(), -cut) %&amp;gt;%
  themis::step_upsample(cut)

wf05 &amp;lt;- 
  workflow() %&amp;gt;%
  add_model(mod05) %&amp;gt;%
  add_recipe(rec05) 

set.seed(789)
rs05 &amp;lt;-
  fit_resamples(
    wf05,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs05)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator  mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.106    10 0.00618 Preprocessor1_Model1
## 2 rsq     standard   0.941    10 0.00714 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Okay - resampling gets stricken off our list of recipe steps! Let’s look at how the models compare so far&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics &amp;lt;-
  metrics %&amp;gt;%
  bind_rows(collect_metrics(rs05) %&amp;gt;% mutate(model = &amp;quot;model05&amp;quot;))

metrics %&amp;gt;%
  ggplot(aes(x = model)) +
  geom_point(aes(y = mean)) +
  geom_errorbar(aes(ymin = mean - std_err,
                    ymax = mean + std_err)) +
  facet_wrap(~.metric, scales = &amp;quot;free_y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The first simple linear model was the best as measured by both metrics! Let’s see if we can improve with some normalization of the continuous vars.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(mod03, mod04, rec03, rec04, rs03, rs04, wf03, wf04)

mod06 &amp;lt;- mod05

rec06 &amp;lt;-
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_other(cut, color, clarity) %&amp;gt;%
  bestNormalize::step_best_normalize(all_numeric_predictors()) %&amp;gt;%
  step_dummy(all_nominal_predictors())

wf06 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod06) %&amp;gt;%
  add_recipe(rec06)

foreach::registerDoSEQ()
set.seed(101112)
rs06 &amp;lt;-
  fit_resamples(
    wf06,
    diamonds_folds,
    control = ctrl_preds
  )

collect_metrics(rs06)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator  mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.127    10 0.00115 Preprocessor1_Model1
## 2 rsq     standard   0.916    10 0.00136 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well - that was quite a bit for no improvement! I guess that normalizing the continuous vars in this case isn’t helping. Moving on to adding some interactions - first let’s explore potential interactions a bit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics &amp;lt;-
  metrics %&amp;gt;% 
  bind_rows(collect_metrics(rs06) %&amp;gt;% mutate(model = &amp;quot;model06&amp;quot;))

diamonds_train %&amp;gt;%
  ggplot(aes(x = carat,
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) + 
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(splines)
diamonds_train %&amp;gt;%
  ggplot(aes(x = carat,
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = lm,
              formula = y ~ ns(x, df = 5),
              se = FALSE) +
  facet_wrap(~cut, scales = &amp;quot;free&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;5 spline terms might not be sufficient here - capturing the lower bound well but &lt;em&gt;really&lt;/em&gt; not doing well with the higher carat diamonds.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  ggplot(aes(x = carat,
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = lm,
              formula = y ~ ns(x, df = 10),
              se = FALSE) +
  facet_wrap(~cut, scales = &amp;quot;free&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hmmmm, 10 might be too many. It looks lie we’ll just lose a bit of confidence for the Premium &amp;amp; Very Good diamonds at higher carats. Relative to the total number, I’m not too concerned.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  ggplot(aes(x = carat,
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = lm,
              formula = y ~ ns(x, df = 7),
              se = FALSE) +
  facet_wrap(~cut, scales = &amp;quot;free&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;7 terms feels like the best we’re going to do here - I think this is tuneable, but we’ll leave as is (now &amp;amp; in the final model).&lt;/p&gt;
&lt;p&gt;Next, we’ll look at creating interactions between the &lt;code&gt;color&lt;/code&gt; and &lt;code&gt;carat&lt;/code&gt; variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  ggplot(aes(x = carat, 
             y = price,
             color = color)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = lm, 
              formula = y ~ ns(x, df = 15),
              se = FALSE) +
  facet_wrap(~color)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Adding interactive spline terms with &lt;code&gt;df&lt;/code&gt; of 15 seems to add some useful information!&lt;/p&gt;
&lt;p&gt;We have three shape parameters, &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, and &lt;code&gt;z&lt;/code&gt; - I wonder if creating a stand-in for volume by multiplying them all together will provide any useful information?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  mutate(volume_param = x * y * z) %&amp;gt;%
  ggplot(aes(x = volume_param,
             y = price)) +
  geom_point(alpha = 0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ooh, looks like we’re getting some good info here, but we may want to use &lt;code&gt;log10&lt;/code&gt; to scale this back.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  mutate(volume_param = log10(x * y * z)) %&amp;gt;%
  ggplot(aes(x = volume_param, 
             y = price)) +
  geom_point(alpha = 0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s see if this ought to interact with any other paramaters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  mutate(volume_param = log10(x * y * z)) %&amp;gt;%
  ggplot(aes(x = volume_param, 
             y = price,
             color = cut)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-35-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  mutate(volume_param = log10(x * y * z)) %&amp;gt;%
  ggplot(aes(x = volume_param, 
             y = price,
             color = color)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-35-2.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_train %&amp;gt;%
  mutate(volume_param = log10(x * y * z)) %&amp;gt;%
  ggplot(aes(x = volume_param, 
             y = price,
             color = clarity)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-35-3.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hmm, it doesn’t really look like we’re capturing too great of interactions, so I’ll leave out for now. It looks like the &lt;em&gt;size&lt;/em&gt; of the rock is more important than anything else! I could continue to dig further, but I’ll stop there. I’m likely getting diminishing returns, &amp;amp; I’d like to get back into modeling!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod07 &amp;lt;- mod06

rec07 &amp;lt;-
  recipe(price ~ ., data = diamonds_train) %&amp;gt;%
  step_other(cut, color, clarity) %&amp;gt;%
  step_dummy(all_nominal_predictors()) %&amp;gt;%
  step_interact(~carat:starts_with(&amp;quot;cut_&amp;quot;)) %&amp;gt;%
  step_interact(~carat:starts_with(&amp;quot;color_&amp;quot;)) %&amp;gt;%
  step_mutate_at(c(x, y, z),
                 fn = ~if_else(.x == 0, mean(.x), .x)) %&amp;gt;%
  step_mutate(volume_param = log10(x * y * z)) %&amp;gt;%
  step_ns(starts_with(&amp;quot;carat_x_cut&amp;quot;), deg_free = 7) %&amp;gt;%
  step_ns(starts_with(&amp;quot;carat_x_color&amp;quot;), deg_free = 15) 

rec07&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          9
## 
## Operations:
## 
## Collapsing factor levels for cut, color, clarity
## Dummy variables from all_nominal_predictors()
## Interactions with carat:starts_with(&amp;quot;cut_&amp;quot;)
## Interactions with carat:starts_with(&amp;quot;color_&amp;quot;)
## Variable mutation for c(x, y, z)
## Variable mutation for volume_param
## Natural Splines on starts_with(&amp;quot;carat_x_cut&amp;quot;)
## Natural Splines on starts_with(&amp;quot;carat_x_color&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wf07 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod07) %&amp;gt;%
  add_recipe(rec07)

doParallel::registerDoParallel()
set.seed(9876)
rs07 &amp;lt;-
  fit_resamples(
    wf07,
    diamonds_folds,
    control = ctrl_preds
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is definitely going to &lt;em&gt;way&lt;/em&gt; overfit our data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rs07 %&amp;gt;%
  collect_metrics()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator   mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   0.0750    10 0.00377 Preprocessor1_Model1
## 2 rsq     standard   0.970     10 0.00342 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well we (finally) made a modes improvement! Let’s see how the predictions/residuals plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rs07 %&amp;gt;%
  augment() %&amp;gt;%
  ggplot(aes(x = price,
             y = .pred)) +
  geom_point(alpha = 0.05) +
  geom_abline(linetype = &amp;quot;dashed&amp;quot;,
              alpha = 0.5,
              size = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-38-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s pretty good! We do have one value that’s &lt;strong&gt;&lt;em&gt;way&lt;/em&gt;&lt;/strong&gt; off, so let’s see if regulization can help. This will require setting a new baseline model, and we’ll tune our way to the best regularizaion parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics &amp;lt;- 
  rs07 %&amp;gt;%
  collect_metrics() %&amp;gt;%
  mutate(model = &amp;quot;model07&amp;quot;) %&amp;gt;%
  bind_rows(metrics)

# add normalization step
rec08 &amp;lt;- 
  rec07 %&amp;gt;% 
  step_zv(all_numeric_predictors()) %&amp;gt;%
  step_normalize(all_numeric_predictors(),
                 -cut_Ideal, -cut_Premium, -cut_Very.Good, -cut_other,
                 -color_E, -color_F, -color_G, -color_H, -color_I, -color_J,
                 -clarity_SI2, -clarity_VS1, -clarity_VS2, -clarity_VVS1, -clarity_VVS2, -clarity_other)

rm(mod05, mod06, mod07, rec05, rec06, rec07, wf05, wf06, wf07, rs05, rs06, rs07)

mod08 &amp;lt;-
  linear_reg(penalty = tune(), mixture = tune()) %&amp;gt;%
  set_engine(&amp;quot;glmnet&amp;quot;) %&amp;gt;%
  set_mode(&amp;quot;regression&amp;quot;) 

wf08 &amp;lt;-
  workflow() %&amp;gt;%
  add_model(mod08) %&amp;gt;%
  add_recipe(rec08)

diamonds_grid &amp;lt;- 
  grid_regular(penalty(), mixture(), levels = 20)

doParallel::registerDoParallel()
set.seed(5831)
rs08 &amp;lt;-
  tune_grid(
    wf08,
    resamples = diamonds_folds,
    control = ctrl_preds,
    grid = diamonds_grid
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some notes but let’s explore our results…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rs08 %&amp;gt;%
  collect_metrics() %&amp;gt;%
  ggplot(aes(x = penalty,
             y = mean,
             color = as.character(mixture))) +
  geom_point() +
  geom_line(alpha = 0.75) +
  facet_wrap(~.metric, scales = &amp;quot;free&amp;quot;) +
  scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-40-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like we were performing pretty well with the unregularized model, oddly enough! Let’s select the best and finalize our workflow.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_metrics &amp;lt;- 
  rs08 %&amp;gt;%
  select_best(&amp;quot;rmse&amp;quot;)

wf_final &amp;lt;- 
  finalize_workflow(wf08, best_metrics)

rm(mod08, rec07, rec08, rs08, wf08)

set.seed(333)
final_fit &amp;lt;- 
  wf_final %&amp;gt;%
  fit(diamonds_train)

final_fit %&amp;gt;%
  predict(diamonds_test) %&amp;gt;%
  bind_cols(diamonds_test) %&amp;gt;%
  select(price, .pred) %&amp;gt;%
  ggplot(aes(x = price, 
             y = .pred)) +
  geom_point(alpha = 0.05) + 
  geom_abline(alpha = 0.5,
              linetype = &amp;quot;dashed&amp;quot;,
              size = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-41-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What are the most important variables in this regularized model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_fit %&amp;gt;%
  pull_workflow_fit() %&amp;gt;%
  vi(lambda = best_metrics$penalty) %&amp;gt;%
  mutate(Variable = fct_reorder(Variable, Importance)) %&amp;gt;%
  ggplot(aes(x = Variable,
             y = Importance, 
             fill = Sign)) +
  geom_col() +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-42-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, most of our terms get regularized away, which is what we want! Our chart is a little unreadable; let’s plot just the most important variables in a few ways:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_fit %&amp;gt;%
  pull_workflow_fit() %&amp;gt;%
  vi(lambda = best_metrics$penalty) %&amp;gt;%
  arrange(desc(Importance)) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  mutate(Variable = fct_reorder(Variable, Importance)) %&amp;gt;%
  ggplot(aes(x = Variable,
             y = Importance,
             fill = Sign)) +
  geom_col() +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-43-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_fit %&amp;gt;%
  pull_workflow_fit() %&amp;gt;%
  vi(lambda = best_metrics$penalty) %&amp;gt;%
  arrange(desc(Importance)) %&amp;gt;% 
  slice_head(n = 10) %&amp;gt;%
  mutate(Importance = if_else(Sign == &amp;quot;NEG&amp;quot;, -Importance, Importance),
         Variable = fct_reorder(Variable, Importance)) %&amp;gt;%
  ggplot(aes(x = Variable,
             y = Importance,
             fill = Sign)) + 
  geom_col() +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-44-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And look at that! Our most important variable was one that came from feature engineering! The size of the rock had the biggest impact on price.&lt;/p&gt;
&lt;p&gt;We’ve gone through a lot of steps, so it may be good to look back on what was done:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explored our dataset via some simple exploratory data analysis;&lt;/li&gt;
&lt;li&gt;Fit a simple linear model to predict the log-transform of price;&lt;/li&gt;
&lt;li&gt;Attempted (and failed) to improve upon the simple model with fancier normalization and resampling techniques;&lt;/li&gt;
&lt;li&gt;Explored the dataset further to find meaningful interactions and potential new features;&lt;/li&gt;
&lt;li&gt;Fit a new model with feature engineering;&lt;/li&gt;
&lt;li&gt;Tuned regularization parameters on our model with feature engineering to arrive at the final model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our models’ performances, ranked from best to worst, show that the final tuned model did indeed perform the best on the test dataset!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics %&amp;gt;%
  select(.metric, mean, model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 14 x 3
##    .metric   mean model  
##    &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  
##  1 rmse    0.0750 model07
##  2 rsq     0.970  model07
##  3 rmse    0.0793 model01
##  4 rsq     0.966  model01
##  5 rmse    0.115  model02
##  6 rsq     0.932  model02
##  7 rmse    0.0918 model03
##  8 rsq     0.956  model03
##  9 rmse    0.120  model04
## 10 rsq     0.921  model04
## 11 rmse    0.106  model05
## 12 rsq     0.941  model05
## 13 rmse    0.127  model06
## 14 rsq     0.916  model06&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_preds &amp;lt;-
  final_fit %&amp;gt;%
  predict(diamonds_train) %&amp;gt;%
  bind_cols(diamonds_train) %&amp;gt;%
  select(price, .pred)

bind_rows(final_preds %&amp;gt;% rmse(price, .pred),
          final_preds %&amp;gt;% rsq(price, .pred)) %&amp;gt;%
  rename(mean = .estimate) %&amp;gt;%
  select(-.estimator) %&amp;gt;%
  mutate(model = &amp;quot;model_final&amp;quot;) %&amp;gt;%
  bind_rows(metrics %&amp;gt;% select(.metric, mean, model)) %&amp;gt;%
  pivot_wider(names_from = .metric,
              values_from = mean) %&amp;gt;%
  mutate(model = fct_reorder(model, desc(rmse))) %&amp;gt;%
  pivot_longer(rmse:rsq,
               names_to = &amp;quot;metric&amp;quot;,
               values_to = &amp;quot;value&amp;quot;) %&amp;gt;%
  ggplot(aes(x = model,
             y = value)) +
  geom_point() +
  facet_wrap(~metric, scales = &amp;quot;free&amp;quot;) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index_files/figure-html/unnamed-chunk-45-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
