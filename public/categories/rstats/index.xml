<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>rstats on the data diary</title>
    <link>https://www.thedatadiary.net/categories/rstats/</link>
    <description>Recent content in rstats on the data diary</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Mark Rieke {year}</copyright>
    <lastBuildDate>Sat, 20 Aug 2022 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://www.thedatadiary.net/categories/rstats/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Finding new wedding bops with {tidyclust} and {spotifyr}</title>
      <link>https://www.thedatadiary.net/blog/2022-08-20-finding-new-wedding-bops-with-tidyclust-and-spotifyr/</link>
      <pubDate>Sat, 20 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-08-20-finding-new-wedding-bops-with-tidyclust-and-spotifyr/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-08-20-finding-new-wedding-bops-with-tidyclust-and-spotifyr/index_files/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-08-20-finding-new-wedding-bops-with-tidyclust-and-spotifyr/index_files/d3-bundle/d3-bundle.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-08-20-finding-new-wedding-bops-with-tidyclust-and-spotifyr/index_files/d3-lasso/d3-lasso.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-08-20-finding-new-wedding-bops-with-tidyclust-and-spotifyr/index_files/save-svg-as-png/save-svg-as-png.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://www.thedatadiary.net/blog/2022-08-20-finding-new-wedding-bops-with-tidyclust-and-spotifyr/index_files/ggiraphjs/ggiraphjs.min.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-08-20-finding-new-wedding-bops-with-tidyclust-and-spotifyr/index_files/ggiraphjs/ggiraphjs.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-08-20-finding-new-wedding-bops-with-tidyclust-and-spotifyr/index_files/girafe-binding/girafe.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Last November, I (finally) popped the big question and &lt;a href=&#34;https://www.instagram.com/p/Cc6ReimO2tH/?igshid=YmMyMTA2M2Y=&#34;&gt;proposed&lt;/a&gt;! Since then, my fiance and I have been diligently planning our wedding. While we have most of the big-ticket items checked off (venue, catering, photography, etc.), one area we still have more work to do is on the wedding playlist. We’ve &lt;a href=&#34;https://open.spotify.com/playlist/66saUDfW5ggYD6JDAHbxyV&#34;&gt;started putting together a playlist on spotify&lt;/a&gt;, but it feels like it’s come to a bit of a stand-still. Currently, there’s a mix of zesty bops and tame songs on the playlist (we need to accommodate both our college friends and our grandparents!), but spotify’s track recommender only wants to suggest tamer songs right now. Our goal is to have a full dance floor the entire night — to achieve this, we can use &lt;a href=&#34;https://www.rcharlie.com/spotifyr/index.html&#34;&gt;spotifyr&lt;/a&gt; and the new &lt;a href=&#34;https://emilhvitfeldt.github.io/tidyclust/index.html&#34;&gt;tidyclust&lt;/a&gt; package to pull in the current playlist, cluster the songs based on their features, and find new songs based on the bop cluster.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels)
library(tidyclust)
library(spotifyr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you’d like to follow along, I’d recommend installing the development versions of &lt;a href=&#34;https://github.com/tidymodels/parsnip&#34;&gt;parsnip&lt;/a&gt; and &lt;a href=&#34;https://github.com/tidymodels/workflows&#34;&gt;workflows&lt;/a&gt;, as some of the functionality that interacts with tidyclust &lt;a href=&#34;https://emilhvitfeldt.github.io/tidyclust/articles/k_means.html#setup&#34;&gt;isn’t yet on CRAN&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;pulling-in-the-playlist&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pulling in the playlist&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.rcharlie.com/spotifyr/index.html&#34;&gt;spotifyr&lt;/a&gt; is an R interface to spotify’s web API and gives access to a host of track features (you can follow &lt;a href=&#34;https://msmith7161.github.io/what-is-speechiness/&#34;&gt;this tutorial&lt;/a&gt; to get it setup). I’ll use the functions &lt;code&gt;get_user_playlists()&lt;/code&gt; and &lt;code&gt;get_playlist_tracks()&lt;/code&gt; to pull in songs that are currently on our wedding playlist (appropriately named “Ding dong”).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get the songs that are currently on the wedding playlist
ding_dong &amp;lt;- 
  get_user_playlists(&amp;quot;12130039175&amp;quot;) %&amp;gt;%
  filter(name == &amp;quot;Ding dong&amp;quot;) %&amp;gt;%
  pull(id) %&amp;gt;%
  get_playlist_tracks() %&amp;gt;% 
  as_tibble() %&amp;gt;%
  select(track.id, track.name, track.popularity) %&amp;gt;%
  rename_with(~stringr::str_replace(.x, &amp;quot;\\.&amp;quot;, &amp;quot;_&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;31%&#34; /&gt;
&lt;col width=&#34;45%&#34; /&gt;
&lt;col width=&#34;23%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;track_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;track_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;track_popularity&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5jkFvD4UJrmdoezzT1FRoP&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Rasputin&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;65&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1D066zixBwqFYqBhKgdPzp&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Fergalicious&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;71&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;12jjuxN1gxlm29cqL5M6MW&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;I Got You&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;65&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2grjqo0Frpf2okIBiifQKs&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;September&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;81&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2RlgNHKcydI9sayD2Df2xp&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Mr. Blue Sky&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;80&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6x4tKaOzfNJpEJHySoiJcs&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Mambo No. 5 (a Little Bit of…)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;77&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3n3Ppam7vgaVa1iaRUc9Lp&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Mr. Brightside&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;66&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;7Cp69rNBwU0gaFT8zxExlE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Ymca&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3Gf5nttwcX9aaSQXRWidEZ&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Ride Wit Me&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;76&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3wMUvT6eIw2L5cZFG1yH9j&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Country Grammar (Hot Shit)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;70&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Spotify estimates quite a few &lt;a href=&#34;https://developer.spotify.com/documentation/web-api/reference/#/operations/get-several-audio-features&#34;&gt;features&lt;/a&gt; for each song in their catalog: speechiness (the presence of words on a track), acousticness (whether or not a song includes acoustic instruments), liveness (estimates whether or not the track is live or studio-recorded), etc. We can use &lt;code&gt;get_track_audio_features()&lt;/code&gt; to get the features for each song based on its &lt;code&gt;track_id&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# pull in track features of songs on the playlist
track_features &amp;lt;- 
  ding_dong %&amp;gt;%
  pull(track_id) %&amp;gt;%
  get_track_audio_features()

# join together
ding_dong &amp;lt;- 
  ding_dong %&amp;gt;%
  left_join(track_features, by = c(&amp;quot;track_id&amp;quot; = &amp;quot;id&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In my case, I’m interested in the energy and valence (positivity) of each song, so I’ll select these variables to use in the cluster analysis.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;track_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;valence&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;energy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Rasputin&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.966&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.893&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fergalicious&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.829&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.583&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;I Got You&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.544&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.399&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;September&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.979&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.832&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mr. Blue Sky&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.478&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.338&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mambo No. 5 (a Little Bit of…)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.892&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.807&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mr. Brightside&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.240&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.918&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Ymca&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.671&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.951&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Ride Wit Me&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.722&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.700&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Country Grammar (Hot Shit)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.565&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.664&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;clustering-with-tidyclust&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Clustering with &lt;code&gt;tidyclust&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Currently, the playlist covers a wide spectrum of songs. For new songs on the playlist, I’m really just interested in songs similar to others in the top right corner of the below chart with high energy and valence.&lt;/p&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:4500px;height:3000px;&#34; class=&#34;girafe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;html&#34;:&#34;&lt;?xml version=\&#34;1.0\&#34; encoding=\&#34;UTF-8\&#34;?&gt;\n&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; xmlns:xlink=&#39;http://www.w3.org/1999/xlink&#39; id=&#39;svg_48bb14c9-3e03-46b9-9899-861cf949c068&#39; viewBox=&#39;0 0 432 360&#39;&gt;\n &lt;defs&gt;\n  &lt;clipPath id=&#39;svg_48bb14c9-3e03-46b9-9899-861cf949c068_c1&#39;&gt;\n   &lt;rect x=&#39;0&#39; y=&#39;0&#39; width=&#39;432&#39; height=&#39;360&#39;/&gt;\n  &lt;\/clipPath&gt;\n  &lt;clipPath id=&#39;svg_48bb14c9-3e03-46b9-9899-861cf949c068_c2&#39;&gt;\n   &lt;rect x=&#39;55.03&#39; y=&#39;49.4&#39; width=&#39;370&#39; height=&#39;270.57&#39;/&gt;\n  &lt;\/clipPath&gt;\n &lt;\/defs&gt;\n &lt;g&gt;\n  &lt;g clip-path=&#39;url(#svg_48bb14c9-3e03-46b9-9899-861cf949c068_c1)&#39;&gt;\n   &lt;rect x=&#39;0&#39; y=&#39;0&#39; width=&#39;432&#39; height=&#39;360&#39; fill=&#39;#FFFFFF&#39; stroke=&#39;#FFFFFF&#39; stroke-width=&#39;0.75&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39;/&gt;\n   &lt;rect x=&#39;0&#39; y=&#39;0&#39; width=&#39;432&#39; height=&#39;360&#39; fill=&#39;#FFFFFF&#39; stroke=&#39;#FFFFFF&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39;/&gt;\n  &lt;\/g&gt;\n  &lt;g clip-path=&#39;url(#svg_48bb14c9-3e03-46b9-9899-861cf949c068_c2)&#39;&gt;\n   &lt;polyline points=&#39;55.03,245.02 425.03,245.02&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,167.67 425.03,167.67&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,90.32 425.03,90.32&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;155.20,319.97 155.20,49.40&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;259.92,319.97 259.92,49.40&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;364.64,319.97 364.64,49.40&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,283.69 425.03,283.69&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,206.34 425.03,206.34&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,128.99 425.03,128.99&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,51.64 425.03,51.64&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;102.84,319.97 102.84,49.40&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;207.56,319.97 207.56,49.40&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;312.28,319.97 312.28,49.40&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;417.00,319.97 417.00,49.40&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;circle cx=&#39;402.76&#39; cy=&#39;93.02&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Rasputin&#39;/&gt;\n   &lt;circle cx=&#39;345.38&#39; cy=&#39;212.92&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Fergalicious&#39;/&gt;\n   &lt;circle cx=&#39;225.99&#39; cy=&#39;284.08&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;I Got You&#39;/&gt;\n   &lt;circle cx=&#39;408.21&#39; cy=&#39;116.62&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;September&#39;/&gt;\n   &lt;circle cx=&#39;198.35&#39; cy=&#39;307.67&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Mr. Blue Sky&#39;/&gt;\n   &lt;circle cx=&#39;371.77&#39; cy=&#39;126.28&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Mambo No. 5 (a Little Bit of...)&#39;/&gt;\n   &lt;circle cx=&#39;98.65&#39; cy=&#39;83.35&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Mr. Brightside&#39;/&gt;\n   &lt;circle cx=&#39;279.19&#39; cy=&#39;70.59&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Ymca&#39;/&gt;\n   &lt;circle cx=&#39;300.56&#39; cy=&#39;167.67&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Ride Wit Me&#39;/&gt;\n   &lt;circle cx=&#39;234.79&#39; cy=&#39;181.59&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Country Grammar (Hot Shit)&#39;/&gt;\n   &lt;circle cx=&#39;172.38&#39; cy=&#39;103.47&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Shout, Pts. 1 &amp;amp;amp; 2&#39;/&gt;\n   &lt;circle cx=&#39;125.46&#39; cy=&#39;202.86&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Low (feat. T-Pain)&#39;/&gt;\n   &lt;circle cx=&#39;325.27&#39; cy=&#39;183.91&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Come On Eileen&#39;/&gt;\n   &lt;circle cx=&#39;259.92&#39; cy=&#39;178.5&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Gold Digger&#39;/&gt;\n   &lt;circle cx=&#39;330.71&#39; cy=&#39;76.01&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Fireball (feat. John Ryan)&#39;/&gt;\n   &lt;circle cx=&#39;312.7&#39; cy=&#39;134.41&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Bang Bang&#39;/&gt;\n   &lt;circle cx=&#39;352.92&#39; cy=&#39;81.81&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Can&amp;amp;#39;t Hold Us (feat. Ray Dalton)&#39;/&gt;\n   &lt;circle cx=&#39;249.87&#39; cy=&#39;103.85&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Don&amp;amp;#39;t Stop Me Now - Remastered 2011&#39;/&gt;\n   &lt;circle cx=&#39;365.06&#39; cy=&#39;89.54&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Take on Me&#39;/&gt;\n   &lt;circle cx=&#39;350.82&#39; cy=&#39;143.69&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Uncertain Smile&#39;/&gt;\n   &lt;circle cx=&#39;205.47&#39; cy=&#39;61.7&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Everytime We Touch&#39;/&gt;\n   &lt;circle cx=&#39;400.67&#39; cy=&#39;146.78&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;It&amp;amp;#39;s Tricky&#39;/&gt;\n   &lt;circle cx=&#39;272.49&#39; cy=&#39;211.76&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Brass Monkey&#39;/&gt;\n   &lt;circle cx=&#39;263.69&#39; cy=&#39;282.53&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Wagon Wheel&#39;/&gt;\n   &lt;circle cx=&#39;270.81&#39; cy=&#39;148.72&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Love On Top&#39;/&gt;\n   &lt;circle cx=&#39;391.87&#39; cy=&#39;158.77&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;I Think We&amp;amp;#39;re Alone Now&#39;/&gt;\n   &lt;circle cx=&#39;361.29&#39; cy=&#39;119.71&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;I Wanna Dance with Somebody (Who Loves Me)&#39;/&gt;\n   &lt;circle cx=&#39;386.84&#39; cy=&#39;202.86&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Uptown Funk (feat. Bruno Mars)&#39;/&gt;\n   &lt;circle cx=&#39;112.06&#39; cy=&#39;212.53&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Single Ladies (Put a Ring on It)&#39;/&gt;\n   &lt;circle cx=&#39;392.71&#39; cy=&#39;128.99&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Shake It Off&#39;/&gt;\n   &lt;circle cx=&#39;340.77&#39; cy=&#39;163.8&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Jump Around&#39;/&gt;\n   &lt;circle cx=&#39;315.63&#39; cy=&#39;119.71&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Hips Don&amp;amp;#39;t Lie (feat. Wyclef Jean)&#39;/&gt;\n   &lt;circle cx=&#39;241.91&#39; cy=&#39;142.91&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Body (feat. Brando)&#39;/&gt;\n   &lt;circle cx=&#39;285.89&#39; cy=&#39;115.84&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Jackie Chan&#39;/&gt;\n   &lt;circle cx=&#39;151.85&#39; cy=&#39;214.85&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Wake Up in the Sky&#39;/&gt;\n   &lt;circle cx=&#39;187.46&#39; cy=&#39;228&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Summer, Highland Falls - Live at the Bayou, Washington, D.C. - July 1980&#39;/&gt;\n   &lt;circle cx=&#39;297.2&#39; cy=&#39;108.11&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Magic Dance&#39;/&gt;\n   &lt;circle cx=&#39;315.22&#39; cy=&#39;95.73&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Dancing with Myself - 2001 Remaster&#39;/&gt;\n   &lt;circle cx=&#39;271.65&#39; cy=&#39;71.37&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Waterloo&#39;/&gt;\n   &lt;circle cx=&#39;231.02&#39; cy=&#39;90.7&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Potential Breakup Song&#39;/&gt;\n   &lt;circle cx=&#39;381.4&#39; cy=&#39;119.32&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Levitating (feat. DaBaby)&#39;/&gt;\n   &lt;circle cx=&#39;184.52&#39; cy=&#39;121.64&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Forever&#39;/&gt;\n   &lt;circle cx=&#39;344.54&#39; cy=&#39;135.95&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;White Walls (feat. ScHoolboy Q &amp;amp;amp; Hollis)&#39;/&gt;\n   &lt;circle cx=&#39;253.22&#39; cy=&#39;85.68&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Rose Tattoo&#39;/&gt;\n   &lt;circle cx=&#39;197.09&#39; cy=&#39;85.68&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Kiss Me I&amp;amp;#39;m #!@&amp;amp;#39;faced&#39;/&gt;\n   &lt;circle cx=&#39;162.32&#39; cy=&#39;137.89&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;All Night (feat. Knox Fortune)&#39;/&gt;\n   &lt;circle cx=&#39;282.54&#39; cy=&#39;131.7&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Don&amp;amp;#39;t Start Now&#39;/&gt;\n   &lt;circle cx=&#39;380.56&#39; cy=&#39;106.56&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;The Legend of Chavo Guerrero&#39;/&gt;\n   &lt;circle cx=&#39;398.57&#39; cy=&#39;168.83&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Bam Bam (feat. Ed Sheeran)&#39;/&gt;\n   &lt;circle cx=&#39;228.51&#39; cy=&#39;142.14&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;The Spins&#39;/&gt;\n   &lt;circle cx=&#39;167.77&#39; cy=&#39;188.16&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Hung Up&#39;/&gt;\n   &lt;circle cx=&#39;355.43&#39; cy=&#39;95.34&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;BREAK MY SOUL&#39;/&gt;\n   &lt;circle cx=&#39;270.81&#39; cy=&#39;190.1&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Post Malone (feat. RANI)&#39;/&gt;\n   &lt;circle cx=&#39;218.87&#39; cy=&#39;181.98&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Take Me Out&#39;/&gt;\n   &lt;circle cx=&#39;200.86&#39; cy=&#39;90.32&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Sk8er Boi&#39;/&gt;\n   &lt;circle cx=&#39;328.2&#39; cy=&#39;65.95&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Timber (feat. Ke$ha)&#39;/&gt;\n   &lt;circle cx=&#39;301.39&#39; cy=&#39;128.22&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Time of Our Lives&#39;/&gt;\n   &lt;circle cx=&#39;192.48&#39; cy=&#39;94.57&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Levels - Radio Edit&#39;/&gt;\n   &lt;circle cx=&#39;285.89&#39; cy=&#39;157.61&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Body On My&#39;/&gt;\n   &lt;circle cx=&#39;402.34&#39; cy=&#39;61.7&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Hey Ya!&#39;/&gt;\n   &lt;circle cx=&#39;297.2&#39; cy=&#39;114.68&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;TiK ToK&#39;/&gt;\n   &lt;circle cx=&#39;401.92&#39; cy=&#39;212.92&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;SexyBack (feat. Timbaland)&#39;/&gt;\n   &lt;circle cx=&#39;315.63&#39; cy=&#39;86.84&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Nice For What&#39;/&gt;\n   &lt;circle cx=&#39;341.19&#39; cy=&#39;130.93&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Up&#39;/&gt;\n   &lt;circle cx=&#39;71.84&#39; cy=&#39;136.34&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Clarity&#39;/&gt;\n   &lt;circle cx=&#39;286.31&#39; cy=&#39;181.59&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;good 4 u&#39;/&gt;\n   &lt;circle cx=&#39;373.44&#39; cy=&#39;147.17&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Cake By The Ocean&#39;/&gt;\n   &lt;circle cx=&#39;124.21&#39; cy=&#39;134.02&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Titanium (feat. Sia)&#39;/&gt;\n   &lt;circle cx=&#39;229.34&#39; cy=&#39;99.6&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Where Them Girls At (feat. Nicki Minaj &amp;amp;amp; Flo Rida)&#39;/&gt;\n   &lt;circle cx=&#39;330.3&#39; cy=&#39;155.68&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;I Know You Want Me (Calle Ocho)&#39;/&gt;\n   &lt;circle cx=&#39;196.25&#39; cy=&#39;102.31&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Club Can&amp;amp;#39;t Handle Me (feat. David Guetta)&#39;/&gt;\n   &lt;circle cx=&#39;343.28&#39; cy=&#39;88&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;I Love It (feat. Charli XCX)&#39;/&gt;\n   &lt;circle cx=&#39;390.61&#39; cy=&#39;170.76&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Treasure&#39;/&gt;\n   &lt;circle cx=&#39;357.52&#39; cy=&#39;70.98&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;She Bangs - English Version&#39;/&gt;\n   &lt;circle cx=&#39;327.78&#39; cy=&#39;77.94&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Magic (feat. Rivers Cuomo)&#39;/&gt;\n   &lt;circle cx=&#39;385.17&#39; cy=&#39;114.29&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Toxic&#39;/&gt;\n   &lt;circle cx=&#39;220.13&#39; cy=&#39;75.23&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Give Me Everything (feat. Ne-Yo, Afrojack &amp;amp;amp; Nayer)&#39;/&gt;\n   &lt;circle cx=&#39;342.44&#39; cy=&#39;206.34&#39; r=&#39;3.07pt&#39; fill=&#39;#000000&#39; fill-opacity=&#39;0.5&#39; stroke=&#39;#000000&#39; stroke-opacity=&#39;0.5&#39; title=&#39;Temperature&#39;/&gt;\n  &lt;\/g&gt;\n  &lt;g clip-path=&#39;url(#svg_48bb14c9-3e03-46b9-9899-861cf949c068_c1)&#39;&gt;\n   &lt;text x=&#39;27.91&#39; y=&#39;287.67&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;40%&lt;\/text&gt;\n   &lt;text x=&#39;28.14&#39; y=&#39;210.32&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;60%&lt;\/text&gt;\n   &lt;text x=&#39;28.22&#39; y=&#39;132.97&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;80%&lt;\/text&gt;\n   &lt;text x=&#39;23.41&#39; y=&#39;55.62&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;100%&lt;\/text&gt;\n   &lt;text x=&#39;92.81&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;25%&lt;\/text&gt;\n   &lt;text x=&#39;197.43&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;50%&lt;\/text&gt;\n   &lt;text x=&#39;302.26&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;75%&lt;\/text&gt;\n   &lt;text x=&#39;404.33&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;100%&lt;\/text&gt;\n   &lt;text x=&#39;214.33&#39; y=&#39;350.04&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;valence&lt;\/text&gt;\n   &lt;text transform=&#39;translate(16.93,207.45) rotate(-90.00)&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;energy&lt;\/text&gt;\n   &lt;text x=&#39;6.97&#39; y=&#39;39.44&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;Hover&lt;\/text&gt;\n   &lt;text x=&#39;50.24&#39; y=&#39;39.44&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;over&lt;\/text&gt;\n   &lt;text x=&#39;82.72&#39; y=&#39;39.44&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;each&lt;\/text&gt;\n   &lt;text x=&#39;117.45&#39; y=&#39;39.44&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;point&lt;\/text&gt;\n   &lt;text x=&#39;155.42&#39; y=&#39;39.44&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;to&lt;\/text&gt;\n   &lt;text x=&#39;171.57&#39; y=&#39;39.44&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;see&lt;\/text&gt;\n   &lt;text x=&#39;196.5&#39; y=&#39;39.44&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;the&lt;\/text&gt;\n   &lt;text x=&#39;221.19&#39; y=&#39;39.44&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;song&#39;s&lt;\/text&gt;\n   &lt;text x=&#39;266.55&#39; y=&#39;39.44&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;name!&lt;\/text&gt;\n   &lt;text x=&#39;6.97&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;The&lt;\/text&gt;\n   &lt;text x=&#39;42.15&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;current&lt;\/text&gt;\n   &lt;text x=&#39;104.99&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;wedding&lt;\/text&gt;\n   &lt;text x=&#39;178.64&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;playlist&lt;\/text&gt;\n  &lt;\/g&gt;\n &lt;\/g&gt;\n&lt;\/svg&gt;&#34;,&#34;js&#34;:null,&#34;uid&#34;:&#34;svg_48bb14c9-3e03-46b9-9899-861cf949c068&#34;,&#34;ratio&#34;:1.2,&#34;settings&#34;:{&#34;tooltip&#34;:{&#34;css&#34;:&#34;.tooltip_SVGID_ { background-color:gray;color:white;padding:2px;border-radius:2px;font-family:Roboto Slab; ; position:absolute;pointer-events:none;z-index:999;}&#34;,&#34;placement&#34;:&#34;doc&#34;,&#34;offx&#34;:10,&#34;offy&#34;:0,&#34;use_cursor_pos&#34;:true,&#34;opacity&#34;:0.8,&#34;usefill&#34;:false,&#34;usestroke&#34;:false,&#34;delay&#34;:{&#34;over&#34;:200,&#34;out&#34;:500}},&#34;hover&#34;:{&#34;css&#34;:&#34;.hover_SVGID_ { fill:#1279BF;stroke:#1279BF;cursor:pointer; }&#34;,&#34;reactive&#34;:false},&#34;hoverkey&#34;:{&#34;css&#34;:&#34;.hover_key_SVGID_ { stroke:red; }&#34;,&#34;reactive&#34;:false},&#34;hovertheme&#34;:{&#34;css&#34;:&#34;.hover_theme_SVGID_ { fill:green; }&#34;,&#34;reactive&#34;:false},&#34;hoverinv&#34;:{&#34;css&#34;:&#34;&#34;},&#34;zoom&#34;:{&#34;min&#34;:1,&#34;max&#34;:1},&#34;capture&#34;:{&#34;css&#34;:&#34;.selected_SVGID_ { fill:red;stroke:gray; }&#34;,&#34;type&#34;:&#34;multiple&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;capturekey&#34;:{&#34;css&#34;:&#34;.selected_key_SVGID_ { stroke:gray; }&#34;,&#34;type&#34;:&#34;single&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;capturetheme&#34;:{&#34;css&#34;:&#34;.selected_theme_SVGID_ { stroke:gray; }&#34;,&#34;type&#34;:&#34;single&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;toolbar&#34;:{&#34;position&#34;:&#34;topright&#34;,&#34;saveaspng&#34;:true,&#34;pngname&#34;:&#34;diagram&#34;},&#34;sizing&#34;:{&#34;rescale&#34;:true,&#34;width&#34;:1}}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Broadly, there are three generic categories that the songs on the current playlist fall into: high energy and valence, low energy, or low valence (songs with low energy and valence will fall into one of the “low” categories). Rather than manually assign categories, we can use tidyclust to cluster the songs into three groups using the &lt;a href=&#34;https://en.wikipedia.org/wiki/K-means_clustering&#34;&gt;kmeans&lt;/a&gt; algorithm.&lt;/p&gt;
&lt;p&gt;There’s some &lt;a href=&#34;https://emilhvitfeldt.github.io/tidyclust/articles/k_means.html&#34;&gt;great documentation on the tidyclust site&lt;/a&gt;, but to get started, we’ll categorize the songs on the current playlist by “fitting” a kmeans model (using the &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/stats/html/00Index.html&#34;&gt;stats&lt;/a&gt; engine under the hood).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create a clustering obj
set.seed(918)
ding_dong_clusters &amp;lt;- 
  k_means(num_clusters = 3) %&amp;gt;%
  fit(~ valence + energy,
      data = ding_dong) &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:4500px;height:3000px;&#34; class=&#34;girafe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;html&#34;:&#34;&lt;?xml version=\&#34;1.0\&#34; encoding=\&#34;UTF-8\&#34;?&gt;\n&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; xmlns:xlink=&#39;http://www.w3.org/1999/xlink&#39; id=&#39;svg_5bdb2b70-1919-4a89-9edf-cf363f034ebd&#39; viewBox=&#39;0 0 432 360&#39;&gt;\n &lt;defs&gt;\n  &lt;clipPath id=&#39;svg_5bdb2b70-1919-4a89-9edf-cf363f034ebd_c1&#39;&gt;\n   &lt;rect x=&#39;0&#39; y=&#39;0&#39; width=&#39;432&#39; height=&#39;360&#39;/&gt;\n  &lt;\/clipPath&gt;\n  &lt;clipPath id=&#39;svg_5bdb2b70-1919-4a89-9edf-cf363f034ebd_c2&#39;&gt;\n   &lt;rect x=&#39;55.03&#39; y=&#39;49.46&#39; width=&#39;370&#39; height=&#39;270.51&#39;/&gt;\n  &lt;\/clipPath&gt;\n &lt;\/defs&gt;\n &lt;g&gt;\n  &lt;g clip-path=&#39;url(#svg_5bdb2b70-1919-4a89-9edf-cf363f034ebd_c1)&#39;&gt;\n   &lt;rect x=&#39;0&#39; y=&#39;0&#39; width=&#39;432&#39; height=&#39;360&#39; fill=&#39;#FFFFFF&#39; stroke=&#39;#FFFFFF&#39; stroke-width=&#39;0.75&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39;/&gt;\n   &lt;rect x=&#39;0&#39; y=&#39;0&#39; width=&#39;432&#39; height=&#39;360&#39; fill=&#39;#FFFFFF&#39; stroke=&#39;#FFFFFF&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39;/&gt;\n  &lt;\/g&gt;\n  &lt;g clip-path=&#39;url(#svg_5bdb2b70-1919-4a89-9edf-cf363f034ebd_c2)&#39;&gt;\n   &lt;polyline points=&#39;55.03,245.04 425.03,245.04&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,167.70 425.03,167.70&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,90.37 425.03,90.37&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;155.20,319.97 155.20,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;259.92,319.97 259.92,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;364.64,319.97 364.64,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,283.70 425.03,283.70&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,206.37 425.03,206.37&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,129.04 425.03,129.04&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;55.03,51.70 425.03,51.70&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;102.84,319.97 102.84,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;207.56,319.97 207.56,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;312.28,319.97 312.28,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;417.00,319.97 417.00,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;circle cx=&#39;402.76&#39; cy=&#39;93.08&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Rasputin&#39;/&gt;\n   &lt;circle cx=&#39;345.38&#39; cy=&#39;212.94&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Fergalicious&#39;/&gt;\n   &lt;circle cx=&#39;225.99&#39; cy=&#39;284.09&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;I Got You&#39;/&gt;\n   &lt;circle cx=&#39;408.21&#39; cy=&#39;116.66&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;September&#39;/&gt;\n   &lt;circle cx=&#39;198.35&#39; cy=&#39;307.67&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Mr. Blue Sky&#39;/&gt;\n   &lt;circle cx=&#39;371.77&#39; cy=&#39;126.33&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Mambo No. 5 (a Little Bit of...)&#39;/&gt;\n   &lt;circle cx=&#39;98.65&#39; cy=&#39;83.41&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Mr. Brightside&#39;/&gt;\n   &lt;circle cx=&#39;279.19&#39; cy=&#39;70.65&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Ymca&#39;/&gt;\n   &lt;circle cx=&#39;300.56&#39; cy=&#39;167.7&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Ride Wit Me&#39;/&gt;\n   &lt;circle cx=&#39;234.79&#39; cy=&#39;181.62&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Country Grammar (Hot Shit)&#39;/&gt;\n   &lt;circle cx=&#39;172.38&#39; cy=&#39;103.52&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Shout, Pts. 1 &amp;amp;amp; 2&#39;/&gt;\n   &lt;circle cx=&#39;125.46&#39; cy=&#39;202.89&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Low (feat. T-Pain)&#39;/&gt;\n   &lt;circle cx=&#39;325.27&#39; cy=&#39;183.94&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Come On Eileen&#39;/&gt;\n   &lt;circle cx=&#39;259.92&#39; cy=&#39;178.53&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Gold Digger&#39;/&gt;\n   &lt;circle cx=&#39;330.71&#39; cy=&#39;76.06&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Fireball (feat. John Ryan)&#39;/&gt;\n   &lt;circle cx=&#39;312.7&#39; cy=&#39;134.45&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Bang Bang&#39;/&gt;\n   &lt;circle cx=&#39;352.92&#39; cy=&#39;81.86&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Can&amp;amp;#39;t Hold Us (feat. Ray Dalton)&#39;/&gt;\n   &lt;circle cx=&#39;249.87&#39; cy=&#39;103.9&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Don&amp;amp;#39;t Stop Me Now - Remastered 2011&#39;/&gt;\n   &lt;circle cx=&#39;365.06&#39; cy=&#39;89.6&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Take on Me&#39;/&gt;\n   &lt;circle cx=&#39;350.82&#39; cy=&#39;143.73&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Uncertain Smile&#39;/&gt;\n   &lt;circle cx=&#39;205.47&#39; cy=&#39;61.76&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Everytime We Touch&#39;/&gt;\n   &lt;circle cx=&#39;400.67&#39; cy=&#39;146.82&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;It&amp;amp;#39;s Tricky&#39;/&gt;\n   &lt;circle cx=&#39;272.49&#39; cy=&#39;211.78&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Brass Monkey&#39;/&gt;\n   &lt;circle cx=&#39;263.69&#39; cy=&#39;282.54&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Wagon Wheel&#39;/&gt;\n   &lt;circle cx=&#39;270.81&#39; cy=&#39;148.76&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Love On Top&#39;/&gt;\n   &lt;circle cx=&#39;391.87&#39; cy=&#39;158.81&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;I Think We&amp;amp;#39;re Alone Now&#39;/&gt;\n   &lt;circle cx=&#39;361.29&#39; cy=&#39;119.76&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;I Wanna Dance with Somebody (Who Loves Me)&#39;/&gt;\n   &lt;circle cx=&#39;386.84&#39; cy=&#39;202.89&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Uptown Funk (feat. Bruno Mars)&#39;/&gt;\n   &lt;circle cx=&#39;112.06&#39; cy=&#39;212.56&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Single Ladies (Put a Ring on It)&#39;/&gt;\n   &lt;circle cx=&#39;392.71&#39; cy=&#39;129.04&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Shake It Off&#39;/&gt;\n   &lt;circle cx=&#39;340.77&#39; cy=&#39;163.84&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Jump Around&#39;/&gt;\n   &lt;circle cx=&#39;315.63&#39; cy=&#39;119.76&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Hips Don&amp;amp;#39;t Lie (feat. Wyclef Jean)&#39;/&gt;\n   &lt;circle cx=&#39;241.91&#39; cy=&#39;142.96&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Body (feat. Brando)&#39;/&gt;\n   &lt;circle cx=&#39;285.89&#39; cy=&#39;115.89&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Jackie Chan&#39;/&gt;\n   &lt;circle cx=&#39;151.85&#39; cy=&#39;214.88&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Wake Up in the Sky&#39;/&gt;\n   &lt;circle cx=&#39;187.46&#39; cy=&#39;228.02&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Summer, Highland Falls - Live at the Bayou, Washington, D.C. - July 1980&#39;/&gt;\n   &lt;circle cx=&#39;297.2&#39; cy=&#39;108.16&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Magic Dance&#39;/&gt;\n   &lt;circle cx=&#39;315.22&#39; cy=&#39;95.78&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Dancing with Myself - 2001 Remaster&#39;/&gt;\n   &lt;circle cx=&#39;271.65&#39; cy=&#39;71.42&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Waterloo&#39;/&gt;\n   &lt;circle cx=&#39;231.02&#39; cy=&#39;90.76&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Potential Breakup Song&#39;/&gt;\n   &lt;circle cx=&#39;381.4&#39; cy=&#39;119.37&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Levitating (feat. DaBaby)&#39;/&gt;\n   &lt;circle cx=&#39;184.52&#39; cy=&#39;121.69&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Forever&#39;/&gt;\n   &lt;circle cx=&#39;344.54&#39; cy=&#39;136&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;White Walls (feat. ScHoolboy Q &amp;amp;amp; Hollis)&#39;/&gt;\n   &lt;circle cx=&#39;253.22&#39; cy=&#39;85.73&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Rose Tattoo&#39;/&gt;\n   &lt;circle cx=&#39;197.09&#39; cy=&#39;85.73&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Kiss Me I&amp;amp;#39;m #!@&amp;amp;#39;faced&#39;/&gt;\n   &lt;circle cx=&#39;162.32&#39; cy=&#39;137.93&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;All Night (feat. Knox Fortune)&#39;/&gt;\n   &lt;circle cx=&#39;282.54&#39; cy=&#39;131.74&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Don&amp;amp;#39;t Start Now&#39;/&gt;\n   &lt;circle cx=&#39;380.56&#39; cy=&#39;106.61&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;The Legend of Chavo Guerrero&#39;/&gt;\n   &lt;circle cx=&#39;398.57&#39; cy=&#39;168.86&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Bam Bam (feat. Ed Sheeran)&#39;/&gt;\n   &lt;circle cx=&#39;228.51&#39; cy=&#39;142.18&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;The Spins&#39;/&gt;\n   &lt;circle cx=&#39;167.77&#39; cy=&#39;188.2&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Hung Up&#39;/&gt;\n   &lt;circle cx=&#39;355.43&#39; cy=&#39;95.4&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;BREAK MY SOUL&#39;/&gt;\n   &lt;circle cx=&#39;270.81&#39; cy=&#39;190.13&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Post Malone (feat. RANI)&#39;/&gt;\n   &lt;circle cx=&#39;218.87&#39; cy=&#39;182.01&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Take Me Out&#39;/&gt;\n   &lt;circle cx=&#39;200.86&#39; cy=&#39;90.37&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Sk8er Boi&#39;/&gt;\n   &lt;circle cx=&#39;328.2&#39; cy=&#39;66.01&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Timber (feat. Ke$ha)&#39;/&gt;\n   &lt;circle cx=&#39;301.39&#39; cy=&#39;128.26&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Time of Our Lives&#39;/&gt;\n   &lt;circle cx=&#39;192.48&#39; cy=&#39;94.62&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Levels - Radio Edit&#39;/&gt;\n   &lt;circle cx=&#39;285.89&#39; cy=&#39;157.65&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Body On My&#39;/&gt;\n   &lt;circle cx=&#39;402.34&#39; cy=&#39;61.76&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Hey Ya!&#39;/&gt;\n   &lt;circle cx=&#39;297.2&#39; cy=&#39;114.73&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;TiK ToK&#39;/&gt;\n   &lt;circle cx=&#39;401.92&#39; cy=&#39;212.94&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;SexyBack (feat. Timbaland)&#39;/&gt;\n   &lt;circle cx=&#39;315.63&#39; cy=&#39;86.89&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Nice For What&#39;/&gt;\n   &lt;circle cx=&#39;341.19&#39; cy=&#39;130.97&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Up&#39;/&gt;\n   &lt;circle cx=&#39;71.84&#39; cy=&#39;136.38&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Clarity&#39;/&gt;\n   &lt;circle cx=&#39;286.31&#39; cy=&#39;181.62&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;good 4 u&#39;/&gt;\n   &lt;circle cx=&#39;373.44&#39; cy=&#39;147.21&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Cake By The Ocean&#39;/&gt;\n   &lt;circle cx=&#39;124.21&#39; cy=&#39;134.06&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Titanium (feat. Sia)&#39;/&gt;\n   &lt;circle cx=&#39;229.34&#39; cy=&#39;99.65&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Where Them Girls At (feat. Nicki Minaj &amp;amp;amp; Flo Rida)&#39;/&gt;\n   &lt;circle cx=&#39;330.3&#39; cy=&#39;155.72&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;I Know You Want Me (Calle Ocho)&#39;/&gt;\n   &lt;circle cx=&#39;196.25&#39; cy=&#39;102.36&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Club Can&amp;amp;#39;t Handle Me (feat. David Guetta)&#39;/&gt;\n   &lt;circle cx=&#39;343.28&#39; cy=&#39;88.05&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;I Love It (feat. Charli XCX)&#39;/&gt;\n   &lt;circle cx=&#39;390.61&#39; cy=&#39;170.8&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Treasure&#39;/&gt;\n   &lt;circle cx=&#39;357.52&#39; cy=&#39;71.04&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;She Bangs - English Version&#39;/&gt;\n   &lt;circle cx=&#39;327.78&#39; cy=&#39;78&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Magic (feat. Rivers Cuomo)&#39;/&gt;\n   &lt;circle cx=&#39;385.17&#39; cy=&#39;114.34&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Toxic&#39;/&gt;\n   &lt;circle cx=&#39;220.13&#39; cy=&#39;75.29&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Give Me Everything (feat. Ne-Yo, Afrojack &amp;amp;amp; Nayer)&#39;/&gt;\n   &lt;circle cx=&#39;342.44&#39; cy=&#39;206.37&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Temperature&#39;/&gt;\n  &lt;\/g&gt;\n  &lt;g clip-path=&#39;url(#svg_5bdb2b70-1919-4a89-9edf-cf363f034ebd_c1)&#39;&gt;\n   &lt;text x=&#39;27.91&#39; y=&#39;287.68&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;40%&lt;\/text&gt;\n   &lt;text x=&#39;28.14&#39; y=&#39;210.35&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;60%&lt;\/text&gt;\n   &lt;text x=&#39;28.22&#39; y=&#39;133.02&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;80%&lt;\/text&gt;\n   &lt;text x=&#39;23.41&#39; y=&#39;55.68&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;100%&lt;\/text&gt;\n   &lt;text x=&#39;92.81&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;25%&lt;\/text&gt;\n   &lt;text x=&#39;197.43&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;50%&lt;\/text&gt;\n   &lt;text x=&#39;302.26&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;75%&lt;\/text&gt;\n   &lt;text x=&#39;404.33&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;100%&lt;\/text&gt;\n   &lt;text x=&#39;214.33&#39; y=&#39;350.04&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;valence&lt;\/text&gt;\n   &lt;text transform=&#39;translate(16.93,207.49) rotate(-90.00)&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;energy&lt;\/text&gt;\n   &lt;text x=&#39;6.97&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;Clustered&lt;\/text&gt;\n   &lt;text x=&#39;72.85&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;into&lt;\/text&gt;\n   &lt;text x=&#39;102.64&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#DD5129&#39;&gt;zesty&lt;\/text&gt;\n   &lt;text x=&#39;141.53&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#DD5129&#39;&gt;bops&lt;\/text&gt;\n   &lt;text x=&#39;174.96&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;,&lt;\/text&gt;\n   &lt;text x=&#39;181.19&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#43B284&#39;&gt;angsty&lt;\/text&gt;\n   &lt;text x=&#39;230.18&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#43B284&#39;&gt;bangers&lt;\/text&gt;\n   &lt;text x=&#39;284.62&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;,&lt;\/text&gt;\n   &lt;text x=&#39;290.86&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;and&lt;\/text&gt;\n   &lt;text x=&#39;319.58&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#0F7BA2&#39;&gt;mellow&lt;\/text&gt;\n   &lt;text x=&#39;370.91&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#0F7BA2&#39;&gt;jams&lt;\/text&gt;\n   &lt;text x=&#39;6.97&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;Clusters&lt;\/text&gt;\n   &lt;text x=&#39;75.74&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;in&lt;\/text&gt;\n   &lt;text x=&#39;96.28&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;the&lt;\/text&gt;\n   &lt;text x=&#39;125.92&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;current&lt;\/text&gt;\n   &lt;text x=&#39;188.75&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;playlist&lt;\/text&gt;\n  &lt;\/g&gt;\n &lt;\/g&gt;\n&lt;\/svg&gt;&#34;,&#34;js&#34;:null,&#34;uid&#34;:&#34;svg_5bdb2b70-1919-4a89-9edf-cf363f034ebd&#34;,&#34;ratio&#34;:1.2,&#34;settings&#34;:{&#34;tooltip&#34;:{&#34;css&#34;:&#34;.tooltip_SVGID_ { color:white;padding:2px;border-radius:2px;font-family:Roboto Slab; ; position:absolute;pointer-events:none;z-index:999;}&#34;,&#34;placement&#34;:&#34;doc&#34;,&#34;offx&#34;:10,&#34;offy&#34;:0,&#34;use_cursor_pos&#34;:true,&#34;opacity&#34;:0.8,&#34;usefill&#34;:true,&#34;usestroke&#34;:false,&#34;delay&#34;:{&#34;over&#34;:200,&#34;out&#34;:500}},&#34;hover&#34;:{&#34;css&#34;:&#34;.hover_SVGID_ { fill:#1279BF;stroke:#1279BF;cursor:pointer; }&#34;,&#34;reactive&#34;:false},&#34;hoverkey&#34;:{&#34;css&#34;:&#34;.hover_key_SVGID_ { stroke:red; }&#34;,&#34;reactive&#34;:false},&#34;hovertheme&#34;:{&#34;css&#34;:&#34;.hover_theme_SVGID_ { fill:green; }&#34;,&#34;reactive&#34;:false},&#34;hoverinv&#34;:{&#34;css&#34;:&#34;&#34;},&#34;zoom&#34;:{&#34;min&#34;:1,&#34;max&#34;:1},&#34;capture&#34;:{&#34;css&#34;:&#34;.selected_SVGID_ { fill:red;stroke:gray; }&#34;,&#34;type&#34;:&#34;multiple&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;capturekey&#34;:{&#34;css&#34;:&#34;.selected_key_SVGID_ { stroke:gray; }&#34;,&#34;type&#34;:&#34;single&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;capturetheme&#34;:{&#34;css&#34;:&#34;.selected_theme_SVGID_ { stroke:gray; }&#34;,&#34;type&#34;:&#34;single&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;toolbar&#34;:{&#34;position&#34;:&#34;topright&#34;,&#34;saveaspng&#34;:true,&#34;pngname&#34;:&#34;diagram&#34;},&#34;sizing&#34;:{&#34;rescale&#34;:true,&#34;width&#34;:1}}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;As expected, the majority of songs in the current playlist fall into the bop cluster. Let’s explore this cluster using in more detail with the custom metric &lt;code&gt;vibe&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# assign to clusters
ding_dong_vibes &amp;lt;- 
  ding_dong_clusters %&amp;gt;%
  augment(ding_dong) %&amp;gt;%
  select(track_name,
         valence, 
         energy, 
         .pred_cluster) %&amp;gt;%
  mutate(vibe = valence + energy)

# what are songs with the biggest vibe?
ding_dong_vibes %&amp;gt;%
  arrange(desc(vibe)) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table style=&#34;width:100%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;48%&#34; /&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;col width=&#34;10%&#34; /&gt;
&lt;col width=&#34;20%&#34; /&gt;
&lt;col width=&#34;8%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;track_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;valence&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;energy&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;.pred_cluster&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;vibe&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hey Ya!&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.965&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.974&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.939&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Rasputin&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.966&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.893&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.859&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;September&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.979&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.832&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.811&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;She Bangs - English Version&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.858&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.950&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.808&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Take on Me&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.876&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.902&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.778&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;The Legend of Chavo Guerrero&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.913&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.858&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.771&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Can’t Hold Us (feat. Ray Dalton)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.847&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.922&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.769&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Toxic&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.924&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.838&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.762&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Timber (feat. Ke$ha)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.788&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.963&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.751&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Shake It Off&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.942&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.800&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.742&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As expected, when arranging by &lt;code&gt;vibe&lt;/code&gt;, the top songs are all a part of the first cluster. And they are, indeed, &lt;em&gt;a vibe&lt;/em&gt;:&lt;/p&gt;



&lt;iframe src=&#34;https://open.spotify.com/embed/track/1uPrIHgYztXSkkcts9jet8&#34;
    width=&#34;100%&#34;
    height=&#34;380&#34;
    frameborder=&#34;0&#34;
    allowtransparency=&#34;true&#34;
    allow=&#34;encrypted-media&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;Compare that with the second cluster, which are generally lower energy (I’d personally disagree with spotify ranking Mr. Blue Sky and Single Ladies as “low energy,” but most others make sense).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ding_dong_vibes %&amp;gt;%
  filter(.pred_cluster == &amp;quot;Cluster_2&amp;quot;) %&amp;gt;%
  arrange(vibe) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;67%&#34; /&gt;
&lt;col width=&#34;7%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;5%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;track_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;valence&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;energy&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;.pred_cluster&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;vibe&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mr. Blue Sky&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.478&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.338&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Single Ladies (Put a Ring on It)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.272&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.584&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.856&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Low (feat. T-Pain)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.304&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.609&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.913&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;I Got You&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.544&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.399&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.943&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Wake Up in the Sky&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.367&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.578&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.945&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Summer, Highland Falls - Live at the Bayou, Washington, D.C. - July 1980&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.452&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.544&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.996&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Wagon Wheel&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.634&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.403&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.037&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hung Up&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.405&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.647&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.052&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Take Me Out&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.527&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.663&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.190&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Country Grammar (Hot Shit)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.565&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.664&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.229&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;



&lt;iframe src=&#34;https://open.spotify.com/embed/track/12jjuxN1gxlm29cqL5M6MW&#34;
    width=&#34;100%&#34;
    height=&#34;380&#34;
    frameborder=&#34;0&#34;
    allowtransparency=&#34;true&#34;
    allow=&#34;encrypted-media&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;Finally, the third cluster mostly contains songs with low valence but relatively high energy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ding_dong_vibes %&amp;gt;%
  filter(.pred_cluster == &amp;quot;Cluster_3&amp;quot;) %&amp;gt;%
  arrange(vibe) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;54%&#34; /&gt;
&lt;col width=&#34;10%&#34; /&gt;
&lt;col width=&#34;9%&#34; /&gt;
&lt;col width=&#34;18%&#34; /&gt;
&lt;col width=&#34;7%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;track_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;valence&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;energy&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;.pred_cluster&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;vibe&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Clarity&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.176&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.781&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.957&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Titanium (feat. Sia)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.301&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.787&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.088&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mr. Brightside&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.240&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.918&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.158&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;All Night (feat. Knox Fortune)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.392&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.777&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.169&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Forever&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.445&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.819&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.264&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Shout, Pts. 1 &amp;amp; 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.416&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.866&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.282&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;The Spins&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.550&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.766&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.316&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Club Can’t Handle Me (feat. David Guetta)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.473&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.869&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.342&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Body (feat. Brando)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.582&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.764&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.346&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Levels - Radio Edit&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.464&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.889&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.353&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;



&lt;iframe src=&#34;https://open.spotify.com/embed/track/3n3Ppam7vgaVa1iaRUc9Lp&#34;
    width=&#34;100%&#34;
    height=&#34;380&#34;
    frameborder=&#34;0&#34;
    allowtransparency=&#34;true&#34;
    allow=&#34;encrypted-media&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;Now that I have the songs in the current playlist sorted by cluster, let’s pull in some new songs and assign them to the appropriate cluster!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-new-songs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding new songs&lt;/h2&gt;
&lt;p&gt;To go searching for new songs, we’ll start by casting a wide net then narrow the search with some of the &lt;code&gt;get_*()&lt;/code&gt; functions from spotifyr. I’ll start by using &lt;code&gt;get_categories()&lt;/code&gt; to explore the categories available in spotify.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_categories() %&amp;gt;%
  as_tibble() %&amp;gt;%
  select(id, name) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;toplists&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Top Lists&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;hiphop&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Hip-Hop&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;pop&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Pop&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;country&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Country&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;0JQ5DAqbMKFxXaXKP7zcDp&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Latin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;rock&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Rock&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;summer&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Summer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;0JQ5DAqbMKFAXlCG6QvYQ4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Workout&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;0JQ5DAqbMKFEZPnFQSFB1T&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R&amp;amp;B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;edm_dance&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Dance/Electronic&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I don’t really want to play country music or R&amp;amp;B during the wedding, so I’ll filter to a few categories before using &lt;code&gt;get_category_playlists()&lt;/code&gt; to pull in the featured playlists available in each category.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# pull in playlist ids
playlists &amp;lt;- 
  get_categories() %&amp;gt;%
  as_tibble() %&amp;gt;%
  filter(id %in% c(&amp;quot;toplists&amp;quot;, &amp;quot;hiphop&amp;quot;, &amp;quot;pop&amp;quot;, &amp;quot;rock&amp;quot;, &amp;quot;summer&amp;quot;)) %&amp;gt;%
  pull(id) %&amp;gt;%
  map_dfr(get_category_playlists) %&amp;gt;%
  as_tibble() %&amp;gt;%
  select(id, name, description) %&amp;gt;%
  distinct(id, .keep_all = TRUE)

playlists %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;col width=&#34;74%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;name&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZEVXbLRQDuF5jeBp&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Top 50 - USA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Your daily update of the most played tracks right now - USA.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZEVXbMDoHDwVN2tF&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Top 50 - Global&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Your daily update of the most played tracks right now - Global.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZEVXbLiRSasKsNU9&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Viral 50 - Global&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Your daily update of the most viral tracks right now - Global.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZEVXbKuaTI1Z1Afx&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Viral 50 - USA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Your daily update of the most viral tracks right now - USA.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZF1DX4JAvHpjipBk&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;New Music Friday&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;New music from BLACKPINK, Zedd &amp;amp; Maren Morris, Panic! At The Disco, and more!&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZF1DX0XUsuxWHRQd&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RapCaviar&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Music from Drake, Offset and 42 Dugg.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZF1DX6GwdWRQMQpq&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Feelin’ Myself&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;The hip-hop playlist that’s a whole mood. Art By Laci Jordan; Cover: Megan Thee Stallion&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZF1DX2RxBh64BHjQ&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Most Necessary&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;The official voice of the next generation. Cover: &lt;a href=&#34;spotify🧑‍🎨1iwUuIOKYjV7SKIg27v4zi&#34;&gt;Real Boston Richey&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZF1DXaxIqwkEGFEh&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Out The Mud&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Strictly for the streets. Cover: Nardo Wick&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;37i9dQZF1DWY4xHQp97fN6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Get Turnt&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Mode: Turnt&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There’s a lot of playlists in &lt;code&gt;playlists&lt;/code&gt;, so I’ve gone through and selected a few that I’m interested in exploring further.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;selected_playlists &amp;lt;-
  c(&amp;quot;Today&amp;#39;s Top Hits&amp;quot;,
    &amp;quot;mint&amp;quot;,
    &amp;quot;Top 50 - US&amp;quot;,
    &amp;quot;Top 50 - Global&amp;quot;,
    &amp;quot;Viral 50 - US&amp;quot;,
    &amp;quot;Viral 50 - Global&amp;quot;,
    &amp;quot;New Music Friday&amp;quot;,
    &amp;quot;Most Necessary&amp;quot;,
    &amp;quot;Internet People&amp;quot;,
    &amp;quot;Gold School&amp;quot;,
    &amp;quot;Hot Hits USA&amp;quot;,
    &amp;quot;Pop Rising&amp;quot;,
    &amp;quot;teen beats&amp;quot;,
    &amp;quot;big on the internet&amp;quot;,
    &amp;quot;Party Hits&amp;quot;,
    &amp;quot;Mega Hit Mix&amp;quot;,
    &amp;quot;Pumped Pop&amp;quot;,
    &amp;quot;Hit Rewind&amp;quot;,
    &amp;quot;The Ultimate Hit Mix&amp;quot;,
    &amp;quot;00s Rock Anthems&amp;quot;,
    &amp;quot;Summer Hits&amp;quot;,
    &amp;quot;Barack Obama&amp;#39;s Summer 2022 Playlist&amp;quot;,
    &amp;quot;Summer Hits of the 10s&amp;quot;,
    &amp;quot;Family Road Trip&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this shorter list of playlists, I can pull in the all the songs that appear on each with &lt;code&gt;get_playlist_tracks()&lt;/code&gt;. Some songs may appear on multiple playlists, so we’ll only look at unique songs by &lt;code&gt;track_id&lt;/code&gt;. I’ve already pulled in features for songs currently on the playlist, so we can filter those out as well. Finally, &lt;code&gt;get_track_audio_features()&lt;/code&gt; limits queries to a maximum of 100 songs, so we’ll select the top 100 most popular songs within the sample.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_songs &amp;lt;- 
  playlists %&amp;gt;%
  filter(name %in% selected_playlists) %&amp;gt;%
  pull(id) %&amp;gt;%
  map_dfr(get_playlist_tracks) %&amp;gt;%
  as_tibble()

new_songs &amp;lt;- 
  new_songs %&amp;gt;%
  select(track.id,
         track.name,
         track.popularity) %&amp;gt;%
  rename_with(~stringr::str_replace(.x, &amp;quot;\\.&amp;quot;, &amp;quot;_&amp;quot;)) %&amp;gt;%
  distinct(track_id, .keep_all = TRUE) %&amp;gt;%
  arrange(desc(track_popularity)) %&amp;gt;%
  filter(!track_id %in% ding_dong$track_id) %&amp;gt;%
  slice_head(n = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;29%&#34; /&gt;
&lt;col width=&#34;49%&#34; /&gt;
&lt;col width=&#34;21%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;track_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;track_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;track_popularity&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2tTmW7RDtMQtBk7m2rYeSw&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Quevedo: Bzrp Music Sessions, Vol. 52&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6Sq7ltF9Qa7SNFBsV5Cogx&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Me Porto Bonito&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;99&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1IHWl5LamUGEuP4ozKQSXZ&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Tití Me Preguntó&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;97&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5Eax0qFko2dh7Rl2lYs3bx&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Efecto&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3k3NWokhRRkEPhCzPmV8TW&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Ojitos Lindos&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6xGruZOHLs39ZbVccQTuPZ&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Glimpse of Us&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;4LRPiXqCikLlN15c3yImP7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;As It Was&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;75FEaRjZTKLhTrFGsfMUXR&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Running Up That Hill (A Deal With God)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;95&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6Xom58OOXk2SoU711L2IXO&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Moscow Mule&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;95&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5ildQOEKmJuWGl2vRkFdYc&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;DESPECHÁ&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;94&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now let’s assign these 100 news songs to the clusters we found earlier based on their valence and energy!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_song_features &amp;lt;- 
  new_songs %&amp;gt;%
  pull(track_id) %&amp;gt;%
  get_track_audio_features()

new_songs &amp;lt;- 
  new_songs %&amp;gt;%
  left_join(new_song_features, by = c(&amp;quot;track_id&amp;quot; = &amp;quot;id&amp;quot;))

new_songs_clustered &amp;lt;- 
  ding_dong_clusters %&amp;gt;%
  augment(new_songs) %&amp;gt;%
  select(track_name,
         valence,
         energy,
         .pred_cluster) %&amp;gt;%
  mutate(vibe = valence + energy)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-3&#34; style=&#34;width:4500px;height:3000px;&#34; class=&#34;girafe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-3&#34;&gt;{&#34;x&#34;:{&#34;html&#34;:&#34;&lt;?xml version=\&#34;1.0\&#34; encoding=\&#34;UTF-8\&#34;?&gt;\n&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; xmlns:xlink=&#39;http://www.w3.org/1999/xlink&#39; id=&#39;svg_db01d859-4f2d-45d4-a0df-9e9c12bcdf06&#39; viewBox=&#39;0 0 432 360&#39;&gt;\n &lt;defs&gt;\n  &lt;clipPath id=&#39;svg_db01d859-4f2d-45d4-a0df-9e9c12bcdf06_c1&#39;&gt;\n   &lt;rect x=&#39;0&#39; y=&#39;0&#39; width=&#39;432&#39; height=&#39;360&#39;/&gt;\n  &lt;\/clipPath&gt;\n  &lt;clipPath id=&#39;svg_db01d859-4f2d-45d4-a0df-9e9c12bcdf06_c2&#39;&gt;\n   &lt;rect x=&#39;50.53&#39; y=&#39;49.46&#39; width=&#39;374.5&#39; height=&#39;270.51&#39;/&gt;\n  &lt;\/clipPath&gt;\n &lt;\/defs&gt;\n &lt;g&gt;\n  &lt;g clip-path=&#39;url(#svg_db01d859-4f2d-45d4-a0df-9e9c12bcdf06_c1)&#39;&gt;\n   &lt;rect x=&#39;0&#39; y=&#39;0&#39; width=&#39;432&#39; height=&#39;360&#39; fill=&#39;#FFFFFF&#39; stroke=&#39;#FFFFFF&#39; stroke-width=&#39;0.75&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39;/&gt;\n   &lt;rect x=&#39;0&#39; y=&#39;0&#39; width=&#39;432&#39; height=&#39;360&#39; fill=&#39;#FFFFFF&#39; stroke=&#39;#FFFFFF&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39;/&gt;\n  &lt;\/g&gt;\n  &lt;g clip-path=&#39;url(#svg_db01d859-4f2d-45d4-a0df-9e9c12bcdf06_c2)&#39;&gt;\n   &lt;polyline points=&#39;50.53,284.39 425.03,284.39&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;50.53,211.64 425.03,211.64&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;50.53,138.88 425.03,138.88&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;50.53,66.12 425.03,66.12&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;82.94,319.97 82.94,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;178.88,319.97 178.88,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;274.83,319.97 274.83,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;370.78,319.97 370.78,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;0.68&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;50.53,248.01 425.03,248.01&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;50.53,175.26 425.03,175.26&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;50.53,102.50 425.03,102.50&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;130.91,319.97 130.91,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;226.86,319.97 226.86,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;322.80,319.97 322.80,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;polyline points=&#39;418.75,319.97 418.75,49.46&#39; fill=&#39;none&#39; stroke=&#39;#EBEBEB&#39; stroke-width=&#39;1.36&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;butt&#39;/&gt;\n   &lt;circle cx=&#39;246.05&#39; cy=&#39;109.05&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Quevedo: Bzrp Music Sessions, Vol. 52&#39;/&gt;\n   &lt;circle cx=&#39;198.07&#39; cy=&#39;134.51&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Me Porto Bonito&#39;/&gt;\n   &lt;circle cx=&#39;106.73&#39; cy=&#39;133.42&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Tití Me Preguntó&#39;/&gt;\n   &lt;circle cx=&#39;124.77&#39; cy=&#39;220.73&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Efecto&#39;/&gt;\n   &lt;circle cx=&#39;137.82&#39; cy=&#39;143.97&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Ojitos Lindos&#39;/&gt;\n   &lt;circle cx=&#39;137.82&#39; cy=&#39;278.21&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Glimpse of Us&#39;/&gt;\n   &lt;circle cx=&#39;289.03&#39; cy=&#39;127.6&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;As It Was&#39;/&gt;\n   &lt;circle cx=&#39;110.57&#39; cy=&#39;194.54&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Running Up That Hill (A Deal With God)&#39;/&gt;\n   &lt;circle cx=&#39;147.03&#39; cy=&#39;148.34&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Moscow Mule&#39;/&gt;\n   &lt;circle cx=&#39;332.4&#39; cy=&#39;166.89&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;DESPECHÁ&#39;/&gt;\n   &lt;circle cx=&#39;238.37&#39; cy=&#39;205.82&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;PROVENZA&#39;/&gt;\n   &lt;circle cx=&#39;310.91&#39; cy=&#39;178.17&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Left and Right (Feat. Jung Kook of BTS)&#39;/&gt;\n   &lt;circle cx=&#39;289.03&#39; cy=&#39;127.6&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;As It Was&#39;/&gt;\n   &lt;circle cx=&#39;361.18&#39; cy=&#39;146.52&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;La Bachata&#39;/&gt;\n   &lt;circle cx=&#39;351.59&#39; cy=&#39;103.59&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;I Ain&amp;amp;#39;t Worried&#39;/&gt;\n   &lt;circle cx=&#39;195.77&#39; cy=&#39;144.7&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Tarot&#39;/&gt;\n   &lt;circle cx=&#39;255.64&#39; cy=&#39;162.16&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Te Felicito&#39;/&gt;\n   &lt;circle cx=&#39;380.75&#39; cy=&#39;128.69&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Late Night Talking&#39;/&gt;\n   &lt;circle cx=&#39;215.73&#39; cy=&#39;102.87&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Party&#39;/&gt;\n   &lt;circle cx=&#39;408&#39; cy=&#39;185.08&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Doja&#39;/&gt;\n   &lt;circle cx=&#39;358.11&#39; cy=&#39;133.79&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Sunroof&#39;/&gt;\n   &lt;circle cx=&#39;216.11&#39; cy=&#39;149.79&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;I Like You (A Happier Song) (with Doja Cat)&#39;/&gt;\n   &lt;circle cx=&#39;175.43&#39; cy=&#39;148.7&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Jimmy Cooks (feat. 21 Savage)&#39;/&gt;\n   &lt;circle cx=&#39;257.18&#39; cy=&#39;107.59&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;La Corriente&#39;/&gt;\n   &lt;circle cx=&#39;69.47&#39; cy=&#39;171.26&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;MIDDLE OF THE NIGHT&#39;/&gt;\n   &lt;circle cx=&#39;312.06&#39; cy=&#39;123.24&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;About Damn Time&#39;/&gt;\n   &lt;circle cx=&#39;300.54&#39; cy=&#39;142.52&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Ferrari&#39;/&gt;\n   &lt;circle cx=&#39;187.71&#39; cy=&#39;99.96&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Sweater Weather&#39;/&gt;\n   &lt;circle cx=&#39;163.15&#39; cy=&#39;127.97&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Blinding Lights&#39;/&gt;\n   &lt;circle cx=&#39;318.97&#39; cy=&#39;174.89&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Vegas (From the Original Motion Picture Soundtrack ELVIS)&#39;/&gt;\n   &lt;circle cx=&#39;354.27&#39; cy=&#39;194.17&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Una Noche en Medellín&#39;/&gt;\n   &lt;circle cx=&#39;262.17&#39; cy=&#39;79.95&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;One Kiss (with Dua Lipa)&#39;/&gt;\n   &lt;circle cx=&#39;267.92&#39; cy=&#39;65.03&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Después de la Playa&#39;/&gt;\n   &lt;circle cx=&#39;299.78&#39; cy=&#39;209.09&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Bad Habit&#39;/&gt;\n   &lt;circle cx=&#39;88.31&#39; cy=&#39;199.63&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Running Up That Hill (A Deal With God) - 2018 Remaster&#39;/&gt;\n   &lt;circle cx=&#39;135.52&#39; cy=&#39;162.16&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Where Are You Now&#39;/&gt;\n   &lt;circle cx=&#39;122.09&#39; cy=&#39;208.73&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Until I Found You&#39;/&gt;\n   &lt;circle cx=&#39;209.2&#39; cy=&#39;141.79&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Dandelions&#39;/&gt;\n   &lt;circle cx=&#39;238.76&#39; cy=&#39;202.54&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Heat Waves&#39;/&gt;\n   &lt;circle cx=&#39;153.94&#39; cy=&#39;142.52&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Under The Influence&#39;/&gt;\n   &lt;circle cx=&#39;401.48&#39; cy=&#39;80.31&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Bad Decisions (with BTS &amp;amp;amp; Snoop Dogg)&#39;/&gt;\n   &lt;circle cx=&#39;204.21&#39; cy=&#39;123.96&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Ghost&#39;/&gt;\n   &lt;circle cx=&#39;159.31&#39; cy=&#39;188.72&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;First Class&#39;/&gt;\n   &lt;circle cx=&#39;373.08&#39; cy=&#39;115.6&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Woman&#39;/&gt;\n   &lt;circle cx=&#39;218.41&#39; cy=&#39;115.6&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;STAY (with Justin Bieber)&#39;/&gt;\n   &lt;circle cx=&#39;165.07&#39; cy=&#39;159.98&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;WAIT FOR U (feat. Drake &amp;amp;amp; Tems)&#39;/&gt;\n   &lt;circle cx=&#39;396.49&#39; cy=&#39;103.23&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Cold Heart - PNAU Remix&#39;/&gt;\n   &lt;circle cx=&#39;67.55&#39; cy=&#39;270.21&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;traitor&#39;/&gt;\n   &lt;circle cx=&#39;85.24&#39; cy=&#39;198.18&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Another Love&#39;/&gt;\n   &lt;circle cx=&#39;248.73&#39; cy=&#39;96.68&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Watermelon Sugar&#39;/&gt;\n   &lt;circle cx=&#39;241.06&#39; cy=&#39;68.67&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Bad Habits&#39;/&gt;\n   &lt;circle cx=&#39;244.51&#39; cy=&#39;85.77&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;THATS WHAT I WANT&#39;/&gt;\n   &lt;circle cx=&#39;350.44&#39; cy=&#39;81.04&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Shivers&#39;/&gt;\n   &lt;circle cx=&#39;196.92&#39; cy=&#39;103.59&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Belly Dancer&#39;/&gt;\n   &lt;circle cx=&#39;94.45&#39; cy=&#39;125.42&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;I Was Never There&#39;/&gt;\n   &lt;circle cx=&#39;87.54&#39; cy=&#39;236.74&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;drivers license&#39;/&gt;\n   &lt;circle cx=&#39;103.28&#39; cy=&#39;170.89&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;deja vu&#39;/&gt;\n   &lt;circle cx=&#39;102.13&#39; cy=&#39;177.8&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Call Out My Name&#39;/&gt;\n   &lt;circle cx=&#39;202.3&#39; cy=&#39;184.72&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Kesariya (From &amp;amp;quot;Brahmastra&amp;amp;quot;)&#39;/&gt;\n   &lt;circle cx=&#39;257.56&#39; cy=&#39;112.69&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Betty (Get Money)&#39;/&gt;\n   &lt;circle cx=&#39;104.43&#39; cy=&#39;225.82&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;STAYING ALIVE (feat. Drake &amp;amp;amp; Lil Baby)&#39;/&gt;\n   &lt;circle cx=&#39;150.1&#39; cy=&#39;197.09&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;In The Stars&#39;/&gt;\n   &lt;circle cx=&#39;247.97&#39; cy=&#39;108.69&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Enemy (with JID) - from the series Arcane League of Legends&#39;/&gt;\n   &lt;circle cx=&#39;176.97&#39; cy=&#39;204.36&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Night Changes&#39;/&gt;\n   &lt;circle cx=&#39;109.04&#39; cy=&#39;199.27&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Don’t Blame Me&#39;/&gt;\n   &lt;circle cx=&#39;171.98&#39; cy=&#39;133.42&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Music For a Sushi Restaurant&#39;/&gt;\n   &lt;circle cx=&#39;240.29&#39; cy=&#39;104.32&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Everybody Wants To Rule The World&#39;/&gt;\n   &lt;circle cx=&#39;151.64&#39; cy=&#39;71.58&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Only Love Can Hurt Like This&#39;/&gt;\n   &lt;circle cx=&#39;221.48&#39; cy=&#39;179.99&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Starboy&#39;/&gt;\n   &lt;circle cx=&#39;379.99&#39; cy=&#39;138.88&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;MAMIII&#39;/&gt;\n   &lt;circle cx=&#39;204.6&#39; cy=&#39;114.87&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Pepas&#39;/&gt;\n   &lt;circle cx=&#39;213.04&#39; cy=&#39;115.96&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;The Motto&#39;/&gt;\n   &lt;circle cx=&#39;81.02&#39; cy=&#39;285.85&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;lovely (with Khalid)&#39;/&gt;\n   &lt;circle cx=&#39;194.24&#39; cy=&#39;197.09&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;abcdefu&#39;/&gt;\n   &lt;circle cx=&#39;290.57&#39; cy=&#39;109.78&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Believer&#39;/&gt;\n   &lt;circle cx=&#39;254.11&#39; cy=&#39;123.6&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Bones&#39;/&gt;\n   &lt;circle cx=&#39;144.34&#39; cy=&#39;153.07&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Yellow&#39;/&gt;\n   &lt;circle cx=&#39;303.61&#39; cy=&#39;213.82&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Bad Habit&#39;/&gt;\n   &lt;circle cx=&#39;384.98&#39; cy=&#39;69.4&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Super Freaky Girl&#39;/&gt;\n   &lt;circle cx=&#39;218.41&#39; cy=&#39;115.6&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;STAY (with Justin Bieber)&#39;/&gt;\n   &lt;circle cx=&#39;251.8&#39; cy=&#39;237.46&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;cómo dormiste?&#39;/&gt;\n   &lt;circle cx=&#39;277.52&#39; cy=&#39;111.96&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Numb&#39;/&gt;\n   &lt;circle cx=&#39;80.64&#39; cy=&#39;307.67&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;TV&#39;/&gt;\n   &lt;circle cx=&#39;124&#39; cy=&#39;170.89&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Boyfriend&#39;/&gt;\n   &lt;circle cx=&#39;230.7&#39; cy=&#39;202.54&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Die For You&#39;/&gt;\n   &lt;circle cx=&#39;229.16&#39; cy=&#39;96.32&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;House of Memories&#39;/&gt;\n   &lt;circle cx=&#39;292.87&#39; cy=&#39;137.06&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Wait a Minute!&#39;/&gt;\n   &lt;circle cx=&#39;116.71&#39; cy=&#39;219.28&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Atlantis&#39;/&gt;\n   &lt;circle cx=&#39;176.97&#39; cy=&#39;186.9&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Softcore&#39;/&gt;\n   &lt;circle cx=&#39;87.54&#39; cy=&#39;188.35&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;The Hills&#39;/&gt;\n   &lt;circle cx=&#39;213.04&#39; cy=&#39;134.15&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Crazy What Love Can Do&#39;/&gt;\n   &lt;circle cx=&#39;186.94&#39; cy=&#39;125.78&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Envolver&#39;/&gt;\n   &lt;circle cx=&#39;378.07&#39; cy=&#39;137.42&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;INDUSTRY BABY (feat. Jack Harlow)&#39;/&gt;\n   &lt;circle cx=&#39;295.94&#39; cy=&#39;61.76&#39; r=&#39;3.07pt&#39; fill=&#39;#DD5129&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#DD5129&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Afraid To Feel&#39;/&gt;\n   &lt;circle cx=&#39;247.2&#39; cy=&#39;116.33&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Circles&#39;/&gt;\n   &lt;circle cx=&#39;213.04&#39; cy=&#39;140.34&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Peaches (feat. Daniel Caesar &amp;amp;amp; Giveon)&#39;/&gt;\n   &lt;circle cx=&#39;224.56&#39; cy=&#39;148.7&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Infinity&#39;/&gt;\n   &lt;circle cx=&#39;206.13&#39; cy=&#39;246.2&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Someone You Loved&#39;/&gt;\n   &lt;circle cx=&#39;261.01&#39; cy=&#39;102.5&#39; r=&#39;3.07pt&#39; fill=&#39;#43B284&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#43B284&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Beggin&amp;amp;#39;&#39;/&gt;\n   &lt;circle cx=&#39;138.59&#39; cy=&#39;238.92&#39; r=&#39;3.07pt&#39; fill=&#39;#0F7BA2&#39; fill-opacity=&#39;0.75&#39; stroke=&#39;#0F7BA2&#39; stroke-opacity=&#39;0.75&#39; stroke-width=&#39;0.71&#39; stroke-linejoin=&#39;round&#39; stroke-linecap=&#39;round&#39; title=&#39;Heather&#39;/&gt;\n  &lt;\/g&gt;\n  &lt;g clip-path=&#39;url(#svg_db01d859-4f2d-45d4-a0df-9e9c12bcdf06_c1)&#39;&gt;\n   &lt;text x=&#39;23.41&#39; y=&#39;251.99&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;40%&lt;\/text&gt;\n   &lt;text x=&#39;23.64&#39; y=&#39;179.24&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;60%&lt;\/text&gt;\n   &lt;text x=&#39;23.72&#39; y=&#39;106.48&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;80%&lt;\/text&gt;\n   &lt;text x=&#39;120.88&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;25%&lt;\/text&gt;\n   &lt;text x=&#39;216.73&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;50%&lt;\/text&gt;\n   &lt;text x=&#39;312.78&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;75%&lt;\/text&gt;\n   &lt;text x=&#39;406.08&#39; y=&#39;334.2&#39; font-size=&#39;8.4pt&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#4D4D4D&#39;&gt;100%&lt;\/text&gt;\n   &lt;text x=&#39;212.08&#39; y=&#39;350.04&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;valence&lt;\/text&gt;\n   &lt;text transform=&#39;translate(16.93,207.49) rotate(-90.00)&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;energy&lt;\/text&gt;\n   &lt;text x=&#39;6.97&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;Clustered&lt;\/text&gt;\n   &lt;text x=&#39;72.85&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;into&lt;\/text&gt;\n   &lt;text x=&#39;102.64&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#DD5129&#39;&gt;zesty&lt;\/text&gt;\n   &lt;text x=&#39;141.53&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#DD5129&#39;&gt;bops&lt;\/text&gt;\n   &lt;text x=&#39;174.96&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;,&lt;\/text&gt;\n   &lt;text x=&#39;181.19&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#43B284&#39;&gt;angsty&lt;\/text&gt;\n   &lt;text x=&#39;230.18&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#43B284&#39;&gt;bangers&lt;\/text&gt;\n   &lt;text x=&#39;284.62&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;,&lt;\/text&gt;\n   &lt;text x=&#39;290.86&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-family=&#39;Roboto Slab&#39;&gt;and&lt;\/text&gt;\n   &lt;text x=&#39;319.58&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#0F7BA2&#39;&gt;mellow&lt;\/text&gt;\n   &lt;text x=&#39;370.91&#39; y=&#39;39.5&#39; font-size=&#39;10.5pt&#39; font-weight=&#39;bold&#39; font-family=&#39;Roboto Slab&#39; fill=&#39;#0F7BA2&#39;&gt;jams&lt;\/text&gt;\n   &lt;text x=&#39;6.97&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;New&lt;\/text&gt;\n   &lt;text x=&#39;47.85&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;songs,&lt;\/text&gt;\n   &lt;text x=&#39;101.82&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;same&lt;\/text&gt;\n   &lt;text x=&#39;148.56&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;old&lt;\/text&gt;\n   &lt;text x=&#39;177.3&#39; y=&#39;18.92&#39; font-size=&#39;12.6pt&#39; font-family=&#39;Roboto Slab&#39;&gt;clusters&lt;\/text&gt;\n  &lt;\/g&gt;\n &lt;\/g&gt;\n&lt;\/svg&gt;&#34;,&#34;js&#34;:null,&#34;uid&#34;:&#34;svg_db01d859-4f2d-45d4-a0df-9e9c12bcdf06&#34;,&#34;ratio&#34;:1.2,&#34;settings&#34;:{&#34;tooltip&#34;:{&#34;css&#34;:&#34;.tooltip_SVGID_ { color:white;padding:2px;border-radius:2px;font-family:Roboto Slab; ; position:absolute;pointer-events:none;z-index:999;}&#34;,&#34;placement&#34;:&#34;doc&#34;,&#34;offx&#34;:10,&#34;offy&#34;:0,&#34;use_cursor_pos&#34;:true,&#34;opacity&#34;:0.8,&#34;usefill&#34;:true,&#34;usestroke&#34;:false,&#34;delay&#34;:{&#34;over&#34;:200,&#34;out&#34;:500}},&#34;hover&#34;:{&#34;css&#34;:&#34;.hover_SVGID_ { fill:#1279BF;stroke:#1279BF;cursor:pointer; }&#34;,&#34;reactive&#34;:false},&#34;hoverkey&#34;:{&#34;css&#34;:&#34;.hover_key_SVGID_ { stroke:red; }&#34;,&#34;reactive&#34;:false},&#34;hovertheme&#34;:{&#34;css&#34;:&#34;.hover_theme_SVGID_ { fill:green; }&#34;,&#34;reactive&#34;:false},&#34;hoverinv&#34;:{&#34;css&#34;:&#34;&#34;},&#34;zoom&#34;:{&#34;min&#34;:1,&#34;max&#34;:1},&#34;capture&#34;:{&#34;css&#34;:&#34;.selected_SVGID_ { fill:red;stroke:gray; }&#34;,&#34;type&#34;:&#34;multiple&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;capturekey&#34;:{&#34;css&#34;:&#34;.selected_key_SVGID_ { stroke:gray; }&#34;,&#34;type&#34;:&#34;single&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;capturetheme&#34;:{&#34;css&#34;:&#34;.selected_theme_SVGID_ { stroke:gray; }&#34;,&#34;type&#34;:&#34;single&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;toolbar&#34;:{&#34;position&#34;:&#34;topright&#34;,&#34;saveaspng&#34;:true,&#34;pngname&#34;:&#34;diagram&#34;},&#34;sizing&#34;:{&#34;rescale&#34;:true,&#34;width&#34;:1}}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Nice! It looks like the new songs are far more broad than the original playlist, but we can look at just the songs in the first cluster with the biggest vibe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_songs_clustered %&amp;gt;%
  filter(.pred_cluster == &amp;quot;Cluster_1&amp;quot;) %&amp;gt;%
  arrange(desc(vibe)) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;52%&#34; /&gt;
&lt;col width=&#34;10%&#34; /&gt;
&lt;col width=&#34;9%&#34; /&gt;
&lt;col width=&#34;19%&#34; /&gt;
&lt;col width=&#34;8%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;track_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;valence&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;energy&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;.pred_cluster&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;vibe&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Bad Decisions (with BTS &amp;amp; Snoop Dogg)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.955&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.861&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Super Freaky Girl&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.912&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.891&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.803&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cold Heart - PNAU Remix&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.942&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.798&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.740&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Shivers&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.822&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.859&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.681&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Woman&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.881&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.764&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.645&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Late Night Talking&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.901&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.728&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.629&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;I Ain’t Worried&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.825&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.797&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.622&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;MAMIII&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.899&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.700&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.599&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;INDUSTRY BABY (feat. Jack Harlow)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.894&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.704&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.598&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Afraid To Feel&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.680&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.912&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cluster_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.592&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now for the true vibe check — do these songs &lt;em&gt;belong&lt;/em&gt; on the playlist?&lt;/p&gt;



&lt;iframe src=&#34;https://open.spotify.com/embed/track/0xzI1KAr0Yd9tv8jlIk3sn&#34;
    width=&#34;100%&#34;
    height=&#34;380&#34;
    frameborder=&#34;0&#34;
    allowtransparency=&#34;true&#34;
    allow=&#34;encrypted-media&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;Oh &lt;strong&gt;&lt;em&gt;hell&lt;/em&gt; yeah!&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Some notes&lt;/strong&gt;: this analysis was done on Aug. 20th, 2022 — spotify’s featured playlists and tracks change on on a regular basis and may also depend on unique user data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introducing {nplyr}</title>
      <link>https://www.thedatadiary.net/blog/2022-07-24-introducing-nplyr/</link>
      <pubDate>Sun, 24 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-07-24-introducing-nplyr/</guid>
      <description>


&lt;p&gt;Data manipulation and transformation is a fundamental part of any analysis. There are excellent tools in the R ecosystem for manipulating data frames (&lt;a href=&#34;https://dplyr.tidyverse.org/&#34;&gt;dplyr&lt;/a&gt;, &lt;a href=&#34;https://rdatatable.gitlab.io/data.table/&#34;&gt;data.table&lt;/a&gt;, and &lt;a href=&#34;https://arrow.apache.org/docs/r/&#34;&gt;arrow&lt;/a&gt;, to name a few). Sometimes, however, it is desirable to work with &lt;em&gt;nested&lt;/em&gt; data frames, for which few tools are readily available.&lt;/p&gt;
&lt;p&gt;This is where &lt;a href=&#34;https://markjrieke.github.io/nplyr/&#34;&gt;nplyr&lt;/a&gt; comes into play! nplyr is a grammar of nested data manipulation that allows users to perform dplyr-like manipulations on data frames nested within a list-col of another data frame. Most dplyr verbs have nested equivalents in nplyr. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nest_mutate()&lt;/code&gt; is the nested equivalent of &lt;code&gt;mutate()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nest_select()&lt;/code&gt; is the nested equivalent of &lt;code&gt;select()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nest_filter()&lt;/code&gt; is the nested equivalent of &lt;code&gt;filter()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nest_summarise()&lt;/code&gt; is the nested equivalent of &lt;code&gt;summarise()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nest_group_by()&lt;/code&gt; is the nested equivalent of &lt;code&gt;group_by()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;installation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Installation&lt;/h3&gt;
&lt;p&gt;nplyr 0.1.0 is available on &lt;a href=&#34;https://cran.r-project.org/web/packages/nplyr/index.html&#34;&gt;CRAN&lt;/a&gt;. Alternatively, you can install the development version from github with the &lt;a href=&#34;https://cran.r-project.org/package=devtools&#34;&gt;devtools&lt;/a&gt; or &lt;a href=&#34;https://cran.r-project.org/package=remotes&#34;&gt;remotes&lt;/a&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install from CRAN
install.packages(&amp;quot;nplyr&amp;quot;)

# install from github
devtools::install_github(&amp;quot;markjrieke/nplyr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;usage&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Usage&lt;/h3&gt;
&lt;p&gt;To get started, we’ll create a nested column for the country data within each continent from the &lt;a href=&#34;https://cran.r-project.org/package=gapminder&#34;&gt;gapminder&lt;/a&gt; dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nplyr)

gm_nest &amp;lt;-
  gapminder::gapminder_unfiltered %&amp;gt;%
  tidyr::nest(country_data = -continent)

gm_nest&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 2
##   continent country_data        
##   &amp;lt;fct&amp;gt;     &amp;lt;list&amp;gt;              
## 1 Asia      &amp;lt;tibble [578 × 5]&amp;gt;  
## 2 Europe    &amp;lt;tibble [1,302 × 5]&amp;gt;
## 3 Africa    &amp;lt;tibble [637 × 5]&amp;gt;  
## 4 Americas  &amp;lt;tibble [470 × 5]&amp;gt;  
## 5 FSU       &amp;lt;tibble [139 × 5]&amp;gt;  
## 6 Oceania   &amp;lt;tibble [187 × 5]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;dplyr can perform operations on the top-level data frame, but with nplyr, we can perform operations on the nested data frames:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gm_nest_example &amp;lt;-
  gm_nest %&amp;gt;%
  nest_filter(country_data, year == max(year)) %&amp;gt;%
  nest_mutate(country_data, pop_millions = pop/1000000)

# each nested tibble is now filtered to the most recent year
gm_nest_example&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 2
##   continent country_data     
##   &amp;lt;fct&amp;gt;     &amp;lt;list&amp;gt;           
## 1 Asia      &amp;lt;tibble [43 × 6]&amp;gt;
## 2 Europe    &amp;lt;tibble [34 × 6]&amp;gt;
## 3 Africa    &amp;lt;tibble [53 × 6]&amp;gt;
## 4 Americas  &amp;lt;tibble [33 × 6]&amp;gt;
## 5 FSU       &amp;lt;tibble [9 × 6]&amp;gt; 
## 6 Oceania   &amp;lt;tibble [11 × 6]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# if we unnest, we can see that a new column for pop_millions has been created
gm_nest_example %&amp;gt;%
  slice_head(n = 1) %&amp;gt;%
  tidyr::unnest(country_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 43 × 7
##    continent country           year lifeExp        pop gdpPercap pop_millions
##    &amp;lt;fct&amp;gt;     &amp;lt;fct&amp;gt;            &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;      &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
##  1 Asia      Afghanistan       2007    43.8   31889923      975.       31.9  
##  2 Asia      Azerbaijan        2007    67.5    8017309     7709.        8.02 
##  3 Asia      Bahrain           2007    75.6     708573    29796.        0.709
##  4 Asia      Bangladesh        2007    64.1  150448339     1391.      150.   
##  5 Asia      Bhutan            2007    65.6    2327849     4745.        2.33 
##  6 Asia      Brunei            2007    77.1     386511    48015.        0.387
##  7 Asia      Cambodia          2007    59.7   14131858     1714.       14.1  
##  8 Asia      China             2007    73.0 1318683096     4959.     1319.   
##  9 Asia      Hong Kong, China  2007    82.2    6980412    39725.        6.98 
## 10 Asia      India             2007    64.7 1110396331     2452.     1110.   
## # … with 33 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;nplyr also supports grouped operations with &lt;code&gt;nest_group_by()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gm_nest_example &amp;lt;-
  gm_nest %&amp;gt;%
  nest_group_by(country_data, year) %&amp;gt;%
  nest_summarise(
    country_data,
    n = n(),
    lifeExp = median(lifeExp),
    pop = median(pop),
    gdpPercap = median(gdpPercap)
  )

gm_nest_example&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 2
##   continent country_data     
##   &amp;lt;fct&amp;gt;     &amp;lt;list&amp;gt;           
## 1 Asia      &amp;lt;tibble [58 × 5]&amp;gt;
## 2 Europe    &amp;lt;tibble [58 × 5]&amp;gt;
## 3 Africa    &amp;lt;tibble [13 × 5]&amp;gt;
## 4 Americas  &amp;lt;tibble [57 × 5]&amp;gt;
## 5 FSU       &amp;lt;tibble [44 × 5]&amp;gt;
## 6 Oceania   &amp;lt;tibble [56 × 5]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# unnesting shows summarised tibbles for each continent
gm_nest_example %&amp;gt;%
  slice(2) %&amp;gt;%
  tidyr::unnest(country_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 58 × 6
##    continent  year     n lifeExp      pop gdpPercap
##    &amp;lt;fct&amp;gt;     &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 Europe     1950    22    65.8 7408264      6343.
##  2 Europe     1951    18    65.7 7165515      6509.
##  3 Europe     1952    31    65.9 7124673      5210.
##  4 Europe     1953    17    67.3 7346100      6774.
##  5 Europe     1954    17    68.0 7423300      7046.
##  6 Europe     1955    17    68.5 7499400      7817.
##  7 Europe     1956    17    68.5 7575800      8224.
##  8 Europe     1957    31    67.5 7363802      6093.
##  9 Europe     1958    18    69.6 8308052.     8833.
## 10 Europe     1959    18    69.6 8379664.     9088.
## # … with 48 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;other-use-cases&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other use cases&lt;/h3&gt;
&lt;p&gt;In the previous set of examples, the output from nplyr’s nested operations could be obtained by unnesting and performing grouped dplyr operations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# we can use nplyr to perform operations on the nested data
gm_nest %&amp;gt;%
  nest_filter(country_data, year == max(year)) %&amp;gt;%
  nest_mutate(country_data, pop_millions = pop/1000000) %&amp;gt;%
  slice_head(n = 1) %&amp;gt;%
  tidyr::unnest(country_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 43 × 7
##    continent country           year lifeExp        pop gdpPercap pop_millions
##    &amp;lt;fct&amp;gt;     &amp;lt;fct&amp;gt;            &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;      &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
##  1 Asia      Afghanistan       2007    43.8   31889923      975.       31.9  
##  2 Asia      Azerbaijan        2007    67.5    8017309     7709.        8.02 
##  3 Asia      Bahrain           2007    75.6     708573    29796.        0.709
##  4 Asia      Bangladesh        2007    64.1  150448339     1391.      150.   
##  5 Asia      Bhutan            2007    65.6    2327849     4745.        2.33 
##  6 Asia      Brunei            2007    77.1     386511    48015.        0.387
##  7 Asia      Cambodia          2007    59.7   14131858     1714.       14.1  
##  8 Asia      China             2007    73.0 1318683096     4959.     1319.   
##  9 Asia      Hong Kong, China  2007    82.2    6980412    39725.        6.98 
## 10 Asia      India             2007    64.7 1110396331     2452.     1110.   
## # … with 33 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# in this case, we could have obtained the same result with tidyr and dplyr
gm_nest %&amp;gt;%
  tidyr::unnest(country_data) %&amp;gt;%
  group_by(continent) %&amp;gt;%
  filter(year == max(year)) %&amp;gt;%
  mutate(pop_millions = pop/1000000) %&amp;gt;%
  ungroup() %&amp;gt;%
  filter(continent == &amp;quot;Asia&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 43 × 7
##    continent country           year lifeExp        pop gdpPercap pop_millions
##    &amp;lt;fct&amp;gt;     &amp;lt;fct&amp;gt;            &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;      &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
##  1 Asia      Afghanistan       2007    43.8   31889923      975.       31.9  
##  2 Asia      Azerbaijan        2007    67.5    8017309     7709.        8.02 
##  3 Asia      Bahrain           2007    75.6     708573    29796.        0.709
##  4 Asia      Bangladesh        2007    64.1  150448339     1391.      150.   
##  5 Asia      Bhutan            2007    65.6    2327849     4745.        2.33 
##  6 Asia      Brunei            2007    77.1     386511    48015.        0.387
##  7 Asia      Cambodia          2007    59.7   14131858     1714.       14.1  
##  8 Asia      China             2007    73.0 1318683096     4959.     1319.   
##  9 Asia      Hong Kong, China  2007    82.2    6980412    39725.        6.98 
## 10 Asia      India             2007    64.7 1110396331     2452.     1110.   
## # … with 33 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why, then, might we need to use nplyr? Well, in other scenarios, it may be far more convenient to work with nested data frames or it may not even be possible to unnest!&lt;/p&gt;
&lt;p&gt;Consider a set of surveys that an organization might use to gather market data. It is common for organization to have separate surveys for separate purposes but to gather the same baseline set of data across all surveys (for example , a respondent’s age and gender may be recorded across all surveys, but each survey will have a different set of questions). Let’s use two fake surveys with the below questions for this example:&lt;/p&gt;
&lt;div id=&#34;survey-1-job&#34; class=&#34;section level6&#34;&gt;
&lt;h6&gt;Survey 1: Job&lt;/h6&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How old are you? (multiple choice)&lt;/li&gt;
&lt;li&gt;What city do you live in? (multiple choice)&lt;/li&gt;
&lt;li&gt;What field do you work in? (multiple choice)&lt;/li&gt;
&lt;li&gt;Overall, how satisfied are you with your job? (multiple choice)&lt;/li&gt;
&lt;li&gt;What is your annual salary? (numeric entry)&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;survey-2-personal-life&#34; class=&#34;section level6&#34;&gt;
&lt;h6&gt;Survey 2: Personal Life&lt;/h6&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How old are you? (multiple choice)&lt;/li&gt;
&lt;li&gt;What city do you live in? (multiple choice)&lt;/li&gt;
&lt;li&gt;What field do you work in? (mulitple choice)&lt;/li&gt;
&lt;li&gt;Overall, how satisfied are you with your personal life (multiple choice)&lt;/li&gt;
&lt;li&gt;Please provide any additional detail (text entry)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this scenario, both surveys are collecting demographic information — age, location, and industry — but differ in the remaining questions. A convenient way to get the response files into the environment would be to use &lt;code&gt;purrr::map()&lt;/code&gt; to read each file to a nested data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;path &amp;lt;- &amp;quot;https://raw.githubusercontent.com/markjrieke/nplyr/main/data-raw/&amp;quot;

surveys &amp;lt;- 
  tibble::tibble(survey_file = c(&amp;quot;job_survey&amp;quot;, &amp;quot;personal_survey&amp;quot;)) %&amp;gt;%
  mutate(survey_data = purrr::map(survey_file, ~readr::read_csv(paste0(path, .x, &amp;quot;.csv&amp;quot;))))

surveys&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 2
##   survey_file     survey_data            
##   &amp;lt;chr&amp;gt;           &amp;lt;list&amp;gt;                 
## 1 job_survey      &amp;lt;spec_tbl_df [500 × 6]&amp;gt;
## 2 personal_survey &amp;lt;spec_tbl_df [750 × 6]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;tidyr::unnest()&lt;/code&gt; can usually handle idiosyncracies in layout when unnesting, but in this case unnesting throws an error!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;surveys %&amp;gt;%
  tidyr::unnest(survey_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error:
## ! Can&amp;#39;t combine `Q5` &amp;lt;double&amp;gt; and `Q5` &amp;lt;character&amp;gt;.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is because the surveys share column names but not necessarily column types! In this case, both data frames contain a column named &lt;code&gt;Q5&lt;/code&gt;, but in &lt;code&gt;job_survey&lt;/code&gt; it’s a double and in &lt;code&gt;personal_survey&lt;/code&gt; it’s a character.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;surveys %&amp;gt;%
  slice(1) %&amp;gt;%
  tidyr::unnest(survey_data) %&amp;gt;%
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 500
## Columns: 7
## $ survey_file &amp;lt;chr&amp;gt; &amp;quot;job_survey&amp;quot;, &amp;quot;job_survey&amp;quot;, &amp;quot;job_survey&amp;quot;, &amp;quot;job_survey&amp;quot;, &amp;quot;j…
## $ survey_name &amp;lt;chr&amp;gt; &amp;quot;job&amp;quot;, &amp;quot;job&amp;quot;, &amp;quot;job&amp;quot;, &amp;quot;job&amp;quot;, &amp;quot;job&amp;quot;, &amp;quot;job&amp;quot;, &amp;quot;job&amp;quot;, &amp;quot;job&amp;quot;, &amp;quot;j…
## $ Q1          &amp;lt;dbl&amp;gt; 100, 81, 51, 81, 80, 32, 65, 57, 43, 94, 25, 83, 61, 66, 8…
## $ Q2          &amp;lt;chr&amp;gt; &amp;quot;Austin&amp;quot;, &amp;quot;San Antonio&amp;quot;, &amp;quot;Austin&amp;quot;, &amp;quot;Austin&amp;quot;, &amp;quot;Dallas&amp;quot;, &amp;quot;Fo…
## $ Q3          &amp;lt;chr&amp;gt; &amp;quot;Consulting&amp;quot;, &amp;quot;Consulting&amp;quot;, &amp;quot;Consulting&amp;quot;, &amp;quot;Technology&amp;quot;, &amp;quot;C…
## $ Q4          &amp;lt;chr&amp;gt; &amp;quot;Somewhat dissatisfied&amp;quot;, &amp;quot;Neither satisfied nor dissatisfi…
## $ Q5          &amp;lt;dbl&amp;gt; 163, 48, 190, 25, 143, 233, 43, 243, 158, 235, 245, 195, 2…&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;surveys %&amp;gt;%
  slice(2) %&amp;gt;%
  tidyr::unnest(survey_data) %&amp;gt;%
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 750
## Columns: 7
## $ survey_file &amp;lt;chr&amp;gt; &amp;quot;personal_survey&amp;quot;, &amp;quot;personal_survey&amp;quot;, &amp;quot;personal_survey&amp;quot;, &amp;quot;…
## $ survey_name &amp;lt;chr&amp;gt; &amp;quot;personal&amp;quot;, &amp;quot;personal&amp;quot;, &amp;quot;personal&amp;quot;, &amp;quot;personal&amp;quot;, &amp;quot;personal&amp;quot;…
## $ Q1          &amp;lt;dbl&amp;gt; 91, 32, 40, 23, 88, 69, 96, 40, 57, 40, 39, 70, 29, 38, 57…
## $ Q2          &amp;lt;chr&amp;gt; &amp;quot;Austin&amp;quot;, &amp;quot;San Antonio&amp;quot;, &amp;quot;San Antonio&amp;quot;, &amp;quot;Austin&amp;quot;, &amp;quot;Dallas&amp;quot;…
## $ Q3          &amp;lt;chr&amp;gt; &amp;quot;Energy&amp;quot;, &amp;quot;Healthcare&amp;quot;, &amp;quot;Consulting&amp;quot;, &amp;quot;Consulting&amp;quot;, &amp;quot;Techn…
## $ Q4          &amp;lt;chr&amp;gt; &amp;quot;Neither satisfied nor dissatisfied&amp;quot;, &amp;quot;Extremely satisfied…
## $ Q5          &amp;lt;chr&amp;gt; &amp;quot;Blandit eros! A, ligula facilisis imperdiet! Interdum pla…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could potentially get around this issue with unnesting by reading in all columns as characters via &lt;code&gt;readr::read_csv(x, col_types = cols(.default = &#34;c&#34;))&lt;/code&gt;, but this presents its own challenges. &lt;code&gt;Q5&lt;/code&gt; would still be better represented as a double in &lt;code&gt;job_survey&lt;/code&gt; and, from the survey question text, &lt;code&gt;Q4&lt;/code&gt; has similar, but distinctly different, meanings across the survey files.&lt;/p&gt;
&lt;p&gt;This is where nplyr can assist! Rather than malign the data types or create separate objects for each survey file, we can use nplyr to perform operations directly on the nested data frames.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;surveys &amp;lt;- 
  surveys %&amp;gt;%
  nest_mutate(survey_data,
              age_group = if_else(Q1 &amp;lt; 65, &amp;quot;Adult&amp;quot;, &amp;quot;Retirement Age&amp;quot;)) %&amp;gt;%
  nest_group_by(survey_data, Q3) %&amp;gt;%
  nest_add_count(survey_data, 
                 name = &amp;quot;n_respondents_in_industry&amp;quot;) %&amp;gt;%
  nest_mutate(survey_data, 
              median_industry_age = median(Q1)) %&amp;gt;%
  nest_ungroup(survey_data)

surveys %&amp;gt;%
  slice(1) %&amp;gt;%
  tidyr::unnest(survey_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 500 × 10
##    survey_file survey_name    Q1 Q2          Q3            Q4       Q5 age_group
##    &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    
##  1 job_survey  job           100 Austin      Consulting    Some…   163 Retireme…
##  2 job_survey  job            81 San Antonio Consulting    Neit…    48 Retireme…
##  3 job_survey  job            51 Austin      Consulting    Extr…   190 Adult    
##  4 job_survey  job            81 Austin      Technology    Extr…    25 Retireme…
##  5 job_survey  job            80 Dallas      Consulting    Extr…   143 Retireme…
##  6 job_survey  job            32 Fort Worth  Energy        Some…   233 Adult    
##  7 job_survey  job            65 Dallas      Consulting    Some…    43 Retireme…
##  8 job_survey  job            57 Houston     Healthcare    Some…   243 Adult    
##  9 job_survey  job            43 Dallas      Government S… Neit…   158 Adult    
## 10 job_survey  job            94 Fort Worth  Healthcare    Extr…   235 Retireme…
## # … with 490 more rows, and 2 more variables: n_respondents_in_industry &amp;lt;int&amp;gt;,
## #   median_industry_age &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;surveys %&amp;gt;%
  slice(2) %&amp;gt;%
  tidyr::unnest(survey_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 750 × 10
##    survey_file     survey_name    Q1 Q2          Q3        Q4    Q5    age_group
##    &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;    
##  1 personal_survey personal       91 Austin      Energy    Neit… Blan… Retireme…
##  2 personal_survey personal       32 San Antonio Healthca… Extr… Elem… Adult    
##  3 personal_survey personal       40 San Antonio Consulti… Some… Eget… Adult    
##  4 personal_survey personal       23 Austin      Consulti… Extr… Scel… Adult    
##  5 personal_survey personal       88 Dallas      Technolo… Neit… Aene… Retireme…
##  6 personal_survey personal       69 Fort Worth  Technolo… Neit… Inte… Retireme…
##  7 personal_survey personal       96 Houston     Healthca… Extr… Blan… Retireme…
##  8 personal_survey personal       40 Houston     Consulti… Extr… Scel… Adult    
##  9 personal_survey personal       57 Fort Worth  Energy    Extr… Pede… Adult    
## 10 personal_survey personal       40 Fort Worth  Healthca… Extr… Phar… Adult    
## # … with 740 more rows, and 2 more variables: n_respondents_in_industry &amp;lt;int&amp;gt;,
## #   median_industry_age &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Math Behind workboots</title>
      <link>https://www.thedatadiary.net/blog/2022-07-05-the-math-behind-workboots/</link>
      <pubDate>Tue, 05 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-07-05-the-math-behind-workboots/</guid>
      <description>


&lt;p&gt;Generating prediction intervals with workboots hinges on a few core concepts: bootstrap resampling, estimating prediction error for each resample, and aggregating the resampled prediction errors for each observation. The &lt;a href=&#34;https://rsample.tidymodels.org/reference/bootstraps.html&#34;&gt;&lt;code&gt;bootstraps()&lt;/code&gt; documentation from {rsample}&lt;/a&gt; gives a concise definition of bootstrap resampling:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A bootstrap sample is a sample that is the same size as the original data set that is made using replacement. This results in analysis samples that have multiple replicates of some of the original rows of the data. The assessment set is defined as the rows of the original data that were not included in the bootstrap sample. This is often referred to as the “out-of-bag” (OOB) sample.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This vignette will walk through the details of estimating and aggregating prediction errors — additional resources can be found in Davison and Hinkley’s book, &lt;a href=&#34;https://www.cambridge.org/core/books/bootstrap-methods-and-their-application/ED2FD043579F27952363566DC09CBD6A&#34;&gt;&lt;em&gt;Bootstrap Methods and their Application&lt;/em&gt;&lt;/a&gt;, or Efron and Tibshirani’s paper, &lt;a href=&#34;https://www.jstor.org/stable/2965703&#34;&gt;&lt;em&gt;Improvements on Cross-Validation: The Bootstrap .632+ Method&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;the-bootstrap-.632-method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Bootstrap .632+ Method&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;What follows here is largely a summary of &lt;a href=&#34;https://stats.stackexchange.com/questions/96739/what-is-the-632-rule-in-bootstrapping/96750#96750&#34;&gt;this explanation&lt;/a&gt; of the .632+ error rate by Benjamin Deonovic.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When working with bootstrap resamples of a dataset, there are two error estimates we can work with: the bootstrap training error and the out-of-bag (oob) error. Using the &lt;a href=&#34;https://modeldata.tidymodels.org/reference/Sacramento.html&#34;&gt;Sacramento housing dataset&lt;/a&gt;, we can estimate the training and oob error for a single bootstrap.&lt;/p&gt;
&lt;pre class=&#34;r fold-show&#34;&gt;&lt;code&gt;sacramento_boots
#&amp;gt; # Bootstrap sampling 
#&amp;gt; # A tibble: 1 × 2
#&amp;gt;   splits            id        
#&amp;gt;   &amp;lt;list&amp;gt;            &amp;lt;chr&amp;gt;     
#&amp;gt; 1 &amp;lt;split [699/261]&amp;gt; Bootstrap1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using a &lt;a href=&#34;https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#k-NN_regression&#34;&gt;k-nearest-neighbor regression model&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Root-mean-square_deviation#:~:text=The%20root%2Dmean%2Dsquare%20deviation,estimator%20and%20the%20values%20observed.&#34;&gt;rmse&lt;/a&gt; as our error metric, we find that the training and oob error differ, with the training error lesser than the oob error.&lt;/p&gt;
&lt;pre class=&#34;r fold-show&#34;&gt;&lt;code&gt;sacramento_train_err
#&amp;gt; [1] 0.08979873
sacramento_oob_err
#&amp;gt; [1] 0.1661675&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The training error is overly optimistic in the model’s performance and likely to under-estimate the prediction error. We are interested in the model’s performance on new data. The oob error, on the other hand, is likely to over-estimate the prediction error! This is due to non-distinct observations in the bootstrap sample that results from sampling with replacement. Given that &lt;a href=&#34;https://stats.stackexchange.com/questions/88980/why-on-average-does-each-bootstrap-sample-contain-roughly-two-thirds-of-observat?lq=1&#34;&gt;the average number of distinct observations in a bootstrap training set is about &lt;code&gt;0.632 * total_observations&lt;/code&gt;&lt;/a&gt;, Efron and Tibshirani proposed a blend of the training and oob error with the 0.632 estimate:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
Err_{.632} &amp;amp; = 0.368 Err_{train} + 0.632 Err_{oob}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r fold-show&#34;&gt;&lt;code&gt;sacramento_632 &amp;lt;- 0.368 * sacramento_train_err + 0.632 * sacramento_oob_err
sacramento_632
#&amp;gt; [1] 0.1380638&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If, however, the model is highly overfit to the bootstrap training set, the training error will approach 0 and the 0.632 estimate will &lt;em&gt;under estimate&lt;/em&gt; the prediction error.&lt;/p&gt;
&lt;p&gt;An example from &lt;a href=&#34;http://appliedpredictivemodeling.com/&#34;&gt;&lt;em&gt;Applied Predictive Modeling&lt;/em&gt;&lt;/a&gt; shows that as model complexity increases, the reported resample accuracy by the 0.632 estimate continues to increase whereas other resampling strategies report diminishing returns:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5731043/157986232-9c32c1c2-a7ed-4f9f-b28e-7d8ccb7ac41c.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As an alternative to the 0.632 estimate, Efron &amp;amp; Tibshirani also propose the 0.632+ estimate, which re-weights the blend of training and oob error based on the model overfit rate:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
Err_{0.632+} &amp;amp; = (1 - w) Err_{train} + w Err_{oob} \\
\\
w &amp;amp; = \frac{0.632}{1 - 0.368 R} \\
\\
R &amp;amp; = \frac{Err_{oob} - Err_{train}}{\gamma - Err_{train}}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; represents the overfit rate and &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is the no-information error rate, estimated by evaulating all combinations of predictions and actual values in the bootstrap training set.&lt;/p&gt;
&lt;pre class=&#34;r fold-show&#34;&gt;&lt;code&gt;sacramento_632_plus &amp;lt;- (1 - w) * sacramento_train_err + w * sacramento_oob_err
sacramento_632_plus
#&amp;gt; [1] 0.1450502&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When there is no overfitting (i.e., &lt;span class=&#34;math inline&#34;&gt;\(R = 0\)&lt;/span&gt;) the 0.632+ estimate will equal the 0.632 estimate. In this case, however, the model is overfitting the training set and the 0.632+ error estimate is pushed a bit closer to the oob error.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prediction-intervals-with-many-bootstraps&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Prediction intervals with many bootstraps&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Root-mean-square_deviation#Formula&#34;&gt;For an unbiased estimator, rmse is the standard deviation of the residuals&lt;/a&gt;. With this in mind, we can modify our predictions to include a sample from the residual distribution (for more information, see Algorithm 6.4 from Davison and Hinkley’s &lt;em&gt;Bootstrap Methods and their Application&lt;/em&gt;):&lt;/p&gt;
&lt;pre class=&#34;r fold-show&#34;&gt;&lt;code&gt;set.seed(999)
resid_train_add &amp;lt;- rnorm(length(preds_train), 0, sacramento_632_plus)
preds_train_mod &amp;lt;- preds_train + resid_train_add&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus far, we’ve been working with a single bootstrap resample. When working with a single bootstrap resample, adding this residual term gives a pretty poor estimate for each observation:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-07-05-the-math-behind-workboots/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With workboots, however, we can repeat this process over many bootstrap datasets to generate a prediction distribution for each observation:&lt;/p&gt;
&lt;pre class=&#34;r fold-show&#34;&gt;&lt;code&gt;library(workboots)
# fit and predict price in sacramento_test from 100 models
# the default number of resamples is 2000 - dropping here to speed up knitting
set.seed(555)
sacramento_pred_int &amp;lt;-
  sacramento_wf %&amp;gt;%
  predict_boots(
    n = 100,
    training_data = sacramento_train,
    new_data = sacramento_test
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-07-05-the-math-behind-workboots/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This methodology produces prediction distributions that are &lt;a href=&#34;https://markjrieke.github.io/workboots/articles/Estimating-Linear-Intervals.html&#34;&gt;consistent with what we might expect from linear models&lt;/a&gt; while making no assumptions about model type (i.e., we can use a non-parametric model; in this case, a k-nearest neighbors regression).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Estimate your uncertainty</title>
      <link>https://www.thedatadiary.net/blog/2022-06-12-estimate-your-uncertainty/</link>
      <pubDate>Sun, 12 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-06-12-estimate-your-uncertainty/</guid>
      <description>


&lt;p&gt;I recently picked up &lt;a href=&#34;http://varianceexplained.org/about/&#34;&gt;David Robinson’s&lt;/a&gt; book, &lt;a href=&#34;http://varianceexplained.org/r/empirical-bayes-book/&#34;&gt;Introduction to Empirical Bayes&lt;/a&gt;. It’s available online for a price of your own choosing (operating under a “pay-what-you-want” model), so you can technically pick it up for free, but it’s well worth the suggested price of $9.95. The book has a particular focus on practical steps for implementing Bayesian methods with code, which I appreciate. I’ve made it through Part I (of four), which makes for a good stopping point to practice what I’ve read.&lt;/p&gt;
&lt;p&gt;The first section is highly focused on modeling the probability of success/failure of some binary outcome using a &lt;a href=&#34;https://en.wikipedia.org/wiki/Beta_distribution&#34;&gt;beta distribution&lt;/a&gt;. This is highly relevant to my work as an analyst, where whether or not a patient responded positively to a particular question on a survey can be modeled with this method. Thus far, however, I’ve taken the &lt;a href=&#34;https://en.wikipedia.org/wiki/Frequentist_inference&#34;&gt;frequentist&lt;/a&gt; approach to analyses, which assumes we know nothing about what the data ought to look like prior to analyzing it. This is largely because I didn’t know of a robust way to estimate a &lt;a href=&#34;https://en.wikipedia.org/wiki/Prior_probability&#34;&gt;prior&lt;/a&gt; for a Bayesian analysis.&lt;/p&gt;
&lt;p&gt;Thankfully, however, the book walks through examples of exactly how to do this! We can use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&#34;&gt;maximum likelihood estimator&lt;/a&gt; to estimate a reasonable prior given the current data. That’s quite a bit of statistical mumbo-jumbo — in this post I’ll walk through an example that spells it out a bit more clearly using fake hospital satisfaction data (N.B.; this is largely a recreation of the steps taken in the book — practice makes perfect!).&lt;/p&gt;
&lt;div id=&#34;setting-up-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting up the data&lt;/h2&gt;
&lt;p&gt;First, let’s simulate responses to patient satisfaction surveys. I tend to look at patient satisfaction scores across individual hospital units (e.g., ED, ICU, IMU, etc.). Units can have varying numbers of discharges, so we’ll use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Log-normal_distribution&#34;&gt;log-normal&lt;/a&gt; distribution to estimate the number of responses for each unit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# simulate 1,500 hospital units with an average of 150 survey returns per unit
set.seed(123)
survey_data &amp;lt;- 
  rlnorm(1500, log(150), 1.5) %&amp;gt;%
  as_tibble() %&amp;gt;%
  rename(n = value)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-06-12-estimate-your-uncertainty/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The spectrum of responses is incredibly broad — some units have a massive number of returns (in the tens of thousands!) while others have just a handful. This is fairly consistent with the real-world data that I’ve seen (though the units on the high-side are a bit over-represented here).&lt;/p&gt;
&lt;p&gt;Next, let’s assume that there is some true satisfaction rate that is associated with each unit. If each unit had an infinite number of survey returns, the satisfaction rate from the survey returns would approach this true value. In this case, we’ll set the true satisfaction for each unit randomly but have it hover around 66%.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set the true satisfaction to be different for each unit, but hover around 66%
set.seed(234)
survey_data &amp;lt;- 
  survey_data %&amp;gt;%
  rowwise() %&amp;gt;%
  mutate(true_satisfaction = rbeta(1, 66, 34))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although there is a true satisfaction associated with each unit, we wouldn’t expect that the reported survey scores would match this exactly. This is especially true when there are few responses — if a unit has a true satisfaction rate of 75% but only 3 responses, it’s impossible for the reported score to match the underlying true rate!&lt;/p&gt;
&lt;p&gt;We can simulate the number of patients who responded positively (in survey terms, the number of “topbox” responses) by generating &lt;code&gt;n&lt;/code&gt; responses for each unit using a &lt;a href=&#34;https://en.wikipedia.org/wiki/Binomial_distribution&#34;&gt;binomial distribution&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# simulate the number of patients responding with the topbox value
# we *know* the true value, but the actual score may vary!
set.seed(345)
survey_data &amp;lt;-
  survey_data %&amp;gt;%
  mutate(n = round(n),
         topbox = rbinom(1, n, true_satisfaction)) %&amp;gt;%
  ungroup() %&amp;gt;%
  
  # name each unit
  rowid_to_column() %&amp;gt;%
  mutate(unit = paste(&amp;quot;Unit&amp;quot;, rowid)) %&amp;gt;%
  relocate(unit) %&amp;gt;%
  
  # remove the true satisfaction so we don&amp;#39;t know what it is!
  select(-rowid, -true_satisfaction)

# find patient satisfaction scores
survey_data &amp;lt;- 
  survey_data %&amp;gt;%
  mutate(score = topbox/n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-06-12-estimate-your-uncertainty/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, most of our simulated data hovers around a score of 66%. However, there are a few scores at the extremes of 0% and 100% — given how we simulated the data, it is unlikely that these units are &lt;em&gt;really&lt;/em&gt; performing so poorly/so well and it’s likelier that they just have few returns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# which units have the highest scores?
survey_data %&amp;gt;%
  arrange(desc(score)) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;unit&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;topbox&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 26&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 591&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 616&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 811&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 943&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1217&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1435&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1437&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 863&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9473684&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 372&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9230769&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# which units have the lowest scores?
survey_data %&amp;gt;%
  arrange(score) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;unit&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;topbox&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1092&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 248&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2500000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1120&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2857143&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 416&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3333333&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 456&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3333333&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 972&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3333333&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 113&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3846154&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 260&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 695&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1352&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4117647&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As expected, the units on either end of the spectrum aren’t necessarily outperforming/underperforming — they simply don’t have a lot of survey responses! We can use Bayesian inference to estimate the true satisfaction rate by specifying and updating a prior!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generating-a-prior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generating a prior distribution&lt;/h2&gt;
&lt;p&gt;When looking at the entire dataset, the distribution of scores is thrown off a bit by the units with few responses. If we restrict the dataset to only the units that have more than 30 responses (which, &lt;a href=&#34;https://www.thedatadiary.net/blog/2022-04-28-30-is-not-statistical/&#34;&gt;as I’ve written about before&lt;/a&gt;, isn’t necessarily a data-driven cutoff for analysis) we can get a clearer idea of the distribution of the scores.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;survey_data_filtered &amp;lt;-
  survey_data %&amp;gt;%
  filter(n &amp;gt; 30)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-06-12-estimate-your-uncertainty/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Alternatively, we can represent this distribution with a density plot:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-06-12-estimate-your-uncertainty/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This looks suspiciously like a beta distribution! A beta distribution’s shape can be defined by two parameters — alpha and beta. Varying these parameters lets us adjust the center and width to match any possible beta distribution.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/7/78/PDF_of_the_Beta_distribution.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What may make sense would be to use &lt;em&gt;this distribution&lt;/em&gt; as our prior. I.e., if we have no responses for a unit, we can probably guess that their score would be somewhere around 66% with some healthy room on either side for variability. To do so, we need to estimate an appropriate alpha and beta — rather than guess the values using trial and error we can pass the work off to our computer to find parameters that &lt;strong&gt;maximize&lt;/strong&gt; the &lt;strong&gt;likelihood&lt;/strong&gt; that our &lt;strong&gt;estimated distribution&lt;/strong&gt; matches the true distribution (hence the name, &lt;em&gt;maximum likelihood estimator&lt;/em&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stats4)

# log-likelihood function
log_likelihood &amp;lt;- function(alpha, beta) {
  -sum(dbeta(survey_data_filtered$score, alpha, beta, log = TRUE))
}

# pass various alphas &amp;amp; betas to `log_likelihood` 
# to find combination that maximizes the likelihood!
params &amp;lt;- 
  mle(
    log_likelihood, 
    start = list(alpha = 50, beta = 50),
    lower = c(1, 1)
  )

# extract alpha &amp;amp; beta
params &amp;lt;- coef(params)
alpha0 &amp;lt;- params[1]
beta0 &amp;lt;- params[2]

print(paste(&amp;quot;alpha:&amp;quot;, round(alpha0, 1), &amp;quot;beta:&amp;quot;, round(beta0, 1)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;alpha: 39.7 beta: 20.5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How well does a beta distribution defined by these parameters match our actual data?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-06-12-estimate-your-uncertainty/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a pretty good representation of our initial data! When we have no survey responses, we can use a beta distribution with the initial parameters as specified by the maximum likelihood estimation. As a unit gets more responses, we can update our estimation to rely more heavily on the data rather than the prior:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# update alpha &amp;amp; beta as new responses come in!
alpha_new &amp;lt;- alpha0 + n_topbox
beta_new &amp;lt;- beta0 + n - n_topbox&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;updating-our-priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Updating our priors&lt;/h2&gt;
&lt;p&gt;With a prior distribution defined by &lt;code&gt;alpha0&lt;/code&gt; and &lt;code&gt;beta0&lt;/code&gt;, we can upgrade our frequentest estimation of each unit’s score to a Bayesian estimation!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# empirical bayes estimation of satisfaction score
survey_eb &amp;lt;-
  survey_data %&amp;gt;%
  mutate(eb_estimate = (topbox + alpha0) / (n + alpha0 + beta0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What are the top and bottom performing units by this new Bayesian estimation?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# which units have the highest estimated scores?
survey_eb %&amp;gt;%
  arrange(desc(eb_estimate)) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;unit&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;topbox&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;score&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;eb_estimate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 133&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;133&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8312500&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7841640&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1004&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;123&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;103&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8373984&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7787827&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 172&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;165&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;133&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8060606&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7667547&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1042&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;372&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;291&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7822581&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7650930&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1294&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1409&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1083&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7686302&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7641391&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 892&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;349&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;273&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7822350&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7641085&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 306&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;247&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;195&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7894737&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7639102&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1249&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1234&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;943&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7641815&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7592901&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 427&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5469&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4151&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7590053&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7579168&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 920&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1637&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1243&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7593158&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7557585&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# which units have the lowest estimated scores?
survey_eb %&amp;gt;%
  arrange(eb_estimate) %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;unit&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;topbox&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;score&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;eb_estimate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 613&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1886&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;932&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4941676&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4992689&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 760&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;112&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;49&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4375000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5149645&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 363&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;226&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;112&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4955752&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5299674&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 316&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;431&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;224&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5197216&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5368008&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1032&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;235&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;119&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5063830&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5375222&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 1093&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;354&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;183&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5169492&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5376064&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 749&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5286&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2839&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5370791&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5384528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 291&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;865&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;460&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5317919&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5400741&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 515&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;60&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4333333&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5463929&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Unit 622&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;242&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;127&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5247934&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5515432&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There are a few things that are worth noting with these estimates:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The estimated score is not the same as the actual reported score! As more responses come in, however, the estimated score converges to the actual.&lt;/li&gt;
&lt;li&gt;The prior pulls estimated scores towards the prior mean — low scores are pulled up a bit and high scores are pulled down a bit.&lt;/li&gt;
&lt;li&gt;The top (and bottom) performing units are no longer dominated by units with few returns!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also estimate the uncertainty around the estimated score with a &lt;a href=&#34;https://en.wikipedia.org/wiki/Credible_interval&#34;&gt;credible interval&lt;/a&gt;. Credible intervals are the Bayesian counterpart to a frequentist’s &lt;a href=&#34;https://en.wikipedia.org/wiki/Confidence_interval&#34;&gt;confidence interval&lt;/a&gt; — both estimate the region that the true value could fall in given a certain probability — credible intervals, however, are informed by the prior distribution.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-06-12-estimate-your-uncertainty/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Because credible intervals are informed in part by the prior, they are tighter than their confidence interval counterparts. Like with the estimated score, however, as n-size increases, the Bayesian and frequentist interval estimations converge. In the absence of larger swathes of data, Bayesian methods can offer additional insight into our data by means of a prior distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-closing-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some closing thoughts&lt;/h2&gt;
&lt;p&gt;Overall, this has been a fairly glowing review of the methods laid out in the first section of &lt;em&gt;Introduction to Empirical Bayes&lt;/em&gt;. That being said, Bayesian methods of inference are not inherently &lt;em&gt;better&lt;/em&gt; than frequentist methods — while they can offer additional context via a prior, there are situations where frequentist methods are preferred. From a math perspective, the prior provides diminishing returns as sample size increases, so it may be better forgoe Bayesian analysis when sample sizes are large. From an organizational perspective, Bayesian inference may be difficult to explain. In my own work, it’s highly unlikely that I’ll use Bayesian inference in any critical projects any time soon — I can imagine a lengthy uphill battle trying to explain the difference between the reported score and the estimated score informed by a prior.&lt;/p&gt;
&lt;p&gt;Finally, there are a few things in this toy analysis that I am hoping to improve upon as I progress further through the book:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As I mentioned above and &lt;a href=&#34;https://www.thedatadiary.net/blog/2022-04-28-30-is-not-statistical/&#34;&gt;in previous writings&lt;/a&gt;, using &lt;code&gt;n = 30&lt;/code&gt; is a relatively arbitrary cutoff point for analysis. In this case, the prior distribution is fairly sensitive to the cutoff point selected — I am hoping that later sections in the book highilight more robust ways of partitioning data for setting priors.&lt;/li&gt;
&lt;li&gt;In the above analysis we’re only examining one variable (univariate analysis) — I am looking forward to extending these methods to multivariate analyses and regressions.&lt;/li&gt;
&lt;li&gt;The beta distribution is appropriate for modeling the probability distribution of binary outcomes. In this example, where the outcome is simply the proportion of patients that responded favorably to the survey, modeling the outcome with a beta distribution is appropriate (responses can either be in the “topbox” or not). When there are more than two possible outcomes — for example, when trying to model &lt;a href=&#34;https://en.wikipedia.org/wiki/Net_promoter_score&#34;&gt;Net Promoter Score&lt;/a&gt; as the proportion of “promoters,” “passives,” and “detractors” — the more general &lt;a href=&#34;https://en.wikipedia.org/wiki/Dirichlet_distribution&#34;&gt;Dirichlet distribution&lt;/a&gt; is more appropriate.&lt;/li&gt;
&lt;li&gt;I’m hoping also that the book covers methods for dealing with time-dependent data. For example, we’d expect that concerted efforts (or lack thereof) by the hospital units could significantly impact the underlying “true satisfaction” that we’re attempting to estimate via surveying. We expect that more recent survey responses should be more impactful in informing our posterior estimation, but I’ve yet to find any robust literature on how to weight the recency of responses. In the past, I’ve used &lt;a href=&#34;https://en.wikipedia.org/wiki/Exponential_decay&#34;&gt;exponentional decay&lt;/a&gt; to reduce the weight of old responses, but this feels a bit arbitrary.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overall, this has been a long way of saying that I’m happy with the book so far and I’m excited to see what comes next as I continue reading!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical Data Visualization Tips for Excel Users</title>
      <link>https://www.thedatadiary.net/blog/2022-05-31-practical-data-vizualization-tips-for-excel-users/</link>
      <pubDate>Tue, 31 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-05-31-practical-data-vizualization-tips-for-excel-users/</guid>
      <description>


&lt;p&gt;I am an avid R user and will always advocate that others use R (or another programming language) for generating reproducible visualizations. In just about every organization, however, Excel plays an important role in an analyst’s toolkit. In this post, I’ll share some visualization design practices that I picked up while learning R but are ubiquitous and transferable to Excel (most of these suggestions are ripped directly from &lt;a href=&#34;https://www.williamrchase.com/about/&#34;&gt;Will Chase’s&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=h5cTacaWE6I&#34;&gt;“Glamour of Graphics”&lt;/a&gt; talk from rstudio::conf(2020), which has &lt;em&gt;heavily informed&lt;/em&gt; how I think about visualization design).&lt;/p&gt;
&lt;div id=&#34;a-motivating-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A motivating example&lt;/h2&gt;
&lt;p&gt;For the purposes of this exercise, let’s use fake patient satisfaction data from the Sesame Street Health System, which includes several hospitals and campuses. Let’s say that our system-wide patient satisfaction for the current fiscal year looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/system_px.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Most of the hospitals have pretty high satisfaction scores — generally greater than &lt;strong&gt;75%&lt;/strong&gt;! The overall system score, however, sits at &lt;strong&gt;65%&lt;/strong&gt;. Concerned that there may be an error in the data pipeline or dashboard, your boss asks that you investigate what’s going on and provide an update at the next team meeting.&lt;/p&gt;
&lt;p&gt;At first glance, it may be pretty obvious what’s going on — Big Bird Emergency has a pretty low satisfaction score and you know from experience that it’s a larger hospital that generates a lot of survey returns, which may be driving the score down. Since you’re presenting, however, it’s best to use a visualization to communicate this.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-together-a-bad-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Putting together a bad plot&lt;/h2&gt;
&lt;p&gt;Downloading the hospital data and opening in Excel confirms that Big Bird Emergency has far more survey returns than other hospitals.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/system_raw.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A quick plot that &lt;em&gt;technically&lt;/em&gt; includes all the information needed may look something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_01.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While this does answer the question originally asked, the answer is not &lt;strong&gt;clearly communicated by a quick glance&lt;/strong&gt;. Viewers who know what was originally asked have to do some extra mental work to decode the plot, and viewers who see this without the original context may not find anything useful. Our goal should be to provide a visualization that:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Clearly communicates the message we want to convey.&lt;/li&gt;
&lt;li&gt;Is able to stand alone in other contexts and still communicate the same message.&lt;/li&gt;
&lt;li&gt;Is visually appealing.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s get started!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-together-a-good-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Putting together a good plot&lt;/h2&gt;
&lt;div id=&#34;changing-to-a-bar-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Changing to a bar plot&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_02.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The first order of business is to convert the plot from a pie chart to a bar plot. Pie charts are loved by executives but loathed by visualization practitioners, since information is encoded in each slice’s angle and &lt;a href=&#34;https://en.wikipedia.org/wiki/Graphical_perception&#34;&gt;differences in angle are difficult for the human eye to detect&lt;/a&gt;. Bar plots encode the same information with relative position on a scale, which is the most effective method for showing differences. This also has the added benefit in Excel of automatically converting each hospital to the same color, which reduces a lot of the visual noise that was in the original plot.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;flipping-axes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Flipping axes&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_03.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In order to read the hospital names in the previous plot, viewers need to crane their necks to align with the axis text. The angled text also takes up a lot of whitespace and makes the important part — the actual data — look a bit squished. Changing to a horizontal bar plot alleviates both of these issues (horizontal bar plots are preferred over vertical ones in general for this reason).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ordering-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ordering the data&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_04.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unordered categories in a plot can be messy and visually confusing — the viewer’s eye needs to dart around to determine which values are greater than other ones. Ordering the categories reduces this cognitive load and allows the viewer to simply read through the list. In this case (and in most cases), we don’t care about the exact values (just the &lt;em&gt;relative difference&lt;/em&gt; between values), so we can also remove the data labels.&lt;/p&gt;
&lt;p&gt;To order a plot in Excel, we don’t actually need to do anything to the plot itself — simply turn the raw data into a table then arrange the rows by &lt;code&gt;survey_returns&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/ordered_campuses.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;giving-the-plot-a-narrative&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Giving the plot a narrative&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_05.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The original title, “FYTD Surveys”, while technically informative, is uninspiring. We’re putting together this plot to answer the specific question, &lt;em&gt;why is the system satisfaction score 65% when most hospitals have a higher score?&lt;/em&gt; A good visualization will directly answer this without needing additional context from the analyst — the title is a great place to state that &lt;em&gt;Big Bird Emergency is driving the system score down&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Also note that there are no axis labels on this plot. Axis labels are often unnecessary — they take up valuable whitespace with information that is either readily apparent (I don’t need a label to know that the y-axis refers to each hospital!) or encoded elsewhere. When possible, remove axis labels and describe the necessary detail elsewhere (i.e., the title or subtitle).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;highlighting-the-important-bits&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Highlighting the important bits&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_06.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Color can be a great way to draw our attention to a particular portion of a plot. In this case, not all of the hospitals are equally important in this plot’s narrative — we’re making a distinct point regarding Big Bird Emergency. Highlighting the text and bar for Big Bird Emergency in yellow (Big Bird is, after all, a big yellow bird) while graying out the other bars visually communicates to the viewer this is the hospital deserving the most attention in this plot.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;realigning-the-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Realigning the plot&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_07.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the vast majority of cases, a left-aligned the title/subtitle is strongly preferred over center-aligned one. In western countries, we tend to naturally orient our attention in the top-left corner of plots when we first view them, then migrate our gaze downwards and leftwards (eye-tracking studies confirm this, however I can’t seem to find the source I heard this from, so you’ll just have to take my word for it here). By aligning the title to the left, we reduce how much the viewer needs to dart their eyes around the plot to understand it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-the-final-touches&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding the final touches&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;pics/plot_08.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Adding some final formatting touches to polish up the visualization not only improves the quality of the plot but also shows the viewer that you’re serious about your craft and willing to go the extra mile to really make a visualization shine. In this case, applying comma-formatting to the x-axis, changing the font, and updating the background to an off-white are all minor edits, but their effects have a big impact on the visualization’s overall presentation.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;My buddy is an electrician and told me a few months ago that he always leaves the screws in a vertical position on jobs as a sign of craftsmanship. Been thinking ever since what my “vertical screws” equivalent is for product design. &lt;a href=&#34;https://t.co/dM9CFEG8MF&#34;&gt;pic.twitter.com/dM9CFEG8MF&lt;/a&gt;&lt;/p&gt;&amp;mdash; Mike Rundle (@flyosity) &lt;a href=&#34;https://twitter.com/flyosity/status/1495087213150879747?ref_src=twsrc%5Etfw&#34;&gt;February 19, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>&#34;30 is not Statistical&#34;</title>
      <link>https://www.thedatadiary.net/blog/2022-04-28-30-is-not-statistical/</link>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-04-28-30-is-not-statistical/</guid>
      <description>


&lt;p&gt;In my role as an analyst, my team and I are required to put together reports that summarize each hospital’s patient satisfaction performance in a table. These are reviewed by our system’s executive leadership team and the hospital directors in monthly operational reviews (MORs). The format I inherited, loosely recreated below with fake data, color codes each month’s performance against the hospital’s goal: green when outperforming and red when underperforming.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/table.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;“But wait!” you may ask, “what do the gray cells mean?” &lt;em&gt;That&lt;/em&gt;, dear reader, has been the source of most of my organizational frustration in this role. When the number of surveys returned is less than 30 for a given month, we simply gray-out that cell under the guise of the phrase, “30 is not statistical.”&lt;/p&gt;
&lt;p&gt;I don’t think this practice (or something similar) is unique to my organization — I’ve seen similar outputs from previous employers and in other companies’ published reports. While this isn’t the best use of the underlying data, I understand &lt;em&gt;why&lt;/em&gt; this sort of method gets adapted into so many organizational lexicons: companies want their decisions to be based on data and their understanding, albeit incorrect, is that a sample size less than 30 doesn’t provide meaningful info. For this reason, I think it’s important to explore &lt;strong&gt;where&lt;/strong&gt; this sentiment came from, &lt;strong&gt;what&lt;/strong&gt; the problems with this data-presentation style are, &lt;strong&gt;what&lt;/strong&gt; I think would be a better way of presenting the data, and ultimately &lt;strong&gt;why&lt;/strong&gt; companies may be hesitant to update their methodology.&lt;/p&gt;
&lt;div id=&#34;where-does-this-come-from&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Where does this come from?&lt;/h2&gt;
&lt;p&gt;At first glance, 30 is a pretty arbitrary number to use as a cutoff. In this case, this cutoff can cause downstream issues with interpreting the data because the difference between &lt;code&gt;n = 30&lt;/code&gt; and &lt;code&gt;n = 31&lt;/code&gt; is so visually distinct! In our case, the cutoff of 30 was passed down from one of our previous survey vendors, but I believe that the wider root of why this value appears has to do with the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34;&gt;central limit theorem&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The central limit theorem states that as a sample size increases, the probability distribution of the sample mean approaches a normal distribution, &lt;em&gt;regardless of the source distribution!&lt;/em&gt; As a rule of thumb, this theorem holds true when the &lt;a href=&#34;https://stats.stackexchange.com/questions/2541/what-references-should-be-cited-to-support-using-30-as-a-large-enough-sample-siz&#34;&gt;sample size is at least 30&lt;/a&gt;. In practice, this means that when there are at least 30 samples, we can generally approximate the distribution as normal. The central limit theorem is incredibly useful and an important foundation for a wide array of statistical techniques. Stating that the data doesn’t meet the criteria for the central limit theorem to hold, however, is very different from saying that data is worthless when the sample size is less than 30!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problems-with-this-approach&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Problems with this approach&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Sometimes, the source distribution is known, and whether or not the central limit theorem holds is irrelevant! In my particular case, I am often dealing with patient satisfaction data that lies on a 0-100% scale. This is the perfect use case for modeling the sample with the &lt;a href=&#34;https://en.wikipedia.org/wiki/Beta_distribution&#34;&gt;beta distribution&lt;/a&gt; (which is bound by 0 and 1).&lt;/li&gt;
&lt;li&gt;As mentioned above, graying-out samples where the sample size is less than 30 visually communicates that some months can be ignored. This is a waste of valuable data! While it is true that a larger sample size implies greater confidence in the score, the confidence interval widths for &lt;code&gt;n = 30&lt;/code&gt; and &lt;code&gt;n = 31&lt;/code&gt; are not &lt;em&gt;so different&lt;/em&gt; and we can still estimate the uncertainty with the smaller sample.&lt;/li&gt;
&lt;li&gt;Tabular data is incredibly difficult to parse at-a-glance! &lt;a href=&#34;https://www.rstudio.com/resources/rstudioconf-2020/effective-visualizations/&#34;&gt;Research shows&lt;/a&gt; that spatial encoding (e.g., length, position) is the most interpretable mode of data presentation. Intuitively, it makes sense — there’s a lot less mental overhead involved in looking at a set of points and comparing positions &lt;em&gt;collectively&lt;/em&gt; than stringing together several comparisons of individual pairs of numbers in your head.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;a-better-approach&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A better approach&lt;/h2&gt;
&lt;p&gt;When the underlying distribution is known, a better approach would be to display the data in a plot, regardless of n-size, and use a confidence interval to indicate uncertainty. In this case, we can plot each survey’s scores over time with a line and use a shaded area for the confidence interval.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-04-28-30-is-not-statistical/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This, I believe, has a few benefits.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;This is simply more visually appealing than the table. I (along with most people, I’d assume) prefer the look of a well formatted plot over a well formatted table (even if well formatted, a table is still a big block of text).&lt;/li&gt;
&lt;li&gt;It is far easier to discern the overall trend. Instead of reading and trying to compare values, we can simply see which direction the line is moving!&lt;/li&gt;
&lt;li&gt;Most imortantly, &lt;em&gt;we do not throw out valuable data because of sample size.&lt;/em&gt; We actually end up encoding &lt;em&gt;more&lt;/em&gt; information — n-size, which was missing from the table, is encoded in the width of the confidence interval (a smaller confidence interval indicates a larger sample). In this toy example, surveys B and D included a few months with fewer than 30 returns — can you tell which months they were without looking at the table?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The tradeoff is that we can no longer explicitly see each month’s score and it is a bit harder to tell if a hospital is meeting the goal when the score is close. In my experience, however, this is not how formatted tables are used — executives that I interact with typically try to determine overall trends from tabular data!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;organizational-resistance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Organizational resistance&lt;/h2&gt;
&lt;p&gt;While the changes suggested here have clear benefits over the table, I’ve thus far been unsuccessful in any attempts to change the reporting methodology and I suspect that similar efforts at other companies would encounter similar organizational resistance. Much of what’s stated below is anec-data, but I assume will ring true to anyone who has struggled with getting their proposed operational changes implemented.&lt;/p&gt;
&lt;p&gt;As companies scale, it becomes more &amp;amp; more difficult to implement change. On top of that, some industries (including Healthcare, the one I work in) are similarly inertial on an industry level. In this particular case, changing a report’s format may seem small in the grand scheme of things, but this is the same format the executive team has been seeing since 2017! The system executives and individual hospital leaders have a rapport and vernacular built around these monthly reports in this format — updating the format similarly requires that the executives and leaders update their long-held understanding and language built around tabular data.&lt;/p&gt;
&lt;p&gt;Tabular data in general shows up in reports across industries. My hunch is that the main driver of this is the widespread integration of Microsoft Excel as the workhorse for most analysts’ tasks. Excel get wide use as a calculator, a data storage system (eek!), and a presentation tool. Most analysts are incredibly comfortable working in Excel and while it is possible to create plots that show both the score and confidence interval, it is far simpler to apply a bit of conditional formatting and submit the raw data as the report itself.&lt;/p&gt;
&lt;p&gt;This is not to say that tabular reports have no use — when individual values are important, tabular reports are preferred! If, however, the goal is to understand trends over time or relationships between values, plots are a far better option!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introducing {workboots}</title>
      <link>https://www.thedatadiary.net/blog/2022-03-14-introducing-workboots/</link>
      <pubDate>Mon, 14 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-03-14-introducing-workboots/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-03-14-introducing-workboots/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Sometimes, we want a model that generates a range of possible outcomes around each prediction and may opt for a model that can generate a prediction interval, like a linear model. Other times, we just care about point predictions and may opt to use a more powerful model like XGBoost. But what if we want the best of both worlds: getting a range of predictions while still using a powerful model? That’s where &lt;a href=&#34;https://github.com/markjrieke/workboots&#34;&gt;&lt;code&gt;{workboots}&lt;/code&gt;&lt;/a&gt; comes to the rescue! &lt;code&gt;{workboots}&lt;/code&gt; uses bootstrap resampling to train many models which can be used to generate a range of outcomes — regardless of model type.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/workboots.PNG&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;installation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Installation&lt;/h3&gt;
&lt;p&gt;Version 0.1.0 of &lt;code&gt;{workboots}&lt;/code&gt; is available on &lt;a href=&#34;https://cran.r-project.org/web/packages/workboots/index.html&#34;&gt;CRAN&lt;/a&gt;. Given that the package is still in early development, however, I’d recommend installing the development version from &lt;a href=&#34;https://github.com/markjrieke/workboots&#34;&gt;github&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install from CRAN
install.packages(&amp;quot;workboots&amp;quot;)

# or install the development version
devtools::install_github(&amp;quot;markjrieke/workboots&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;usage&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Usage&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;{workboots}&lt;/code&gt; builds on top of the &lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;&lt;code&gt;{tidymodels}&lt;/code&gt;&lt;/a&gt; suite of packages and is intended to be used in conjunction with a &lt;a href=&#34;https://workflows.tidymodels.org/&#34;&gt;tidymodel workflow&lt;/a&gt;. Teaching how to use &lt;code&gt;{tidymodels}&lt;/code&gt; is beyond the scope of this post, but some helpful resources are linked at the bottom for further exploration.&lt;/p&gt;
&lt;p&gt;We’ll walk through two examples that show the benefit of the package: estimating a linear model’s prediction interval and generating a prediction interval for a boosted tree model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-a-prediction-interval&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Estimating a prediction interval&lt;/h3&gt;
&lt;p&gt;Let’s get started with a model we know can generate a prediction interval: a basic linear model. In this example, we’ll use the &lt;a href=&#34;https://modeldata.tidymodels.org/reference/ames.html&#34;&gt;Ames housing dataset&lt;/a&gt; to predict a home’s price based on its square footage.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels)

# setup our data
data(&amp;quot;ames&amp;quot;)
ames_mod &amp;lt;- ames %&amp;gt;% select(First_Flr_SF, Sale_Price)

# relationship between square footage and price
ames_mod %&amp;gt;%
  ggplot(aes(x = First_Flr_SF, y = Sale_Price)) +
  geom_point(alpha = 0.25) +
  scale_y_continuous(labels = scales::dollar_format(), trans = &amp;quot;log10&amp;quot;) +
  scale_x_continuous(labels = scales::comma_format(), trans = &amp;quot;log10&amp;quot;) +
  labs(title = &amp;quot;Relationship between Square Feet and Sale Price&amp;quot;,
       subtitle = &amp;quot;Linear relationship between the log transforms of square footage and price&amp;quot;,
       x = NULL,
       y = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-03-14-introducing-workboots/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can use a linear model to predict the log transform of &lt;code&gt;Sale_Price&lt;/code&gt; based on the log transform of &lt;code&gt;First_Flr_SF&lt;/code&gt;. In this example, we’ll train a linear model then plot our predictions against a holdout set with a prediction interval.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# log transform
ames_mod &amp;lt;- 
  ames_mod %&amp;gt;%
  mutate(across(everything(), log10))

# split into train/test data
set.seed(918)
ames_split &amp;lt;- initial_split(ames_mod)
ames_train &amp;lt;- training(ames_split)
ames_test &amp;lt;- testing(ames_split)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# train a linear model
set.seed(314)
mod &amp;lt;- lm(Sale_Price ~ First_Flr_SF, data = ames_train)

# predict on new data with a prediction interval
ames_preds &amp;lt;-
  mod %&amp;gt;%
  predict(ames_test, interval = &amp;quot;predict&amp;quot;) %&amp;gt;%
  as_tibble()

# plot!
ames_preds %&amp;gt;%
  
  # re-scale predictions to match the original dataset&amp;#39;s scale
  bind_cols(ames_test) %&amp;gt;%
  mutate(across(everything(), ~10^.x)) %&amp;gt;%
  
  # add geoms
  ggplot(aes(x = First_Flr_SF)) +
  geom_point(aes(y = Sale_Price),
             alpha = 0.25) +
  geom_line(aes(y = fit),
            size = 1) +
  geom_ribbon(aes(ymin = lwr,
                  ymax = upr),
              alpha = 0.25) +
  scale_y_continuous(labels = scales::dollar_format(), trans = &amp;quot;log10&amp;quot;) +
  scale_x_continuous(labels = scales::comma_format(), trans = &amp;quot;log10&amp;quot;) +
  labs(title = &amp;quot;Linear Model of Sale Price predicted by Square Footage&amp;quot;,
       subtitle = &amp;quot;Shaded area represents the 95% prediction interval&amp;quot;,
       x = NULL,
       y = NULL) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-03-14-introducing-workboots/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;{workboots}&lt;/code&gt;, we can approximate the linear model’s prediction interval by passing a workflow built on a linear model to the function &lt;code&gt;predict_boots()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels)
library(workboots)

# setup a workflow with a linear model
ames_wf &amp;lt;-
  workflow() %&amp;gt;%
  add_recipe(recipe(Sale_Price ~ First_Flr_SF, data = ames_train)) %&amp;gt;%
  add_model(linear_reg())

# generate bootstrap predictions on ames_test
set.seed(713)
ames_preds_boot &amp;lt;-
  ames_wf %&amp;gt;%
  predict_boots(
    n = 2000,
    training_data = ames_train,
    new_data = ames_test
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;predict_boots()&lt;/code&gt; works by creating 2000 &lt;a href=&#34;https://rsample.tidymodels.org/reference/bootstraps.html&#34;&gt;bootstrap resamples&lt;/a&gt; of the training data, fitting a linear model to each resample, then generating 2000 predictions for each home’s price in the holdout set. We can then use &lt;code&gt;summarise_predictions()&lt;/code&gt; to generate upper and lower intervals for each prediction.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ames_preds_boot %&amp;gt;%
  summarise_predictions()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 733 x 5
##    rowid .preds               .pred_lower .pred .pred_upper
##    &amp;lt;int&amp;gt; &amp;lt;list&amp;gt;                     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
##  1     1 &amp;lt;tibble [2,000 x 2]&amp;gt;        5.17  5.44        5.71
##  2     2 &amp;lt;tibble [2,000 x 2]&amp;gt;        4.98  5.27        5.55
##  3     3 &amp;lt;tibble [2,000 x 2]&amp;gt;        4.97  5.25        5.52
##  4     4 &amp;lt;tibble [2,000 x 2]&amp;gt;        5.12  5.40        5.67
##  5     5 &amp;lt;tibble [2,000 x 2]&amp;gt;        5.15  5.44        5.71
##  6     6 &amp;lt;tibble [2,000 x 2]&amp;gt;        4.93  5.21        5.49
##  7     7 &amp;lt;tibble [2,000 x 2]&amp;gt;        4.67  4.94        5.22
##  8     8 &amp;lt;tibble [2,000 x 2]&amp;gt;        4.85  5.13        5.40
##  9     9 &amp;lt;tibble [2,000 x 2]&amp;gt;        4.87  5.14        5.41
## 10    10 &amp;lt;tibble [2,000 x 2]&amp;gt;        5.14  5.41        5.69
## # ... with 723 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By overlaying the intervals on top of one another, we can see that the prediction interval generated by &lt;code&gt;predict_boots()&lt;/code&gt; is a good approximation of the theoretical interval generated by &lt;code&gt;lm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ames_preds_boot %&amp;gt;%
  summarise_predictions() %&amp;gt;%
  bind_cols(ames_preds) %&amp;gt;%
  bind_cols(ames_test) %&amp;gt;%
  mutate(across(c(.pred_lower:Sale_Price), ~10^.x)) %&amp;gt;%
  ggplot(aes(x = First_Flr_SF)) +
  geom_point(aes(y = Sale_Price),
             alpha = 0.25) +
  geom_line(aes(y = fit),
            size = 1) +
  geom_ribbon(aes(ymin = lwr,
                  ymax = upr),
              alpha = 0.25) +
  geom_point(aes(y = .pred),
             color = &amp;quot;blue&amp;quot;,
             alpha = 0.25) +
  geom_errorbar(aes(ymin = .pred_lower,
                    ymax = .pred_upper),
                color = &amp;quot;blue&amp;quot;,
                alpha = 0.25,
                width = 0.0125) +
  scale_y_continuous(labels = scales::dollar_format(), trans = &amp;quot;log10&amp;quot;) +
  scale_x_continuous(labels = scales::comma_format(), trans = &amp;quot;log10&amp;quot;) +
  labs(title = &amp;quot;Linear Model of Sale Price predicted by Square Footage&amp;quot;,
       subtitle = &amp;quot;Bootstrap prediction interval closely matches theoretical prediction interval&amp;quot;,
       x = NULL,
       y = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-03-14-introducing-workboots/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both &lt;code&gt;lm()&lt;/code&gt; and &lt;code&gt;summarise_predictions()&lt;/code&gt; use a 95% prediction interval by default but we can generate other intervals by passing different values to the parameter &lt;code&gt;conf&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ames_preds_boot %&amp;gt;%
  
  # generate 95% prediction interval
  summarise_predictions(conf = 0.95) %&amp;gt;%
  rename(.pred_lower_95 = .pred_lower,
         .pred_upper_95 = .pred_upper) %&amp;gt;%
  select(-.pred) %&amp;gt;%
  
  # generate 80% prediction interval
  summarise_predictions(conf = 0.80) %&amp;gt;%
  rename(.pred_lower_80 = .pred_lower,
         .pred_upper_80 = .pred_upper) %&amp;gt;%
  bind_cols(ames_test) %&amp;gt;%
  mutate(across(c(.pred_lower_95:Sale_Price), ~10^.x)) %&amp;gt;%
  
  # plot!
  ggplot(aes(x = First_Flr_SF)) +
  geom_point(aes(y = Sale_Price),
             alpha = 0.25) +
  geom_line(aes(y = .pred),
            size = 1,
            color = &amp;quot;blue&amp;quot;) +
  geom_ribbon(aes(ymin = .pred_lower_95,
                  ymax = .pred_upper_95),
              alpha = 0.25,
              fill = &amp;quot;blue&amp;quot;) +
  geom_ribbon(aes(ymin = .pred_lower_80,
                  ymax = .pred_upper_80),
              alpha = 0.25,
              fill = &amp;quot;blue&amp;quot;) +
  scale_y_continuous(labels = scales::dollar_format(), trans = &amp;quot;log10&amp;quot;) +
  scale_x_continuous(labels = scales::comma_format(), trans = &amp;quot;log10&amp;quot;) +
  labs(title = &amp;quot;Linear Model of Sale Price predicted by Square Footage&amp;quot;,
       subtitle = &amp;quot;Predictions alongside 95% and 80% bootstrap prediction interval&amp;quot;,
       x = NULL,
       y = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-03-14-introducing-workboots/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As this example shows, &lt;code&gt;{workboots}&lt;/code&gt; can approximate linear prediction intervals pretty well! But this isn’t very useful, since we can just generate a linear prediction interval from a linear model directly. The real benefit of &lt;code&gt;{workboots}&lt;/code&gt; comes from generating prediction intervals from &lt;em&gt;any&lt;/em&gt; model!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bootstrap-prediction-intervals-with-non-linear-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bootstrap prediction intervals with non-linear models&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://xgboost.readthedocs.io/en/stable/&#34;&gt;XGBoost&lt;/a&gt; is one of my favorite models. Up until now, however, in situations that require a prediction interval, I’ve had to opt for a simpler model. With &lt;code&gt;{workboots}&lt;/code&gt;, that’s no longer an issue! In this example, we’ll use XGBoost and &lt;code&gt;{workboots}&lt;/code&gt; to generate predictions of a penguins weight from the &lt;a href=&#34;https://modeldata.tidymodels.org/reference/penguins.html&#34;&gt;Palmer Penguins dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To get started, let’s build a workflow and train an individual model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load and prep data
data(&amp;quot;penguins&amp;quot;)

penguins &amp;lt;-
  penguins %&amp;gt;%
  drop_na()

# split data into training and testing sets
set.seed(123)
penguins_split &amp;lt;- initial_split(penguins)
penguins_test &amp;lt;- testing(penguins_split)
penguins_train &amp;lt;- training(penguins_split)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create a workflow
penguins_wf &amp;lt;-
  workflow() %&amp;gt;%
  
  # add preprocessing steps
  add_recipe(
    recipe(body_mass_g ~ ., data = penguins_train) %&amp;gt;%
      step_dummy(all_nominal_predictors()) 
  ) %&amp;gt;%
  
  # add xgboost model spec
  add_model(
    boost_tree(&amp;quot;regression&amp;quot;)
  )

# fit to training data &amp;amp; predict on test data
set.seed(234)
penguins_preds &amp;lt;-
  penguins_wf %&amp;gt;%
  fit(penguins_train) %&amp;gt;%
  predict(penguins_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As mentioned above, XGBoost models can only generate point predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;penguins_preds %&amp;gt;%
  bind_cols(penguins_test) %&amp;gt;%
  ggplot(aes(x = body_mass_g,
             y = .pred)) +
  geom_point() +
  geom_abline(linetype = &amp;quot;dashed&amp;quot;,
              color = &amp;quot;gray&amp;quot;) +
  labs(title = &amp;quot;XGBoost Model of Penguin Weight&amp;quot;,
       subtitle = &amp;quot;Individual model can only output individual predictions&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-03-14-introducing-workboots/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;{workboots}&lt;/code&gt;, however, we can generate a prediction interval from our XGBoost model for each penguin’s weight!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create 2000 models from bootstrap resamples and make predictions on the test set
set.seed(345)
penguins_preds_boot &amp;lt;-
  penguins_wf %&amp;gt;%
  predict_boots(
    n = 2000,
    training_data = penguins_train,
    new_data = penguins_test
  )

penguins_preds_boot %&amp;gt;%
  summarise_predictions()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 84 x 5
##    rowid .preds               .pred_lower .pred .pred_upper
##    &amp;lt;int&amp;gt; &amp;lt;list&amp;gt;                     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
##  1     1 &amp;lt;tibble [2,000 x 2]&amp;gt;       2788. 3470.       4136.
##  2     2 &amp;lt;tibble [2,000 x 2]&amp;gt;       2838. 3534.       4231.
##  3     3 &amp;lt;tibble [2,000 x 2]&amp;gt;       2942. 3598.       4301.
##  4     4 &amp;lt;tibble [2,000 x 2]&amp;gt;       3354. 4158.       4889.
##  5     5 &amp;lt;tibble [2,000 x 2]&amp;gt;       3186. 3870.       4500.
##  6     6 &amp;lt;tibble [2,000 x 2]&amp;gt;       2884. 3519.       4208.
##  7     7 &amp;lt;tibble [2,000 x 2]&amp;gt;       2790. 3434.       4094.
##  8     8 &amp;lt;tibble [2,000 x 2]&amp;gt;       3394. 4071.       4772.
##  9     9 &amp;lt;tibble [2,000 x 2]&amp;gt;       2812. 3447.       4096.
## 10    10 &amp;lt;tibble [2,000 x 2]&amp;gt;       2744. 3404.       4063.
## # ... with 74 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How does our bootstrap model perform?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;penguins_preds_boot %&amp;gt;%
  summarise_predictions() %&amp;gt;%
  bind_cols(penguins_test) %&amp;gt;%
  ggplot(aes(x = body_mass_g,
             y = .pred,
             ymin = .pred_lower,
             ymax = .pred_upper)) +
  geom_abline(linetype = &amp;quot;dashed&amp;quot;,
              color = &amp;quot;gray&amp;quot;) +
  geom_errorbar(alpha = 0.5,
                color = &amp;quot;blue&amp;quot;) +
  geom_point(alpha = 0.5,
             color = &amp;quot;blue&amp;quot;) +
  labs(title = &amp;quot;XGBoost Model of Penguin Weight&amp;quot;,
       subtitle = &amp;quot;Bootstrap models can generate prediction intervals&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-03-14-introducing-workboots/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This particular model may be in need of some tuning for better performance, but the important takeaway is that we were able to generate a prediction distribution for the model! This method works with other regression models as well — just create a workflow then let &lt;code&gt;{workboots}&lt;/code&gt; take care of the rest!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidymodel-resources&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tidymodel Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tidymodels.org/start/&#34;&gt;Getting Started with Tidymodels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tmwr.org/&#34;&gt;Tidy Modeling with R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://juliasilge.com/blog/&#34;&gt;Julia Silge’s Blog&lt;/a&gt; provides use cases of tidymodels with weekly &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;#tidytuesday&lt;/a&gt; datasets.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Data Science Hierarchy of Needs</title>
      <link>https://www.thedatadiary.net/blog/2022-02-22-the-data-science-hierarchy-of-needs/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-02-22-the-data-science-hierarchy-of-needs/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-02-22-the-data-science-hierarchy-of-needs/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I’ve never built a house (shocking, I know), but from far too much time spent watching HGTV, I understand the basic gist of it. You lay a foundation, setup framing and walls, route mechanical and electrical, then work on final touches like painting and decorating (to be sure, I’m hand-waiving a lot of detail away here). There’s a bit of wiggle room in the order you go about things — you can paint the living room walls before the ones in the bathroom or vice versa — but some steps definitely need to happen before others — you can’t paint either rooms until the walls themselves are actually up!&lt;/p&gt;
&lt;p&gt;The same logic applies for data science — there are certain activities that are exceptionally hard to do without the proper infrastructure in place. Sometimes, we’re asked to chase after ~shiny objects~ without the support system to do so, when doing so may actually make our job more difficult in the future!&lt;/p&gt;
&lt;p&gt;I recently stumbled across &lt;a href=&#34;https://towardsdatascience.com/the-data-science-pyramid-8a018013c490&#34;&gt;an article&lt;/a&gt; that summarized this really succinctly with the following graphic: &lt;strong&gt;The Data Science Hierarchy of Needs&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/ds_hierarchy.png&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;collect&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Collect&lt;/h4&gt;
&lt;p&gt;At a baseline, to do any sort of data work you need to actually have data on hand to work with! Whether there’s a formal process for collecting data or you need to gather data from disparate public sources, getting raw data out of the wild and into your system is the first step to being able to do any sort of analysis. In my case, as an analyst with a hospital’s patient satisfaction group, we need to actually send patients surveys.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;movestore&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Move/Store&lt;/h4&gt;
&lt;p&gt;Once you know where your data is coming from, setting up a reliable data flow from the source to your environment is needed. This is where a lot of headache can come from. Gathering data can be difficult but if the data is going to be used once for a one-off analysis, you don’t need to worry too much about repeatability, edge cases, or computing speed. Once you need to gather new data, thinking about infrastructure around new data gathering becomes much more important. A good chunk of the last eight months of my job has been working with our new survey vendor on this piece of the puzzle: standardizing data layouts, catching bugs in the pipeline, and setting up standards for access.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploretransform&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Explore/Transform&lt;/h4&gt;
&lt;p&gt;With a reliable flow of new/updated data streaming in, you now need to make sure the data is appropriate for general use. Automated anomaly/fraud/bot detection, light wrangling, and removing errant responses can all be considered a part of this single stage. This is necessary to ensure that any analyses you do or models you build are based on what you expect from the underlying data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregatelabel&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Aggregate/Label&lt;/h4&gt;
&lt;p&gt;I can’t recall the source, but the following quote about data science has stuck with me: “99% of data science work is counting — sometimes dividing.” A significant portion of my day-to-day work involves the tried-and-trusted &lt;code&gt;group_by() %&amp;gt;% summarise()&lt;/code&gt; pipeline. Making counts, percentages, running totals, etc. accessible to stakeholders via a dashboard can likely answer ~80% of the questions an analyst would have to field otherwise. It’s &lt;em&gt;so, so&lt;/em&gt; important, however, to have the collection, storage, and preparation stages setup prior to ensure that stakeholders can &lt;em&gt;trust&lt;/em&gt; that the data they’re seeing is accurate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;learnoptimize&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Learn/Optimize&lt;/h4&gt;
&lt;p&gt;If 80% of questions asked can be solved with grouped summaries and 20% require a model, it’s likely that 80% of that remaining 20% can be solved by a simple linear model. For example, “What effect does patient age have on their overall satisfaction?” can be answered with &lt;code&gt;lm(satisfaction_score ~ age)&lt;/code&gt;. As relationships become more complex, you can add more terms to the model, or switch model architectures, but — in my own experience — the majority of modeling in practice can be represented by linear models (and, by extension, regularized models via &lt;code&gt;{glmnet}&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;complex-models&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Complex Models&lt;/h4&gt;
&lt;p&gt;Finally, a small subset of problems may require a more complex or powerful model type. But before you spin your wheels building a neural net or some other wacky architecture, you should first check if something simpler gets you what you need.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-closing-thoughts&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Some Closing Thoughts&lt;/h4&gt;
&lt;p&gt;This post is partially meant to be able to share some useful info and partially a reminder to me to look for the simple solution! I have a tendency to start off with something complex then realize that I could save a lot of work if I just switch to something simpler. The three baseline layers upstream of my domain are &lt;em&gt;super important&lt;/em&gt; and definitely need oversight from someone with an eye for data engineering.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polling-bites&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Polling Bites&lt;/h3&gt;
&lt;p&gt;The generic congressional ballot is starting to show some movement away from even split as Republicans have slowly climbed to &lt;strong&gt;51.2%&lt;/strong&gt; in the polls and Democrats have fallen to &lt;strong&gt;48.8%&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/generic_ballot_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Biden’s net approval hasn’t shifted significantly since the last post — currently sitting at &lt;strong&gt;10.9%&lt;/strong&gt; underwater with &lt;strong&gt;41.8%&lt;/strong&gt; approval and &lt;strong&gt;52.7%&lt;/strong&gt; disapproval.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/approval_disapproval_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/net_approval_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(p.s., I’ve updated the color palettes here with the &lt;a href=&#34;https://github.com/BlakeRMills/MetBrewer&#34;&gt;&lt;code&gt;{MetBrewer}&lt;/code&gt;&lt;/a&gt; package, which provides colorblind friendly palettes based on artwork in hte Metropolitan Museum of Art in New York).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Pull Yourself Up by Your Bootstraps</title>
      <link>https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/</link>
      <pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Note (3/14/22): This article was written prior to the release of the &lt;a href=&#34;https://github.com/markjrieke/workboots&#34;&gt;{workboots}&lt;/a&gt; package. Since the release of that package, I’ve discovered some errors with the methodology described here and would recommend instead referencing the &lt;a href=&#34;https://thedatadiary.net/blog/2022-03-14-introducing-workboots&#34;&gt;post associated with the release&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Statistical modeling sometimes presents conflicting goals. Oftentimes, building a model involves a mix of objectives that don’t necessarily mesh well together: super-accurate point predictions, explainability, fast performance, or an expression of confidence in the prediction. In my work as an analyst, I generally am focused on how explainable the model is while being able to express a confidence interval around each prediction. For that, simple linear models do the trick. If, however, I want to regularize via &lt;code&gt;{glmnet}&lt;/code&gt; (which — with good reason — &lt;a href=&#34;https://stats.stackexchange.com/questions/224796/why-are-confidence-intervals-and-p-values-not-reported-as-default-for-penalized&#34;&gt;doesn’t provide confidence intervals&lt;/a&gt;) or use a non-linear model like &lt;code&gt;{xgboost}&lt;/code&gt;, I have to drop the confidence interval around predictions. Or so I had previously thought! As it turns out, building a series of models from bootstrap resamples provides an alternative method of generating a confidence interval around a prediction.&lt;/p&gt;
&lt;div id=&#34;setting-a-baseline-with-penguins&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting a baseline with penguins&lt;/h3&gt;
&lt;p&gt;First, let’s build out a baseline linear model with the Palmer Penguins dataset. This dataset contains information on 344 penguins across three species types and three islands. For this example, we’ll use the penguin information to predict &lt;code&gt;body_mass_g&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the data in from the tidytuesdayR package
penguins_src &amp;lt;- tidytuesdayR::tt_load(2020, week = 31)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Downloading file 1 of 2: `penguins.csv`
##  Downloading file 2 of 2: `penguins_raw.csv`&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract out the penguins dataset
penguins &amp;lt;- penguins_src$penguins
rm(penguins_src)

penguins&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 344 x 8
##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
##  1 Adelie  Torgersen           39.1          18.7               181        3750
##  2 Adelie  Torgersen           39.5          17.4               186        3800
##  3 Adelie  Torgersen           40.3          18                 195        3250
##  4 Adelie  Torgersen           NA            NA                  NA          NA
##  5 Adelie  Torgersen           36.7          19.3               193        3450
##  6 Adelie  Torgersen           39.3          20.6               190        3650
##  7 Adelie  Torgersen           38.9          17.8               181        3625
##  8 Adelie  Torgersen           39.2          19.6               195        4675
##  9 Adelie  Torgersen           34.1          18.1               193        3475
## 10 Adelie  Torgersen           42            20.2               190        4250
## # ... with 334 more rows, and 2 more variables: sex &amp;lt;chr&amp;gt;, year &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll need to do some lite preprocessing before we start modeling — it looks like there are some &lt;code&gt;NAs&lt;/code&gt; in &lt;code&gt;body_mass_g&lt;/code&gt; and in &lt;code&gt;sex&lt;/code&gt;. If I were creating a more serious model, I might keep the rows with &lt;code&gt;NAs&lt;/code&gt; for &lt;code&gt;sex&lt;/code&gt;, but since there are so few and this is an explainer, I’ll just filter them out.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# remove NA from body_mass_g and sex
penguins &amp;lt;- 
  penguins %&amp;gt;%
  filter(!is.na(body_mass_g),
         !is.na(sex))

penguins&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 333 x 8
##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
##  1 Adelie  Torgersen           39.1          18.7               181        3750
##  2 Adelie  Torgersen           39.5          17.4               186        3800
##  3 Adelie  Torgersen           40.3          18                 195        3250
##  4 Adelie  Torgersen           36.7          19.3               193        3450
##  5 Adelie  Torgersen           39.3          20.6               190        3650
##  6 Adelie  Torgersen           38.9          17.8               181        3625
##  7 Adelie  Torgersen           39.2          19.6               195        4675
##  8 Adelie  Torgersen           41.1          17.6               182        3200
##  9 Adelie  Torgersen           38.6          21.2               191        3800
## 10 Adelie  Torgersen           34.6          21.1               198        4400
## # ... with 323 more rows, and 2 more variables: sex &amp;lt;chr&amp;gt;, year &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s always good practice to explore the dataset prior to fitting a model, so let’s jump into some good ol’ fashioned EDA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how are species/island related to body mass?
penguins %&amp;gt;%
  ggplot(aes(x = species,
             y = body_mass_g,
             color = species)) +
  geom_boxplot() +
  geom_point(alpha = 0.25,
             position = position_jitter()) +
  facet_wrap(~island)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Interesting! It looks like the Gentoo and Chinstrap species are only found on the Biscoe and Dream islands, respectively, whereas the Adelie species can be found on all three islands. At first glance, there’s not a meaningful difference that Island has on the weight of the Adelie penguins, so I think we’re safe to toss out the &lt;code&gt;island&lt;/code&gt; feature and just keep &lt;code&gt;species&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how does sex relate to body mass?
penguins %&amp;gt;%
  ggplot(aes(x = sex,
             y = body_mass_g,
             color = sex)) +
  geom_boxplot() +
  geom_point(alpha = 0.25,
             position = position_jitter())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unsurprisingly, male penguins are typically heavier than female penguins.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# are penguins getting heavier or lighter as years progress?
penguins %&amp;gt;%
  mutate(year = as.character(year)) %&amp;gt;%
  ggplot(aes(x = year,
             y = body_mass_g)) +
  geom_boxplot() +
  geom_point(alpha = 0.25,
             position = position_jitter())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It doesn’t look like there is significant signal being drawn from &lt;code&gt;year&lt;/code&gt;, so we’ll toss that out as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how do other body measurements compare with the total body mass?
penguins %&amp;gt;%
  select(bill_length_mm:body_mass_g) %&amp;gt;%
  pivot_longer(ends_with(&amp;quot;mm&amp;quot;),
               names_to = &amp;quot;measurement&amp;quot;,
               values_to = &amp;quot;value&amp;quot;) %&amp;gt;%
  ggplot(aes(x = value,
             y = body_mass_g,
             color = measurement)) +
  geom_point(alpha = 0.5) + 
  facet_wrap(~measurement, scales = &amp;quot;free_x&amp;quot;) +
  geom_smooth(method = &amp;quot;lm&amp;quot;,
              se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For bill and flipper length, there’s a pretty clear relationship, but it looks like bill depth has a &lt;em&gt;classic&lt;/em&gt; case of &lt;a href=&#34;https://en.wikipedia.org/wiki/Simpson%27s_paradox&#34;&gt;Simpson’s paradox&lt;/a&gt;. Let’s explore that further to find a meaningful interaction to apply.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# which feature interacts with bill depth to produce simpson&amp;#39;s pardox?
penguins %&amp;gt;%
  ggplot(aes(x = bill_depth_mm,
             y = body_mass_g,
             color = species)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = &amp;quot;lm&amp;quot;,
              se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So, very clearly, the Gentoo species has a very different relationship between bill depth and body mass than the Adelie/Chinstrap species. We’ll add this as an interactive feature to the model.&lt;/p&gt;
&lt;p&gt;With all that completed, let’s (finally) setup and build the baseline linear model with confidence intervals around the prediction!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# remove features
penguins &amp;lt;- 
  penguins %&amp;gt;%
  select(-island, -year)

# split into testing and training datasets
set.seed(123)
penguins_split &amp;lt;- initial_split(penguins)
penguins_test &amp;lt;- testing(penguins_split)
penguins_train &amp;lt;- training(penguins_split)

# setup a pre-processing recipe
penguins_rec &amp;lt;- 
  recipe(body_mass_g ~ ., data = penguins_train) %&amp;gt;%
  step_dummy(all_nominal()) %&amp;gt;% 
  step_interact(~starts_with(&amp;quot;species&amp;quot;):bill_depth_mm)

# fit a workflow
penguins_lm &amp;lt;- 
  workflow() %&amp;gt;%
  add_recipe(penguins_rec) %&amp;gt;%
  add_model(linear_reg() %&amp;gt;% set_engine(&amp;quot;lm&amp;quot;)) %&amp;gt;%
  fit(penguins_train)

# predict on training data with confidence intervals
bind_cols(penguins_lm %&amp;gt;% predict(penguins_train),
          penguins_lm %&amp;gt;% predict(penguins_train, type = &amp;quot;conf_int&amp;quot;, level = 0.95),
          penguins_train) %&amp;gt;%
  ggplot(aes(x = body_mass_g,
             y = .pred)) +
  geom_point(alpha = 0.5) +
  geom_segment(aes(x = body_mass_g,
                   xend = body_mass_g,
                   y = .pred_lower,
                   yend = .pred_upper),
               alpha = 0.25) +
  labs(title = &amp;quot;Predicting the Palmer Penguins - Training&amp;quot;,
       subtitle = &amp;quot;Linear model predicting a penguin&amp;#39;s weight in grams&amp;quot;,
       x = &amp;quot;Actual weight (g)&amp;quot;,
       y = &amp;quot;Predicted weight (g)&amp;quot;,
       caption = &amp;quot;Errorbars represent the a 95% confidence interval&amp;quot;) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This model does generally okay, but the confidence interval around each prediction is pretty &lt;a href=&#34;https://mc-stan.org/rstanarm/articles/rstanarm.html&#34;&gt;clearly too confident&lt;/a&gt;! Let’s solve this with bootstrapping.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;whats-a-bootstrap&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What’s a bootstrap?&lt;/h3&gt;
&lt;p&gt;Before progressing any further, it’s probably important to define what exactly a bootstrap is/what bootstrapping is. Bootstrapping is a resampling method that lets us take one dataset and turn it into many datasets. Bootstrapping accomplishes this by repeatedly pulling a random row from the source dataset and, importantly, bootstrapping allows for rows to be repeated! Let’s look at an example for a bit more clarity.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;rowid&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;104&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;102&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;79&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let’s say we want to make bootstrap resamples of this dataset. We’ll draw five random rows from the dataset and, sometimes, we’ll have the same row show up in our new bootstrapped dataset multiple times:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;rowid&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;104&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;102&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Another bootstrap dataset might look like this:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;rowid&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;102&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;79&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;79&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Bootstrap datasets allow us to create many datasets from the original dataset and evaluate models across these bootstraps. Models that are well informed will give similar outputs across each dataset, despite of the randomness within each dataset, whereas less confident models will have a wider variation across the bootstrapped datasets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generating-some-confident-penguins&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Generating some confident penguins&lt;/h3&gt;
&lt;p&gt;Let’s say we want to use &lt;code&gt;{xgboost}&lt;/code&gt; to predict penguin weight and we’ll use bootstrapping to generate a confidence interval. Firstly, we’ll create the bootstrap datasets from our training set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;penguins_boot &amp;lt;- penguins_train %&amp;gt;% bootstraps()

penguins_boot&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # Bootstrap sampling 
## # A tibble: 25 x 2
##    splits           id         
##    &amp;lt;list&amp;gt;           &amp;lt;chr&amp;gt;      
##  1 &amp;lt;split [249/92]&amp;gt; Bootstrap01
##  2 &amp;lt;split [249/90]&amp;gt; Bootstrap02
##  3 &amp;lt;split [249/91]&amp;gt; Bootstrap03
##  4 &amp;lt;split [249/87]&amp;gt; Bootstrap04
##  5 &amp;lt;split [249/98]&amp;gt; Bootstrap05
##  6 &amp;lt;split [249/84]&amp;gt; Bootstrap06
##  7 &amp;lt;split [249/91]&amp;gt; Bootstrap07
##  8 &amp;lt;split [249/95]&amp;gt; Bootstrap08
##  9 &amp;lt;split [249/94]&amp;gt; Bootstrap09
## 10 &amp;lt;split [249/86]&amp;gt; Bootstrap10
## # ... with 15 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, the &lt;code&gt;bootstraps()&lt;/code&gt; function will create 25 bootstrap datasets, but we could theoretically create as many as we want. Now that we have our bootstraps, let’s create a function that will fit a model to each of the bootstraps and save to disk. We’ll use the default parameters for our &lt;code&gt;{xgboost}&lt;/code&gt; model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define a basic xgboost model
penguins_xgb &amp;lt;-
  boost_tree() %&amp;gt;%
  set_mode(&amp;quot;regression&amp;quot;) %&amp;gt;%
  set_engine(&amp;quot;xgboost&amp;quot;)

# function that will fit a model and save to a folder
fit_bootstrap &amp;lt;- function(index) {
  
  # pull out individual bootstrap to fit
  xgb_boot &amp;lt;- penguins_boot$splits[[index]] %&amp;gt;% training()
  
  # fit to a workflow
  workflow() %&amp;gt;%
    add_recipe(penguins_rec) %&amp;gt;%
    add_model(penguins_xgb) %&amp;gt;%
    fit(xgb_boot) %&amp;gt;%
    write_rds(paste0(&amp;quot;models/model_&amp;quot;, index, &amp;quot;.rds&amp;quot;))
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function will create a new model for each bootstrap, so we’ll end up with 25 separate models. Let’s fit!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fit to 25 bootstrapped datasets
for (i in 1:25) {
  
  fit_bootstrap(i)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s define a function that will predict based on these 25 bootstrapped models, then predict on our training data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_bootstrap &amp;lt;- function(new_data, index){
  
  read_rds(paste0(&amp;quot;models/model_&amp;quot;, index, &amp;quot;.rds&amp;quot;)) %&amp;gt;%
    predict(new_data) %&amp;gt;%
    rename(!!sym(paste0(&amp;quot;pred_&amp;quot;, index)) := .pred)
  
}

# predict!
training_preds &amp;lt;- 
  seq(1, 25) %&amp;gt;%
  map_dfc(~predict_bootstrap(penguins_train, .x))

training_preds&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 249 x 25
##    pred_1 pred_2 pred_3 pred_4 pred_5 pred_6 pred_7 pred_8 pred_9 pred_10
##     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1  5552.  5638.  5555.  5703.  5726.  5783.  5404.  5566.  5493.   5547.
##  2  3470.  3340.  3334.  3350.  3311.  3303.  3315.  3421.  3692.   3436.
##  3  5309.  5274.  5241.  5286.  5206.  5084.  5506.  5531.  5274.   5309.
##  4  4160.  4013.  3988.  4111.  4075.  4073.  4284.  4050.  4033.   4033.
##  5  4003.  3931.  4096.  3968.  4008.  3918.  3941.  4093.  3941.   3880.
##  6  3967.  4039.  4095.  4047.  4021.  4055.  3980.  4115.  4067.   4084.
##  7  4647.  4551.  4750.  4555.  4690.  4396.  4235.  4686.  4764.   4659.
##  8  5240.  5288.  5291.  5276.  5308.  5508.  5570.  5375.  5340.   5268.
##  9  4138.  4111.  4106.  4236.  4135.  4219.  4218.  4211.  4160.   4071.
## 10  4728.  4723.  4715.  4823.  4765.  4727.  4836.  4777.  4765.   4633.
## # ... with 239 more rows, and 15 more variables: pred_11 &amp;lt;dbl&amp;gt;, pred_12 &amp;lt;dbl&amp;gt;,
## #   pred_13 &amp;lt;dbl&amp;gt;, pred_14 &amp;lt;dbl&amp;gt;, pred_15 &amp;lt;dbl&amp;gt;, pred_16 &amp;lt;dbl&amp;gt;, pred_17 &amp;lt;dbl&amp;gt;,
## #   pred_18 &amp;lt;dbl&amp;gt;, pred_19 &amp;lt;dbl&amp;gt;, pred_20 &amp;lt;dbl&amp;gt;, pred_21 &amp;lt;dbl&amp;gt;, pred_22 &amp;lt;dbl&amp;gt;,
## #   pred_23 &amp;lt;dbl&amp;gt;, pred_24 &amp;lt;dbl&amp;gt;, pred_25 &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a column of predictions for each model — we can summarise our point prediction for each row with the average across all models and set the confidence interval based on the standard deviation of the predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_preds %&amp;gt;%
  bind_cols(penguins_train) %&amp;gt;%
  rowid_to_column() %&amp;gt;%
  pivot_longer(starts_with(&amp;quot;pred_&amp;quot;),
               names_to = &amp;quot;model&amp;quot;,
               values_to = &amp;quot;.pred&amp;quot;) %&amp;gt;%
  group_by(rowid) %&amp;gt;%
  summarise(body_mass_g = max(body_mass_g),
            .pred_mean = mean(.pred),
            std_dev = sd(.pred)) %&amp;gt;%
  riekelib::normal_interval(.pred_mean, std_dev) %&amp;gt;%
  ggplot(aes(x = body_mass_g,
             y = .pred_mean)) +
  geom_point(alpha = 0.5) +
  geom_segment(aes(x = body_mass_g, 
                   xend = body_mass_g,
                   y = ci_lower,
                   yend = ci_upper),
               alpha = 0.25) +
  labs(title = &amp;quot;Predicting the Palmer Penguins - Training&amp;quot;,
       subtitle = &amp;quot;XGBoost model predicting a penguin&amp;#39;s weight in grams&amp;quot;,
       x = &amp;quot;Actual weight (g)&amp;quot;,
       y = &amp;quot;Predicted weight (g)&amp;quot;,
       caption = &amp;quot;Errorbars represent the a 95% confidence interval&amp;quot;) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And just like that, we’ve trained a series of models with &lt;code&gt;{xgboost}&lt;/code&gt; that let us apply a confidence interval around a point prediction! Now that we’ve done so on the training set, let’s look at performance on the test set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(1, 25) %&amp;gt;%
  map_dfc(~predict_bootstrap(penguins_test, .x)) %&amp;gt;%
  bind_cols(penguins_test) %&amp;gt;%
  rowid_to_column() %&amp;gt;%
  pivot_longer(starts_with(&amp;quot;pred_&amp;quot;),
               names_to = &amp;quot;model&amp;quot;,
               values_to = &amp;quot;.pred&amp;quot;) %&amp;gt;%
  group_by(rowid) %&amp;gt;%
  summarise(body_mass_g = max(body_mass_g),
            .pred_mean = mean(.pred),
            std_dev = sd(.pred)) %&amp;gt;%
  riekelib::normal_interval(.pred_mean, std_dev) %&amp;gt;%
  ggplot(aes(x = body_mass_g,
             y = .pred_mean)) +
  geom_point(alpha = 0.5) +
  geom_segment(aes(x = body_mass_g, 
                   xend = body_mass_g,
                   y = ci_lower,
                   yend = ci_upper),
               alpha = 0.25) +
  labs(title = &amp;quot;Predicting the Palmer Penguins - Testing&amp;quot;,
       subtitle = &amp;quot;XGBoost model predicting a penguin&amp;#39;s weight in grams&amp;quot;,
       x = &amp;quot;Actual weight (g)&amp;quot;,
       y = &amp;quot;Predicted weight (g)&amp;quot;,
       caption = &amp;quot;Errorbars represent the a 95% confidence interval&amp;quot;) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The performance on the test data is slightly less accurate than on the training data, but that is to be expected. Importantly, we’ve used bootstrap resampling to generate a confidence interval from a model that otherwise normally returns a simple point prediction.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-noteworthy-caveats&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some noteworthy caveats&lt;/h3&gt;
&lt;p&gt;The prediction interval above is all well and good, but it comes with some &lt;em&gt;hefty&lt;/em&gt; caveats. Firstly, the confidence interval in the Testing plot is generated from the mean and standard deviation from each prediction. This assumes that the predictions are distributed normally, which may not necessarily be the case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_preds %&amp;gt;%
  slice_head(n = 1) %&amp;gt;%
  pivot_longer(starts_with(&amp;quot;pred&amp;quot;)) %&amp;gt;%
  ggplot(aes(x = value)) +
  geom_density()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This density plot for one of the predictions shows that there’s definitely some non-normal behavior! There’s a few ways of addressing this.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create many, many, more bootstraps and models so that the prediction distribution approaches normality (with only 25 points, we really shouldn’t even expect normality from this example).&lt;/li&gt;
&lt;li&gt;Report out the actual values of the percentiles in the distribution (e.g., the 2.5% percentile is below X, 97.5% is above Y, and the mean is at Z).&lt;/li&gt;
&lt;li&gt;Report out the actual distribution as the result.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Ideally, you should do all three.&lt;/p&gt;
&lt;p&gt;The second major caveat is that this is not one model, but a whole host of models and these take up a large amount of disk space. In this example, our 25 models take up 25 times more space than our original model and it takes some time to read in, fit, and wrangle the results. We can trade disk space for computation time by writing a function that fits and predicts without saving a model, but again, that’s a tradeoff between speed and space. For linear models, it may be a better route to have STAN simulate thousands of results via &lt;code&gt;{rstanarm}&lt;/code&gt; or &lt;code&gt;{brms}&lt;/code&gt;, but for non-linear models, boostrapping is the best way to go for now!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polling-bites&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Polling Bites&lt;/h3&gt;
&lt;p&gt;Currently, the Generic Ballot is holding steady with a slight sliver more Americans wanting Republicans in Congress than Democrats (&lt;strong&gt;50.7%&lt;/strong&gt; to &lt;strong&gt;49.3%&lt;/strong&gt;, respectively). Joe Biden’s net approval continues to slide, currently sitting at &lt;strong&gt;-11.4%&lt;/strong&gt; (&lt;strong&gt;41.8%&lt;/strong&gt; approve, &lt;strong&gt;53.1%&lt;/strong&gt; disapprove).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/generic_ballot_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/approval_disapproval_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/net_approval_current.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Technical Books!</title>
      <link>https://www.thedatadiary.net/blog/2021-11-28-technical-books/</link>
      <pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-11-28-technical-books/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-11-28-technical-books/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Happy (belated) Thanksgiving! This year, my family drove down to Houston for the holiday &amp;amp; I hosted Thanksgiving for the first time. We played lots of games and ate well - my fridge is &lt;em&gt;still&lt;/em&gt; stocked full of leftovers. Knowing we’d be busy with hosting, I planned ahead and scheduled a lighter post - this week, I thought I’d highlight some technical books that I’ve either referenced for modeling work, have been recommended to me, or I’ve heard about and would like to read:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf&#34;&gt;The Elements of Statistical Learning&lt;/a&gt;&lt;/strong&gt; is referenced as the Bible of Machine Learning by &lt;a href=&#34;https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw&#34;&gt;Josh Starmer&lt;/a&gt; and provides a robust and deeply technical foundation for a wide array of machine learning models. It’s considered a must-have among both machine learning theorists, who look for new model structures, and practitioners (like myself!).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://web.stanford.edu/~hastie/ISLR2/ISLRv2_website.pdf&#34;&gt;An Introduction to Statistical Learning with Applications in R&lt;/a&gt;&lt;/strong&gt; is a companion to &lt;em&gt;The Elements of Statistical Learning&lt;/em&gt;. &lt;em&gt;An Introduction to Statistical Learning&lt;/em&gt; arose as a broader and less technical treatment of the key topics discussed in &lt;em&gt;The Elements of Statistical Learning&lt;/em&gt;. Each section also includes learning-lab lessons walking through the implementation of the statistical learning method from that chapter (&lt;a href=&#34;https://www.emilhvitfeldt.com/&#34;&gt;Emil Hvitfeldt&lt;/a&gt; is also working on a &lt;a href=&#34;https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/&#34;&gt;companion site&lt;/a&gt; for completing the labs with tidymodels).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.tmwr.org/index.html&#34;&gt;Tidy Modeling with R&lt;/a&gt;&lt;/strong&gt; is a guide to using the tidymodel framework and has been an excellent reference in both personal and professional projects.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.tidytextmining.com/&#34;&gt;Text Mining with R: a Tidy Approach&lt;/a&gt;&lt;/strong&gt; serves as an introduction to text mining and other methods for dealing with unstructured, non-rectangular data. In my current role as a Consumer Experience Analyst, I have to interact with unstructured data (in the form of patient comments) daily - this book, along with the &lt;a href=&#34;https://juliasilge.github.io/tidytext/&#34;&gt;tidytext&lt;/a&gt; package, have been incredibly useful for analyzing and visualizing text data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://smltar.com/preface.html&#34;&gt;Supervised Machine Learning for Text Analysis in R&lt;/a&gt;&lt;/strong&gt; picks up where &lt;em&gt;Text Mining with R&lt;/em&gt; left off by exploring (as the title suggests) supervised machine learning methods with text data. While I haven’t done extensive text modeling, this is one area that I’d like to explore further in 2022.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;http://www.feat.engineering/&#34;&gt;Feature Engineering and Selection: A Practical Approach for Predictive Models&lt;/a&gt;&lt;/strong&gt; is a guidebook offering methods for feature engineering (transforming and creating new predictor variables to improve predictive model performance). While I’ve utilized some basic feature engineering in some of my work, I’m interested in adding more robust tools to my feature-engineering toolkit!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://drob.gumroad.com/l/empirical-bayes&#34;&gt;Introduction to Empirical Bayes&lt;/a&gt;&lt;/strong&gt; is &lt;a href=&#34;http://varianceexplained.org/about/&#34;&gt;David Robinson’s&lt;/a&gt; book coalescing a series of blog posts on Bayesian estimation, credible intervals, A/B testing, mixed models, and a host of other methods, all through the example of baseball batting averages.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://jnolis.com/book/&#34;&gt;Build a Career in Data Science&lt;/a&gt;&lt;/strong&gt; is, as the name suggests, a book about building a career in data science. I generally feel that most career-help books are too broad to be useful or offer non-novel information for those in the industry the book is written for. Given, however, that I don’t have an academic or professional background in the field and that I’d like to eventually move from analytics to data science, I’d like to add this to the collection to pick up on some best practices.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
