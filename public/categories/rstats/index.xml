<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>rstats on the data diary</title>
    <link>https://www.thedatadiary.net/categories/rstats/</link>
    <description>Recent content in rstats on the data diary</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Mark Rieke {year}</copyright>
    <lastBuildDate>Tue, 22 Feb 2022 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://www.thedatadiary.net/categories/rstats/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>The Data Science Hierarchy of Needs</title>
      <link>https://www.thedatadiary.net/blog/2022-02-22-the-data-science-hierarchy-of-needs/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-02-22-the-data-science-hierarchy-of-needs/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-02-22-the-data-science-hierarchy-of-needs/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I’ve never built a house (shocking, I know), but from far too much time spent watching HGTV, I understand the basic gist of it. You lay a foundation, setup framing and walls, route mechanical and electrical, then work on final touches like painting and decorating (to be sure, I’m hand-waiving a lot of detail away here). There’s a bit of wiggle room in the order you go about things — you can paint the living room walls before the ones in the bathroom or vice versa — but some steps definitely need to happen before others — you can’t paint either rooms until the walls themselves are actually up!&lt;/p&gt;
&lt;p&gt;The same logic applies for data science — there are certain activities that are exceptionally hard to do without the proper infrastructure in place. Sometimes, we’re asked to chase after ~shiny objects~ without the support system to do so, when doing so may actually make our job more difficult in the future!&lt;/p&gt;
&lt;p&gt;I recently stumbled across &lt;a href=&#34;https://towardsdatascience.com/the-data-science-pyramid-8a018013c490&#34;&gt;an article&lt;/a&gt; that summarized this really succinctly with the following graphic: &lt;strong&gt;The Data Science Hierarchy of Needs&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/ds_hierarchy.png&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;collect&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Collect&lt;/h4&gt;
&lt;p&gt;At a baseline, to do any sort of data work you need to actually have data on hand to work with! Whether there’s a formal process for collecting data or you need to gather data from disparate public sources, getting raw data out of the wild and into your system is the first step to being able to do any sort of analysis. In my case, as an analyst with a hospital’s patient satisfaction group, we need to actually send patients surveys.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;movestore&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Move/Store&lt;/h4&gt;
&lt;p&gt;Once you know where your data is coming from, setting up a reliable data flow from the source to your environment is needed. This is where a lot of headache can come from. Gathering data can be difficult but if the data is going to be used once for a one-off analysis, you don’t need to worry too much about repeatability, edge cases, or computing speed. Once you need to gather new data, thinking about infrastructure around new data gathering becomes much more important. A good chunk of the last eight months of my job has been working with our new survey vendor on this piece of the puzzle: standardizing data layouts, catching bugs in the pipeline, and setting up standards for access.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploretransform&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Explore/Transform&lt;/h4&gt;
&lt;p&gt;With a reliable flow of new/updated data streaming in, you now need to make sure the data is appropriate for general use. Automated anomaly/fraud/bot detection, light wrangling, and removing errant responses can all be considered a part of this single stage. This is necessary to ensure that any analyses you do or models you build are based on what you expect from the underlying data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregatelabel&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Aggregate/Label&lt;/h4&gt;
&lt;p&gt;I can’t recall the source, but the following quote about data science has stuck with me: “99% of data science work is counting — sometimes dividing.” A significant portion of my day-to-day work involves the tried-and-trusted &lt;code&gt;group_by() %&amp;gt;% summarise()&lt;/code&gt; pipeline. Making counts, percentages, running totals, etc. accessible to stakeholders via a dashboard can likely answer ~80% of the questions an analyst would have to field otherwise. It’s &lt;em&gt;so, so&lt;/em&gt; important, however, to have the collection, storage, and preparation stages setup prior to ensure that stakeholders can &lt;em&gt;trust&lt;/em&gt; that the data they’re seeing is accurate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;learnoptimize&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Learn/Optimize&lt;/h4&gt;
&lt;p&gt;If 80% of questions asked can be solved with grouped summaries and 20% require a model, it’s likely that 80% of that remaining 20% can be solved by a simple linear model. For example, “What effect does patient age have on their overall satisfaction?” can be answered with &lt;code&gt;lm(satisfaction_score ~ age)&lt;/code&gt;. As relationships become more complex, you can add more terms to the model, or switch model architectures, but — in my own experience — the majority of modeling in practice can be represented by linear models (and, by extension, regularized models via &lt;code&gt;{glmnet}&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;complex-models&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Complex Models&lt;/h4&gt;
&lt;p&gt;Finally, a small subset of problems may require a more complex or powerful model type. But before you spin your wheels building a neural net or some other wacky architecture, you should first check if something simpler gets you what you need.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-closing-thoughts&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Some Closing Thoughts&lt;/h4&gt;
&lt;p&gt;This post is partially meant to be able to share some useful info and partially a reminder to me to look for the simple solution! I have a tendency to start off with something complex then realize that I could save a lot of work if I just switch to something simpler. The three baseline layers upstream of my domain are &lt;em&gt;super important&lt;/em&gt; and definitely need oversight from someone with an eye for data engineering.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polling-bites&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Polling Bites&lt;/h3&gt;
&lt;p&gt;The generic congressional ballot is starting to show some movement away from even split as Republicans have slowly climbed to &lt;strong&gt;51.2%&lt;/strong&gt; in the polls and Democrats have fallen to &lt;strong&gt;48.8%&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/generic_ballot_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Biden’s net approval hasn’t shifted significantly since the last post — currently sitting at &lt;strong&gt;10.9%&lt;/strong&gt; underwater with &lt;strong&gt;41.8%&lt;/strong&gt; approval and &lt;strong&gt;52.7%&lt;/strong&gt; disapproval.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/approval_disapproval_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/net_approval_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(p.s., I’ve updated the color palettes here with the &lt;a href=&#34;https://github.com/BlakeRMills/MetBrewer&#34;&gt;&lt;code&gt;{MetBrewer}&lt;/code&gt;&lt;/a&gt; package, which provides colorblind friendly palettes based on artwork in hte Metropolitan Museum of Art in New York).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Pull Yourself Up by Your Bootstraps</title>
      <link>https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/</link>
      <pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Statistical modeling sometimes presents conflicting goals. Oftentimes, building a model involves a mix of objectives that don’t necessarily mesh well together: super-accurate point predictions, explainability, fast performance, or an expression of confidence in the prediction. In my work as an analyst, I generally am focused on how explainable the model is while being able to express a confidence interval around each prediction. For that, simple linear models do the trick. If, however, I want to regularize via &lt;code&gt;{glmnet}&lt;/code&gt; (which — with good reason — &lt;a href=&#34;https://stats.stackexchange.com/questions/224796/why-are-confidence-intervals-and-p-values-not-reported-as-default-for-penalized&#34;&gt;doesn’t provide confidence intervals&lt;/a&gt;) or use a non-linear model like &lt;code&gt;{xgboost}&lt;/code&gt;, I have to drop the confidence interval around predictions. Or so I had previously thought! As it turns out, building a series of models from bootstrap resamples provides an alternative method of generating a confidence interval around a prediction.&lt;/p&gt;
&lt;div id=&#34;setting-a-baseline-with-penguins&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting a baseline with penguins&lt;/h3&gt;
&lt;p&gt;First, let’s build out a baseline linear model with the Palmer Penguins dataset. This dataset contains information on 344 penguins across three species types and three islands. For this example, we’ll use the penguin information to predict &lt;code&gt;body_mass_g&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the data in from the tidytuesdayR package
penguins_src &amp;lt;- tidytuesdayR::tt_load(2020, week = 31)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Downloading file 1 of 2: `penguins.csv`
##  Downloading file 2 of 2: `penguins_raw.csv`&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract out the penguins dataset
penguins &amp;lt;- penguins_src$penguins
rm(penguins_src)

penguins&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 344 x 8
##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
##  1 Adelie  Torgersen           39.1          18.7               181        3750
##  2 Adelie  Torgersen           39.5          17.4               186        3800
##  3 Adelie  Torgersen           40.3          18                 195        3250
##  4 Adelie  Torgersen           NA            NA                  NA          NA
##  5 Adelie  Torgersen           36.7          19.3               193        3450
##  6 Adelie  Torgersen           39.3          20.6               190        3650
##  7 Adelie  Torgersen           38.9          17.8               181        3625
##  8 Adelie  Torgersen           39.2          19.6               195        4675
##  9 Adelie  Torgersen           34.1          18.1               193        3475
## 10 Adelie  Torgersen           42            20.2               190        4250
## # ... with 334 more rows, and 2 more variables: sex &amp;lt;chr&amp;gt;, year &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll need to do some lite preprocessing before we start modeling — it looks like there are some &lt;code&gt;NAs&lt;/code&gt; in &lt;code&gt;body_mass_g&lt;/code&gt; and in &lt;code&gt;sex&lt;/code&gt;. If I were creating a more serious model, I might keep the rows with &lt;code&gt;NAs&lt;/code&gt; for &lt;code&gt;sex&lt;/code&gt;, but since there are so few and this is an explainer, I’ll just filter them out.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# remove NA from body_mass_g and sex
penguins &amp;lt;- 
  penguins %&amp;gt;%
  filter(!is.na(body_mass_g),
         !is.na(sex))

penguins&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 333 x 8
##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
##  1 Adelie  Torgersen           39.1          18.7               181        3750
##  2 Adelie  Torgersen           39.5          17.4               186        3800
##  3 Adelie  Torgersen           40.3          18                 195        3250
##  4 Adelie  Torgersen           36.7          19.3               193        3450
##  5 Adelie  Torgersen           39.3          20.6               190        3650
##  6 Adelie  Torgersen           38.9          17.8               181        3625
##  7 Adelie  Torgersen           39.2          19.6               195        4675
##  8 Adelie  Torgersen           41.1          17.6               182        3200
##  9 Adelie  Torgersen           38.6          21.2               191        3800
## 10 Adelie  Torgersen           34.6          21.1               198        4400
## # ... with 323 more rows, and 2 more variables: sex &amp;lt;chr&amp;gt;, year &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s always good practice to explore the dataset prior to fitting a model, so let’s jump into some good ol’ fashioned EDA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how are species/island related to body mass?
penguins %&amp;gt;%
  ggplot(aes(x = species,
             y = body_mass_g,
             color = species)) +
  geom_boxplot() +
  geom_point(alpha = 0.25,
             position = position_jitter()) +
  facet_wrap(~island)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Interesting! It looks like the Gentoo and Chinstrap species are only found on the Biscoe and Dream islands, respectively, whereas the Adelie species can be found on all three islands. At first glance, there’s not a meaningful difference that Island has on the weight of the Adelie penguins, so I think we’re safe to toss out the &lt;code&gt;island&lt;/code&gt; feature and just keep &lt;code&gt;species&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how does sex relate to body mass?
penguins %&amp;gt;%
  ggplot(aes(x = sex,
             y = body_mass_g,
             color = sex)) +
  geom_boxplot() +
  geom_point(alpha = 0.25,
             position = position_jitter())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unsurprisingly, male penguins are typically heavier than female penguins.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# are penguins getting heavier or lighter as years progress?
penguins %&amp;gt;%
  mutate(year = as.character(year)) %&amp;gt;%
  ggplot(aes(x = year,
             y = body_mass_g)) +
  geom_boxplot() +
  geom_point(alpha = 0.25,
             position = position_jitter())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It doesn’t look like there is significant signal being drawn from &lt;code&gt;year&lt;/code&gt;, so we’ll toss that out as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how do other body measurements compare with the total body mass?
penguins %&amp;gt;%
  select(bill_length_mm:body_mass_g) %&amp;gt;%
  pivot_longer(ends_with(&amp;quot;mm&amp;quot;),
               names_to = &amp;quot;measurement&amp;quot;,
               values_to = &amp;quot;value&amp;quot;) %&amp;gt;%
  ggplot(aes(x = value,
             y = body_mass_g,
             color = measurement)) +
  geom_point(alpha = 0.5) + 
  facet_wrap(~measurement, scales = &amp;quot;free_x&amp;quot;) +
  geom_smooth(method = &amp;quot;lm&amp;quot;,
              se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For bill and flipper length, there’s a pretty clear relationship, but it looks like bill depth has a &lt;em&gt;classic&lt;/em&gt; case of &lt;a href=&#34;https://en.wikipedia.org/wiki/Simpson%27s_paradox&#34;&gt;Simpson’s paradox&lt;/a&gt;. Let’s explore that further to find a meaningful interaction to apply.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# which feature interacts with bill depth to produce simpson&amp;#39;s pardox?
penguins %&amp;gt;%
  ggplot(aes(x = bill_depth_mm,
             y = body_mass_g,
             color = species)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = &amp;quot;lm&amp;quot;,
              se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So, very clearly, the Gentoo species has a very different relationship between bill depth and body mass than the Adelie/Chinstrap species. We’ll add this as an interactive feature to the model.&lt;/p&gt;
&lt;p&gt;With all that completed, let’s (finally) setup and build the baseline linear model with confidence intervals around the prediction!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# remove features
penguins &amp;lt;- 
  penguins %&amp;gt;%
  select(-island, -year)

# split into testing and training datasets
set.seed(123)
penguins_split &amp;lt;- initial_split(penguins)
penguins_test &amp;lt;- testing(penguins_split)
penguins_train &amp;lt;- training(penguins_split)

# setup a pre-processing recipe
penguins_rec &amp;lt;- 
  recipe(body_mass_g ~ ., data = penguins_train) %&amp;gt;%
  step_dummy(all_nominal()) %&amp;gt;% 
  step_interact(~starts_with(&amp;quot;species&amp;quot;):bill_depth_mm)

# fit a workflow
penguins_lm &amp;lt;- 
  workflow() %&amp;gt;%
  add_recipe(penguins_rec) %&amp;gt;%
  add_model(linear_reg() %&amp;gt;% set_engine(&amp;quot;lm&amp;quot;)) %&amp;gt;%
  fit(penguins_train)

# predict on training data with confidence intervals
bind_cols(penguins_lm %&amp;gt;% predict(penguins_train),
          penguins_lm %&amp;gt;% predict(penguins_train, type = &amp;quot;conf_int&amp;quot;, level = 0.95),
          penguins_train) %&amp;gt;%
  ggplot(aes(x = body_mass_g,
             y = .pred)) +
  geom_point(alpha = 0.5) +
  geom_segment(aes(x = body_mass_g,
                   xend = body_mass_g,
                   y = .pred_lower,
                   yend = .pred_upper),
               alpha = 0.25) +
  labs(title = &amp;quot;Predicting the Palmer Penguins - Training&amp;quot;,
       subtitle = &amp;quot;Linear model predicting a penguin&amp;#39;s weight in grams&amp;quot;,
       x = &amp;quot;Actual weight (g)&amp;quot;,
       y = &amp;quot;Predicted weight (g)&amp;quot;,
       caption = &amp;quot;Errorbars represent the a 95% confidence interval&amp;quot;) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This model does generally okay, but the confidence interval around each prediction is pretty &lt;a href=&#34;https://mc-stan.org/rstanarm/articles/rstanarm.html&#34;&gt;clearly too confident&lt;/a&gt;! Let’s solve this with bootstrapping.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;whats-a-bootstrap&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What’s a bootstrap?&lt;/h3&gt;
&lt;p&gt;Before progressing any further, it’s probably important to define what exactly a bootstrap is/what bootstrapping is. Bootstrapping is a resampling method that lets us take one dataset and turn it into many datasets. Bootstrapping accomplishes this by repeatedly pulling a random row from the source dataset and, importantly, bootstrapping allows for rows to be repeated! Let’s look at an example for a bit more clarity.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;rowid&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;104&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;102&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;79&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let’s say we want to make bootstrap resamples of this dataset. We’ll draw five random rows from the dataset and, sometimes, we’ll have the same row show up in our new bootstrapped dataset multiple times:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;rowid&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;104&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;102&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Another bootstrap dataset might look like this:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;rowid&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;102&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;79&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;79&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Bootstrap datasets allow us to create many datasets from the original dataset and evaluate models across these bootstraps. Models that are well informed will give similar outputs across each dataset, despite of the randomness within each dataset, whereas less confident models will have a wider variation across the bootstrapped datasets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generating-some-confident-penguins&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Generating some confident penguins&lt;/h3&gt;
&lt;p&gt;Let’s say we want to use &lt;code&gt;{xgboost}&lt;/code&gt; to predict penguin weight and we’ll use bootstrapping to generate a confidence interval. Firstly, we’ll create the bootstrap datasets from our training set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;penguins_boot &amp;lt;- penguins_train %&amp;gt;% bootstraps()

penguins_boot&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # Bootstrap sampling 
## # A tibble: 25 x 2
##    splits           id         
##    &amp;lt;list&amp;gt;           &amp;lt;chr&amp;gt;      
##  1 &amp;lt;split [249/92]&amp;gt; Bootstrap01
##  2 &amp;lt;split [249/90]&amp;gt; Bootstrap02
##  3 &amp;lt;split [249/91]&amp;gt; Bootstrap03
##  4 &amp;lt;split [249/87]&amp;gt; Bootstrap04
##  5 &amp;lt;split [249/98]&amp;gt; Bootstrap05
##  6 &amp;lt;split [249/84]&amp;gt; Bootstrap06
##  7 &amp;lt;split [249/91]&amp;gt; Bootstrap07
##  8 &amp;lt;split [249/95]&amp;gt; Bootstrap08
##  9 &amp;lt;split [249/94]&amp;gt; Bootstrap09
## 10 &amp;lt;split [249/86]&amp;gt; Bootstrap10
## # ... with 15 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, the &lt;code&gt;bootstraps()&lt;/code&gt; function will create 25 bootstrap datasets, but we could theoretically create as many as we want. Now that we have our bootstraps, let’s create a function that will fit a model to each of the bootstraps and save to disk. We’ll use the default parameters for our &lt;code&gt;{xgboost}&lt;/code&gt; model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define a basic xgboost model
penguins_xgb &amp;lt;-
  boost_tree() %&amp;gt;%
  set_mode(&amp;quot;regression&amp;quot;) %&amp;gt;%
  set_engine(&amp;quot;xgboost&amp;quot;)

# function that will fit a model and save to a folder
fit_bootstrap &amp;lt;- function(index) {
  
  # pull out individual bootstrap to fit
  xgb_boot &amp;lt;- penguins_boot$splits[[index]] %&amp;gt;% training()
  
  # fit to a workflow
  workflow() %&amp;gt;%
    add_recipe(penguins_rec) %&amp;gt;%
    add_model(penguins_xgb) %&amp;gt;%
    fit(xgb_boot) %&amp;gt;%
    write_rds(paste0(&amp;quot;models/model_&amp;quot;, index, &amp;quot;.rds&amp;quot;))
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function will create a new model for each bootstrap, so we’ll end up with 25 separate models. Let’s fit!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fit to 25 bootstrapped datasets
for (i in 1:25) {
  
  fit_bootstrap(i)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s define a function that will predict based on these 25 bootstrapped models, then predict on our training data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_bootstrap &amp;lt;- function(new_data, index){
  
  read_rds(paste0(&amp;quot;models/model_&amp;quot;, index, &amp;quot;.rds&amp;quot;)) %&amp;gt;%
    predict(new_data) %&amp;gt;%
    rename(!!sym(paste0(&amp;quot;pred_&amp;quot;, index)) := .pred)
  
}

# predict!
training_preds &amp;lt;- 
  seq(1, 25) %&amp;gt;%
  map_dfc(~predict_bootstrap(penguins_train, .x))

training_preds&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 249 x 25
##    pred_1 pred_2 pred_3 pred_4 pred_5 pred_6 pred_7 pred_8 pred_9 pred_10
##     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1  5552.  5638.  5555.  5703.  5726.  5783.  5404.  5566.  5493.   5547.
##  2  3470.  3340.  3334.  3350.  3311.  3303.  3315.  3421.  3692.   3436.
##  3  5309.  5274.  5241.  5286.  5206.  5084.  5506.  5531.  5274.   5309.
##  4  4160.  4013.  3988.  4111.  4075.  4073.  4284.  4050.  4033.   4033.
##  5  4003.  3931.  4096.  3968.  4008.  3918.  3941.  4093.  3941.   3880.
##  6  3967.  4039.  4095.  4047.  4021.  4055.  3980.  4115.  4067.   4084.
##  7  4647.  4551.  4750.  4555.  4690.  4396.  4235.  4686.  4764.   4659.
##  8  5240.  5288.  5291.  5276.  5308.  5508.  5570.  5375.  5340.   5268.
##  9  4138.  4111.  4106.  4236.  4135.  4219.  4218.  4211.  4160.   4071.
## 10  4728.  4723.  4715.  4823.  4765.  4727.  4836.  4777.  4765.   4633.
## # ... with 239 more rows, and 15 more variables: pred_11 &amp;lt;dbl&amp;gt;, pred_12 &amp;lt;dbl&amp;gt;,
## #   pred_13 &amp;lt;dbl&amp;gt;, pred_14 &amp;lt;dbl&amp;gt;, pred_15 &amp;lt;dbl&amp;gt;, pred_16 &amp;lt;dbl&amp;gt;, pred_17 &amp;lt;dbl&amp;gt;,
## #   pred_18 &amp;lt;dbl&amp;gt;, pred_19 &amp;lt;dbl&amp;gt;, pred_20 &amp;lt;dbl&amp;gt;, pred_21 &amp;lt;dbl&amp;gt;, pred_22 &amp;lt;dbl&amp;gt;,
## #   pred_23 &amp;lt;dbl&amp;gt;, pred_24 &amp;lt;dbl&amp;gt;, pred_25 &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a column of predictions for each model — we can summarise our point prediction for each row with the average across all models and set the confidence interval based on the standard deviation of the predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_preds %&amp;gt;%
  bind_cols(penguins_train) %&amp;gt;%
  rowid_to_column() %&amp;gt;%
  pivot_longer(starts_with(&amp;quot;pred_&amp;quot;),
               names_to = &amp;quot;model&amp;quot;,
               values_to = &amp;quot;.pred&amp;quot;) %&amp;gt;%
  group_by(rowid) %&amp;gt;%
  summarise(body_mass_g = max(body_mass_g),
            .pred_mean = mean(.pred),
            std_dev = sd(.pred)) %&amp;gt;%
  riekelib::normal_interval(.pred_mean, std_dev) %&amp;gt;%
  ggplot(aes(x = body_mass_g,
             y = .pred_mean)) +
  geom_point(alpha = 0.5) +
  geom_segment(aes(x = body_mass_g, 
                   xend = body_mass_g,
                   y = ci_lower,
                   yend = ci_upper),
               alpha = 0.25) +
  labs(title = &amp;quot;Predicting the Palmer Penguins - Training&amp;quot;,
       subtitle = &amp;quot;XGBoost model predicting a penguin&amp;#39;s weight in grams&amp;quot;,
       x = &amp;quot;Actual weight (g)&amp;quot;,
       y = &amp;quot;Predicted weight (g)&amp;quot;,
       caption = &amp;quot;Errorbars represent the a 95% confidence interval&amp;quot;) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And just like that, we’ve trained a series of models with &lt;code&gt;{xgboost}&lt;/code&gt; that let us apply a confidence interval around a point prediction! Now that we’ve done so on the training set, let’s look at performance on the test set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(1, 25) %&amp;gt;%
  map_dfc(~predict_bootstrap(penguins_test, .x)) %&amp;gt;%
  bind_cols(penguins_test) %&amp;gt;%
  rowid_to_column() %&amp;gt;%
  pivot_longer(starts_with(&amp;quot;pred_&amp;quot;),
               names_to = &amp;quot;model&amp;quot;,
               values_to = &amp;quot;.pred&amp;quot;) %&amp;gt;%
  group_by(rowid) %&amp;gt;%
  summarise(body_mass_g = max(body_mass_g),
            .pred_mean = mean(.pred),
            std_dev = sd(.pred)) %&amp;gt;%
  riekelib::normal_interval(.pred_mean, std_dev) %&amp;gt;%
  ggplot(aes(x = body_mass_g,
             y = .pred_mean)) +
  geom_point(alpha = 0.5) +
  geom_segment(aes(x = body_mass_g, 
                   xend = body_mass_g,
                   y = ci_lower,
                   yend = ci_upper),
               alpha = 0.25) +
  labs(title = &amp;quot;Predicting the Palmer Penguins - Testing&amp;quot;,
       subtitle = &amp;quot;XGBoost model predicting a penguin&amp;#39;s weight in grams&amp;quot;,
       x = &amp;quot;Actual weight (g)&amp;quot;,
       y = &amp;quot;Predicted weight (g)&amp;quot;,
       caption = &amp;quot;Errorbars represent the a 95% confidence interval&amp;quot;) +
  theme(plot.title.position = &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The performance on the test data is slightly less accurate than on the training data, but that is to be expected. Importantly, we’ve used bootstrap resampling to generate a confidence interval from a model that otherwise normally returns a simple point prediction.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-noteworthy-caveats&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some noteworthy caveats&lt;/h3&gt;
&lt;p&gt;The prediction interval above is all well and good, but it comes with some &lt;em&gt;hefty&lt;/em&gt; caveats. Firstly, the confidence interval in the Testing plot is generated from the mean and standard deviation from each prediction. This assumes that the predictions are distributed normally, which may not necessarily be the case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_preds %&amp;gt;%
  slice_head(n = 1) %&amp;gt;%
  pivot_longer(starts_with(&amp;quot;pred&amp;quot;)) %&amp;gt;%
  ggplot(aes(x = value)) +
  geom_density()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-02-08-pull-yourself-up-by-your-bootstraps/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This density plot for one of the predictions shows that there’s definitely some non-normal behavior! There’s a few ways of addressing this.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create many, many, more bootstraps and models so that the prediction distribution approaches normality (with only 25 points, we really shouldn’t even expect normality from this example).&lt;/li&gt;
&lt;li&gt;Report out the actual values of the percentiles in the distribution (e.g., the 2.5% percentile is below X, 97.5% is above Y, and the mean is at Z).&lt;/li&gt;
&lt;li&gt;Report out the actual distribution as the result.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Ideally, you should do all three.&lt;/p&gt;
&lt;p&gt;The second major caveat is that this is not one model, but a whole host of models and these take up a large amount of disk space. In this example, our 25 models take up 25 times more space than our original model and it takes some time to read in, fit, and wrangle the results. We can trade disk space for computation time by writing a function that fits and predicts without saving a model, but again, that’s a tradeoff between speed and space. For linear models, it may be a better route to have STAN simulate thousands of results via &lt;code&gt;{rstanarm}&lt;/code&gt; or &lt;code&gt;{brms}&lt;/code&gt;, but for non-linear models, boostrapping is the best way to go for now!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polling-bites&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Polling Bites&lt;/h3&gt;
&lt;p&gt;Currently, the Generic Ballot is holding steady with a slight sliver more Americans wanting Republicans in Congress than Democrats (&lt;strong&gt;50.7%&lt;/strong&gt; to &lt;strong&gt;49.3%&lt;/strong&gt;, respectively). Joe Biden’s net approval continues to slide, currently sitting at &lt;strong&gt;-11.4%&lt;/strong&gt; (&lt;strong&gt;41.8%&lt;/strong&gt; approve, &lt;strong&gt;53.1%&lt;/strong&gt; disapprove).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/generic_ballot_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/approval_disapproval_current.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pics/net_approval_current.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Technical Books!</title>
      <link>https://www.thedatadiary.net/blog/2021-11-28-technical-books/</link>
      <pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2021-11-28-technical-books/</guid>
      <description>
&lt;script src=&#34;https://www.thedatadiary.net/blog/2021-11-28-technical-books/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Happy (belated) Thanksgiving! This year, my family drove down to Houston for the holiday &amp;amp; I hosted Thanksgiving for the first time. We played lots of games and ate well - my fridge is &lt;em&gt;still&lt;/em&gt; stocked full of leftovers. Knowing we’d be busy with hosting, I planned ahead and scheduled a lighter post - this week, I thought I’d highlight some technical books that I’ve either referenced for modeling work, have been recommended to me, or I’ve heard about and would like to read:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf&#34;&gt;The Elements of Statistical Learning&lt;/a&gt;&lt;/strong&gt; is referenced as the Bible of Machine Learning by &lt;a href=&#34;https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw&#34;&gt;Josh Starmer&lt;/a&gt; and provides a robust and deeply technical foundation for a wide array of machine learning models. It’s considered a must-have among both machine learning theorists, who look for new model structures, and practitioners (like myself!).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://web.stanford.edu/~hastie/ISLR2/ISLRv2_website.pdf&#34;&gt;An Introduction to Statistical Learning with Applications in R&lt;/a&gt;&lt;/strong&gt; is a companion to &lt;em&gt;The Elements of Statistical Learning&lt;/em&gt;. &lt;em&gt;An Introduction to Statistical Learning&lt;/em&gt; arose as a broader and less technical treatment of the key topics discussed in &lt;em&gt;The Elements of Statistical Learning&lt;/em&gt;. Each section also includes learning-lab lessons walking through the implementation of the statistical learning method from that chapter (&lt;a href=&#34;https://www.emilhvitfeldt.com/&#34;&gt;Emil Hvitfeldt&lt;/a&gt; is also working on a &lt;a href=&#34;https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/&#34;&gt;companion site&lt;/a&gt; for completing the labs with tidymodels).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.tmwr.org/index.html&#34;&gt;Tidy Modeling with R&lt;/a&gt;&lt;/strong&gt; is a guide to using the tidymodel framework and has been an excellent reference in both personal and professional projects.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.tidytextmining.com/&#34;&gt;Text Mining with R, a Tidy Approach&lt;/a&gt;&lt;/strong&gt; serves as an introduction to text mining and other methods for dealing with unstructured, non-rectangular data. In my current role as a Consumer Experience Analyst, I have to interact with unstructured data (in the form of patient comments) daily - this book, along with the &lt;a href=&#34;https://juliasilge.github.io/tidytext/&#34;&gt;tidytext&lt;/a&gt; package, have been incredibly useful for analyzing and visualizing text data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://smltar.com/preface.html&#34;&gt;Supervised Machine Learning for Text Analysis in R&lt;/a&gt;&lt;/strong&gt; picks up where &lt;em&gt;Text Mining with R&lt;/em&gt; left off by exploring (as the title suggests) supervised machine learning methods with text data. While I haven’t done extensive text modeling, this is one area that I’d like to explore further in 2022.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;http://www.feat.engineering/&#34;&gt;Feature Engineering and Selection: A Practical Approach for Predictive Models&lt;/a&gt;&lt;/strong&gt; is a guidebook offering methods for feature engineering (transforming and creating new predictor variables to improve predictive model performance). While I’ve utilized some basic feature engineering in some of my work, I’m interested in adding more robust tools to my feature-engineering toolkit!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://drob.gumroad.com/l/empirical-bayes&#34;&gt;Introduction to Empirical Bayes&lt;/a&gt;&lt;/strong&gt; is &lt;a href=&#34;http://varianceexplained.org/about/&#34;&gt;David Robinson’s&lt;/a&gt; book coalescing a series of blog posts on Bayesian estimation, credible intervals, A/B testing, mixed models, and a host of other methods, all through the example of baseball batting averages.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://jnolis.com/book/&#34;&gt;Build a Career in Data Science&lt;/a&gt;&lt;/strong&gt; is, as the name suggests, a book about building a career in data science. I generally feel that most career-help books are too broad to be useful or offer non-novel information for those in the industry the book is written for. Given, however, that I don’t have an academic or professional background in the field and that I’d like to eventually move from analytics to data science, I’d like to add this to the collection to pick up on some best practices.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
