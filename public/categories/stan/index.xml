<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>stan on the data diary</title>
    <link>https://www.thedatadiary.net/categories/stan/</link>
    <description>Recent content in stan on the data diary</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Mark Rieke {year}</copyright>
    <lastBuildDate>Fri, 30 Dec 2022 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://www.thedatadiary.net/categories/stan/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>My 2022 Magnum Opus</title>
      <link>https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/</link>
      <pubDate>Fri, 30 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/</guid>
      <description>


&lt;p&gt;Over the past few years, the hospital system I work for has transitioned from the old metric for measuring patient satisfaction, Likelihood to Recommend (LTR), to a newer metric, Net Promoter Score (NPS). Both metrics ask the same question — &lt;em&gt;how likely are you to recommend this hospital to a friend or relative?&lt;/em&gt; — but they are measured very differently. The score for LTR is simply the percentage of patients who respond with the “topbox” option of &lt;em&gt;Very likely&lt;/em&gt; on a scale from &lt;em&gt;Very likely&lt;/em&gt; to &lt;em&gt;Very unlikely&lt;/em&gt;. NPS, on the other hand, is a bit more involved. Patients are categorized based on their response on a 0-10 point scale: responses between 0 and 6 are considered &lt;em&gt;detractors&lt;/em&gt;, 7 and 8s are considered &lt;em&gt;passives&lt;/em&gt;, while 9 and 10s are considered &lt;em&gt;promoters&lt;/em&gt;. The score for NPS is then the percentage of promoters &lt;em&gt;minus&lt;/em&gt; the percentage of detractors.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
\text{NPS} = \text{Promoter %} - \text{Detractor %}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As a metric, NPS is a bit better than the alternative of LTR, since it is somewhat able to take into account the distribution of responses along the 0-10 scale. Consider the following set of responses (and let’s just pretend for sake of example that “promoter” here is equivalent to “topbox”). LTR’s topbox isn’t able to detect a difference in scores since promoters comprise 50% of responses in both sets. NPS, however, &lt;em&gt;can&lt;/em&gt; detect a difference, since the second set is rewarded for have fewer detractors than the first set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble::tribble(
  ~promoters, ~passives, ~detractors, ~topbox, ~nps,
  &amp;quot;50%&amp;quot;, &amp;quot;25%&amp;quot;, &amp;quot;25%&amp;quot;, &amp;quot;50%&amp;quot;, &amp;quot;25%&amp;quot;,
  &amp;quot;50%&amp;quot;, &amp;quot;50%&amp;quot;, &amp;quot;0%&amp;quot;, &amp;quot;50%&amp;quot;, &amp;quot;50%&amp;quot;
) |&amp;gt;
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;promoters&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;passives&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;detractors&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;topbox&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;nps&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;50%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;25%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;25%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;50%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;25%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;50%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;50%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;50%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;50%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With only a few sets of responses to compare, this seems like a trivial improvement — since we have all the data, why don’t we just look at the response distribution for every set? In practice, however, I’m often looking at responses for &lt;em&gt;hundreds&lt;/em&gt; of individual hospital units across the system — encoding this extra bit of information into a single number allows for a more nuanced comparison &lt;em&gt;without&lt;/em&gt; any costs to the viewer’s cognitive load.&lt;/p&gt;
&lt;p&gt;Unfortunately, there’s no free lunch here, and the additional nuance that NPS provides comes at the cost of modeling complexity. Relative modeling binary choices like LTR’s topbox, the ecosystem for modeling the choice between three or more categories is far smaller. Additionally, the &lt;em&gt;order&lt;/em&gt; of the categories matters — a promoter response is better than a passive response, which is better than a detractor response. This adds a layer of complexity over unordered categories (e.g., red, blue, or green).&lt;/p&gt;
&lt;p&gt;Fortunately, I’m not the first person to run into this problem. I’ve been (slowly) working through &lt;a href=&#34;https://xcelab.net/rm/&#34;&gt;Richard McElreath’s&lt;/a&gt; &lt;a href=&#34;https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919&#34;&gt;Statistical Rethinking&lt;/a&gt;, which conveniently covers this topic directly (and comes with the added benefit of utilizing a Bayesian approach via &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Let’s test both my understanding of the ordered categorical data-generating process and my ability to model it by doing a few things:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Define a data-generating process that links a linear model to an ordered categorical outcome.&lt;/li&gt;
&lt;li&gt;Manually set model parameters and simulate responses.&lt;/li&gt;
&lt;li&gt;See if I can recover those parameters with a model in Stan.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;the-data-generating-process&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The data generating process&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(rethinking)
library(riekelib)
library(broom.mixed)

# I&amp;#39;ve been having ~issues~ with cmdstan, so switching default to rstan
set_ulam_cmdstan(FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each patient’s response &lt;span class=&#34;math inline&#34;&gt;\(R_i\)&lt;/span&gt; can be described as a probability &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; of selecting from each of the three available categories:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
R_i \sim \text{Categorical}(p_i) \\
p_i = \langle p_{\text{detractor}[i]}, \ p_{\text{passive}[i]}, \ p_{\text{promoter}[i]} \rangle \\
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There’s a useful math trick we can use to enforce the order of the categories. Rather than working with the &lt;em&gt;individual probability&lt;/em&gt; of each category directly, we can instead define the probabilities in terms of the &lt;em&gt;cumulative probability&lt;/em&gt; of each category. For example, in the set below, the probability of selecting “passive” is 10%, but the &lt;em&gt;cumulative probability&lt;/em&gt; of selecting a rating of passive &lt;em&gt;or lower&lt;/em&gt; is 30%:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;probs_example &amp;lt;- 
  tibble(nps_group = as_factor(c(&amp;quot;detractor&amp;quot;, &amp;quot;passive&amp;quot;, &amp;quot;promoter&amp;quot;)),
         prob = c(0.2, 0.1, 0.7),
         cumulative_prob = cumsum(prob))

probs_example %&amp;gt;%
  mutate(across(ends_with(&amp;quot;prob&amp;quot;), ~scales::label_percent(accuracy = 1)(.x))) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;nps_group&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;prob&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;cumulative_prob&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;detractor&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;20%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;20%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;passive&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;promoter&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;70%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;100%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With this in mind, any individual probability can be described as the difference between two cumulative probabilities &lt;span class=&#34;math inline&#34;&gt;\(q_k\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;probs_example %&amp;gt;%
  ggplot(aes(x = nps_group,
             xend = nps_group)) + 
  geom_hline(yintercept = c(0.2, 0.3),
             linetype = &amp;quot;dashed&amp;quot;,
             color = &amp;quot;gray60&amp;quot;) + 
  geom_segment(aes(y = 0,
                   yend = cumulative_prob)) +
  geom_point(aes(y = cumulative_prob)) +
  geom_segment(aes(y = cumulative_prob - prob,
                   yend = cumulative_prob),
               color = &amp;quot;blue&amp;quot;,
               position = position_nudge(x = 0.125)) + 
  geom_point(aes(y = cumulative_prob),
             color = &amp;quot;blue&amp;quot;,
             position = position_nudge(x = 0.125)) + 
  geom_text(x = 1 - 0.1,
            y = 0.1,
            label = &amp;quot;q1: 20%&amp;quot;,
            hjust = &amp;quot;right&amp;quot;) +
  geom_text(x = 1 + 0.125 + 0.1,
            y = 0.1,
            label = &amp;quot;p1: 20%&amp;quot;,
            hjust = &amp;quot;left&amp;quot;,
            color = &amp;quot;blue&amp;quot;) + 
  geom_text(x = 2 - 0.1,
            y = 0.25,
            label = &amp;quot;q2: 30%&amp;quot;,
            hjust = &amp;quot;right&amp;quot;) +
  geom_text(x = 2 + 0.125 + 0.1,
            y = 0.25,
            label = &amp;quot;p2: 10%&amp;quot;,
            hjust = &amp;quot;left&amp;quot;,
            color = &amp;quot;blue&amp;quot;) +
  geom_text(x = 3 + 0.125 + 0.1,
            y = 0.625,
            label = &amp;quot;p3: 70%&amp;quot;,
            color = &amp;quot;blue&amp;quot;,
            hjust = &amp;quot;left&amp;quot;) +
  expand_limits(y = c(0, 1)) +
  scale_y_continuous(labels = scales::label_percent()) +
  labs(title = glue::glue(&amp;quot;**Cumulative** and 
                          **{color_text(&amp;#39;individual&amp;#39;, &amp;#39;blue&amp;#39;)}** 
                          probabilities of each response&amp;quot;),
       x = NULL,
       y = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/index_files/figure-html/cumulative-prob-plot-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The probability of selecting a response less than detractor is 0% and the probability of selecting a response of promoter &lt;em&gt;or lower&lt;/em&gt; is 100%, so we can rewrite the individual probabilities in terms of just two cumulative probabilities.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
R_i \sim \text{Categorical}(p_i) \\
p_i = \langle p_{\text{detractor}[i]}, \ p_{\text{passive}[i]}, \ p_{\text{promoter}[i]} \rangle \\
\color{blue}{p_{\text{detractor}[i]} = q_{1, i} \\
p_{\text{passive}[i]} = q_{2,i} - q_{1,i} \\
p_{\text{promoter}[i]} = 1 - q_{2,i}}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the &lt;a href=&#34;https://en.wikipedia.org/wiki/Logit&#34;&gt;logit&lt;/a&gt; space, these two cumulative probabilities can be represented by a linear model’s output &lt;span class=&#34;math inline&#34;&gt;\(\phi_i\)&lt;/span&gt; &lt;em&gt;relative&lt;/em&gt; to a set of &lt;span class=&#34;math inline&#34;&gt;\(k = 2\)&lt;/span&gt; “cutpoints” &lt;span class=&#34;math inline&#34;&gt;\(\kappa_k\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
R_i \sim \text{Categorical}(p_i) \\
p_i = \langle p_{\text{detractor}[i]}, \ p_{\text{passive}[i]}, \ p_{\text{promoter}[i]} \rangle \\
p_{\text{detractor}[i]} = q_{1, i} \\
p_{\text{passive}[i]} = q_{2,i} - q_{1,i} \\
p_{\text{promoter}[i]} = 1 - q_{2,i} \\
\color{blue}{\text{logit}(q_{k, i}) = \kappa_k - \phi_i \\
\phi_i = \text{some linear model}}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is all a bit involved but I wouldn’t worry about the details too much. The important takeaway is that we now have a linear model &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; that maps to a categorical outcome while preserving the order of the categories.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulating data&lt;/h2&gt;
&lt;p&gt;In this case, let’s let &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; vary by the patient’s age and the hospital unit they visit:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
R_i \sim \text{Categorical}(p_i) \\
p_i = \langle p_{\text{detractor}[i]}, \ p_{\text{passive}[i]}, \ p_{\text{promoter}[i]} \rangle \\
p_{\text{detractor}[i]} = q_{1, i} \\
p_{\text{passive}[i]} = q_{2,i} - q_{1,i} \\
p_{\text{promoter}[i]} = 1 - q_{2,i} \\
\text{logit}(q_{k, i}) = \kappa_k - \phi_i \\
\phi_i = \color{blue}{\beta_{\text{unit}[i]} + \beta_{\text{age}} \ \text{age}_i}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We’ll manually fix the &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\text{unit}}\)&lt;/span&gt; term for each unit. Additionally, let’s define a sampling weight so that the simulated data ends up with a wide range of response counts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unit_params &amp;lt;-
  tribble(
    ~unit, ~beta, ~weight,
    &amp;quot;A&amp;quot;, 1.00, 2,
    &amp;quot;B&amp;quot;, 0.66, 3,
    &amp;quot;C&amp;quot;, 0.33, 2,
    &amp;quot;D&amp;quot;, 0.00, 4,
    &amp;quot;E&amp;quot;, -0.50, 1
  )

unit_params %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;unit&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;beta&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.66&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here, unit A is likely to have the best scores, while unit E is likely to have the worst. Unit D is likely to have the most returns, while unit E is likely to have the fewest. The randomization/discretization means that the simulated scores won’t match the expected scores exactly, but if we set cutpoints in the logit space, we can work backwards through the data-generating process to see the expected score for each unit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set cutpoints in the logit space
cutpoints &amp;lt;- c(-0.5, -0.15)

# here&amp;#39;s how we expect the units to score for an average aged patient
unit_params %&amp;gt;%
  mutate(q1 = cutpoints[1] - beta,
         q2 = cutpoints[2] - beta,
         detractor = expit(q1),
         passive = expit(q2) - expit(q1),
         promoter = 1 - expit(q2)) %&amp;gt;%
  select(unit, weight, promoter, passive, detractor) %&amp;gt;%
  mutate(nps = promoter - detractor,
         across(c(promoter:nps), ~scales::label_percent(accuracy = 1)(.x))) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;unit&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;weight&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;promoter&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;passive&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;detractor&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;nps&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;76%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;6%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;18%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;58%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;69%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;7%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;24%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;45%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;62%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;30%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;31%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;54%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;9%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;38%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;16%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;41%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;9%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;50%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-9%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now let’s simulate patient visits. We’ll have 500 patients return surveys and the number of returns at each unit will be proportional to the &lt;code&gt;weight&lt;/code&gt; set earlier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_patients &amp;lt;- 500

# simulate patient visits
set.seed(30)
unit_samples &amp;lt;-
  sample(
    unit_params$unit,
    size = n_patients,
    prob = unit_params$weight,
    replace = TRUE
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(unit = unit_samples) %&amp;gt;%
  percent(unit, .keep_n = TRUE) %&amp;gt;%
  mutate(pct = scales::label_percent(accuracy = 1)(pct)) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;unit&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;pct&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;69&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;14%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;111&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;22%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;78&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;16%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;195&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;39%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;47&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;9%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now let’s add in each patient’s age. In this case, we won’t include any relationship between age and unit; age will just vary randomly across all units. In reality, this often isn’t the case — you can imagine, for example, that patients visiting a Labor &amp;amp; Delivery unit will tend to be younger than patients visiting a Geriatric unit! Ignoring this reality, in our simulated patient population the ages will vary generally between 25 and 65.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# simulate ages of patients &amp;amp; combine with the unit visited
set.seed(31)
patients &amp;lt;-
  tibble(
    unit = unit_samples,
    age = round(rnorm(n_patients, 45, 10))
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;patients %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;unit&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;age&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;46&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;61&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;55&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;41&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;49&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;54&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;38&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Finally, we’ll set &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\text{age}}\)&lt;/span&gt; such that there is a modest positive relationship between age and the probability of a positive response — older patients at any unit will be likelier than younger patients to be a promoter!&lt;/p&gt;
&lt;p&gt;With all that wrapped up, we can finally simulate individual responses.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beta_age &amp;lt;- 0.35

# simulate individual patient responses 
set.seed(32)
responses &amp;lt;- 
  patients %&amp;gt;%
  left_join(unit_params) %&amp;gt;%
  mutate(phi = beta + beta_age * ((age - 45)/10),
         q1 = cutpoints[1] - phi,
         q2 = cutpoints[2] - phi,
         detractor = expit(q1),
         passive = expit(q2) - expit(q1),
         promoter = 1 - expit(q2)) %&amp;gt;%
  select(unit, age, promoter, passive, detractor) %&amp;gt;%
  rowwise() %&amp;gt;%
  mutate(response = sample(c(&amp;quot;promoter&amp;quot;, &amp;quot;passive&amp;quot;, &amp;quot;detractor&amp;quot;),
                           size = 1, 
                           prob = c(promoter, passive, detractor))) %&amp;gt;%
  ungroup() %&amp;gt;%
  select(unit, age, response)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the distribution of responses for each unit — as expected, unit A has lots of promoters while units D and E have the highest proportion of detractors, and unit D has the most responses while unit E has the fewest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;egypt_blu &amp;lt;- MetBrewer::MetPalettes$Egypt[[1]][2]
egypt_red &amp;lt;- MetBrewer::MetPalettes$Egypt[[1]][1]
egypt_grn &amp;lt;- MetBrewer::MetPalettes$Egypt[[1]][3]

responses %&amp;gt;%
  mutate(response = fct_relevel(response, c(&amp;quot;detractor&amp;quot;, &amp;quot;promoter&amp;quot;, &amp;quot;passive&amp;quot;))) %&amp;gt;%
  ggplot(aes(x = age,
             fill = response)) + 
  geom_histogram(position = &amp;quot;identity&amp;quot;,
                 alpha = 0.5) + 
  facet_wrap(~unit, scales = &amp;quot;free_y&amp;quot;) +
  MetBrewer::scale_fill_met_d(&amp;quot;Egypt&amp;quot;) +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  labs(title = glue::glue(&amp;quot;Simulated **{color_text(&amp;#39;promoters&amp;#39;, egypt_blu)}**, 
                          **{color_text(&amp;#39;passives&amp;#39;, egypt_grn)}**, and 
                          **{color_text(&amp;#39;detractors&amp;#39;, egypt_red)}**&amp;quot;),
       subtitle = &amp;quot;Distribution of patient responses by age at each unit&amp;quot;,
       x = &amp;quot;Patient age&amp;quot;,
       y = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/index_files/figure-html/plot-response-distribution-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s how each simulated unit scored for NPS:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;responses %&amp;gt;%
  group_by(unit) %&amp;gt;%
  count(response) %&amp;gt;%
  pivot_wider(names_from = response,
              values_from = n,
              values_fill = 0) %&amp;gt;%
  mutate(n = promoter + passive + detractor,
         nps = (promoter - detractor)/n,
         nps = scales::label_percent(accuracy = 1)(nps)) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;unit&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;detractor&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;passive&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;promoter&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;nps&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;59&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;69&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;74%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;27&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;78&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;111&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;46%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;46&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;78&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;27%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;68&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;106&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;195&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;19%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;24&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;47&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Importantly, this differs from the expected outcome at each unit! For smaller sample sizes, each individual patient response has an outsized impact on NPS. Despite this, we should be able to recover the underlying parameters used to simulate the data with a model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model&lt;/h2&gt;
&lt;p&gt;Remember the lengthy data-generating process nonsense from beforehand? As is, that’d be a bit of a mess to implement by hand. Luckily for us, however, McElreath’s &lt;a href=&#34;https://github.com/rmcelreath/rethinking&#34;&gt;{&lt;code&gt;rethinking&lt;/code&gt;}&lt;/a&gt; package contains a useful function, &lt;code&gt;dordlogit()&lt;/code&gt;, that interfaces nicely with Stan’s &lt;a href=&#34;https://mc-stan.org/docs/stan-users-guide/ordered-logistic.html&#34;&gt;ordered logistic model&lt;/a&gt;. This plunks some of the rote computational steps under the hood and leaves us with the most important bits: the linear model &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; and the cutpoints &lt;span class=&#34;math inline&#34;&gt;\(\kappa\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
R_i \sim \text{Ordered-logit}(\phi_i, \kappa_k) \\
\phi_i = \text{some linear model} \\
\text{~priors~}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s build a series of increasingly complex models using this framework. I’m in a mood for raccoons, so the models are named appropriately:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;raccoon_01&lt;/code&gt;: a term-less model that just estimates the cutpoints.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;raccoon_02&lt;/code&gt;: a model with terms for age and unit.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;raccoon_03&lt;/code&gt;: a model with a term for age and a hierarchical term for unit.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;raccoon_04&lt;/code&gt;: a model with a term for age and a non-centered hierarchical term for unit.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Before doing any of that, however, we’ll need to prep the data for Stan. Each unit and response category will get assigned a numeric ID and we’ll standardize patient ages across the population.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;responses &amp;lt;- 
  responses %&amp;gt;%
  left_join(tibble(unit = LETTERS[1:5],
                   unit_id = seq(1:5))) %&amp;gt;%
  mutate(response_id = case_when(response == &amp;quot;promoter&amp;quot; ~ 3,
                                 response == &amp;quot;passive&amp;quot; ~ 2,
                                 response == &amp;quot;detractor&amp;quot; ~ 1),
         response_id = as.integer(response_id),
         age_std = (age - mean(age))/sd(age))

responses_stan &amp;lt;- 
  responses %&amp;gt;%
  select(response_id,
         unit_id,
         age_std) %&amp;gt;%
  as.list()&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;raccoon-01&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Raccoon #01&lt;/h3&gt;
&lt;p&gt;The first model doesn’t contain any terms and just estimates the cutpoints from the data. In McElreath’s words, this sort of model is little more than a Bayesian histogram of the data. To get started, we just need to provide a prior for the cutpoints &lt;span class=&#34;math inline&#34;&gt;\(\kappa_k\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
R_i \sim \text{Ordered-logit}(\phi_i, \kappa_k) \\
\color{blue}{\phi_i = 0 \\
\kappa_k \sim \text{Normal}(0, 1)}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Modeling in Stan via &lt;code&gt;rethinking::ulam()&lt;/code&gt; is essentially as basic as re-writing the mathematical model and supplying the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;raccoon_01 &amp;lt;-
  ulam(
    alist(
      # model
      response_id ~ dordlogit(0, cutpoints),
      
      # priors
      cutpoints ~ dnorm(0, 1)
    ),
    
    data = responses_stan,
    chains = 4,
    cores = 4
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This initial model doesn’t do a good job of recovering the manually set cutpoints:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;precis(raccoon_01, depth = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    mean         sd       5.5%      94.5%    n_eff    Rhat4
## cutpoints[1] -0.8350663 0.09496125 -0.9825748 -0.6847435 1078.130 1.002124
## cutpoints[2] -0.5054729 0.08830633 -0.6494612 -0.3641978 1291.444 1.001888&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is expected! This model doesn’t account for the variation by unit/age and instead lumps all the data together. As mentioned before, this really can be thought of as a Bayesian histogram — while it doesn’t recover the cutpoint parameters, &lt;code&gt;raccoon_01&lt;/code&gt; matches the overall proportion of promoters, passives, and detractors really well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;raccoon_01@stanfit %&amp;gt;%
  
  # extract posterior draws &amp;amp; convert to tibble
  posterior::as_draws_df() %&amp;gt;%
  as_tibble() %&amp;gt;%
  select(starts_with(&amp;quot;cut&amp;quot;)) %&amp;gt;%
  rename_with(~str_remove_all(.x, &amp;quot;[:punct:]&amp;quot;)) %&amp;gt;%
  
  # summarise each category w/50/80% quantiles
  mutate(detractor = expit(cutpoints1),
         passive = expit(cutpoints2) - expit(cutpoints1),
         promoter = 1 - expit(cutpoints2)) %&amp;gt;%
  select(-starts_with(&amp;quot;cut&amp;quot;)) %&amp;gt;%
  pivot_longer(cols = everything(),
               names_to = &amp;quot;nps&amp;quot;,
               values_to = &amp;quot;estimate&amp;quot;) %&amp;gt;%
  group_by(nps) %&amp;gt;%
  tidybayes::median_qi(estimate, .width = c(0.5, 0.8)) %&amp;gt;%
  
  # plot alongside original data
  ggplot() + 
  geom_col(data = percent(responses, response),
           aes(x = response,
               y = pct),
           fill = egypt_red,
           alpha = 0.5,
           width = 0.5) +
  ggdist::geom_pointinterval(aes(x = nps,
                                 y = estimate,
                                 ymin = .lower,
                                 ymax = .upper),
                             color = egypt_blu) +
  scale_y_continuous(labels = scales::label_percent()) + 
  labs(title = &amp;quot;**Raccoon #01** Posterior Fit&amp;quot;,
       subtitle = glue::glue(&amp;quot;Comparison of each category&amp;#39;s 
                             **{color_text(&amp;#39;true proportion&amp;#39;, egypt_red)}** 
                             to the 
                             **{color_text(&amp;#39;model\\&amp;#39;s estimate&amp;#39;, egypt_blu)}**&amp;quot;),
       x = NULL,
       y = NULL,
       caption = &amp;quot;Pointrange indicates 50/80% &amp;lt;br&amp;gt;posterior credible interval&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/index_files/figure-html/raccoon-01-post-fit-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This model isn’t terribly useful since we could have gotten the same inference from just plotting the data directly, but this serves as a base upon which we can build more complicated and useful models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;raccoon-02&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Raccoon #02&lt;/h3&gt;
&lt;p&gt;The second model is where things get a bit more interesting — now we’ll actually include predictors for &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\text{unit}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\text{age}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
R_i \sim \text{Ordered-logit}(\phi_i, \kappa_k) \\
\phi_i = \color{blue}{\beta_{\text{unit}} + \beta_{\text{age}} \ \text{age}_i} \\
\kappa_k \sim \text{Normal}(0, 1) \\
\color{blue}{\beta_{\text{unit}} \sim \text{Normal}(0, 1) \\
\beta_{\text{age}} \sim \text{Normal}(0, 0.5)}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I’ve upped the number of samples for this particular model to avoid a warning from Stan.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;raccoon_02 &amp;lt;- 
  ulam(
    alist(
      # model
      response_id ~ dordlogit(phi, cutpoints),
      phi &amp;lt;- b[unit_id] + b_age * age_std,
      
      # priors
      cutpoints ~ dnorm(0, 1), 
      b[unit_id] ~ dnorm(0, 1),
      b_age ~ dnorm(0, 0.5)
    ),
    
    data = responses_stan,
    chains = 4,
    cores = 4,
    iter = 2000
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model does a pretty good job! Extracting the parameter estimates shows that all of the parameter values that we set manually fall within the 80% posterior credible range estimated by the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;raccoon_02@stanfit %&amp;gt;%
  
  # extract parameter draws &amp;amp; summarise with 50/80% quantiles
  posterior::as_draws_df() %&amp;gt;%
  as_tibble() %&amp;gt;%
  select(-c(lp__:.draw)) %&amp;gt;%
  pivot_longer(cols = everything(),
               names_to = &amp;quot;term&amp;quot;,
               values_to = &amp;quot;estimate&amp;quot;) %&amp;gt;%
  mutate(term = if_else(str_sub(term, 1, 2) == &amp;quot;b[&amp;quot;, 
                        LETTERS[as.integer(str_sub(term, 3, 3))], 
                        term)) %&amp;gt;%
  group_by(term) %&amp;gt;%
  tidybayes::median_qi(estimate, .width = c(0.5, 0.8)) %&amp;gt;%
  
  # append with actual values used to simulate data
  left_join(unit_params, by = c(&amp;quot;term&amp;quot; = &amp;quot;unit&amp;quot;)) %&amp;gt;%
  rename(true_value = beta) %&amp;gt;%
  mutate(true_value = case_when(term == &amp;quot;cutpoints[1]&amp;quot; ~ cutpoints[1],
                                term == &amp;quot;cutpoints[2]&amp;quot; ~ cutpoints[2],
                                term == &amp;quot;b_age&amp;quot; ~ beta_age,
                                TRUE ~ true_value),
         term = fct_relevel(term, c(paste0(&amp;quot;cutpoints[&amp;quot;, 1:2, &amp;quot;]&amp;quot;),
                                    &amp;quot;b_age&amp;quot;,
                                    LETTERS[5:1]))) %&amp;gt;%
  
  # plot!
  ggplot(aes(x = term,
             y = estimate,
             ymin = .lower,
             ymax = .upper)) + 
  ggdist::geom_pointinterval(color = egypt_blu) +
  geom_point(aes(y = true_value),
             color = egypt_red,
             size = 2.5) + 
  coord_flip() + 
  labs(title = &amp;quot;**Raccoon #02** Posterior Fit&amp;quot;,
       subtitle = glue::glue(&amp;quot;Comparison of each parameter&amp;#39;s 
                             **{color_text(&amp;#39;true value&amp;#39;, egypt_red)}** 
                             to the 
                             **{color_text(&amp;#39;model\\&amp;#39;s estimate&amp;#39;, egypt_blu)}**&amp;quot;),
       x = NULL,
       y = NULL,
       caption = &amp;quot;Pointrange indicates 50/80% &amp;lt;br&amp;gt;posterior credible interval&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/index_files/figure-html/raccoon-02-params-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This model, however, could be improved. The model only uses categorical indicators for the unit, which causes two issues. Firstly, we can only make predictions for the few units that are in the dataset — this model would fail if we tried to make a prediction on a hypothetical new unit, unit F. Secondly, information about each unit is contained just to that unit. In this case, unit E has relatively few responses, and therefore can only draw inference from those responses. A &lt;a href=&#34;https://www.thedatadiary.net/blog/2022-11-14-hierarchical-hospitals/&#34;&gt;hierarchical model&lt;/a&gt;, however, can help in both these areas.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;raccoon-03&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Raccoon #03&lt;/h3&gt;
&lt;p&gt;To add a hierarchical term for the unit-level intercept, &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\text{unit}}\)&lt;/span&gt;, we don’t actually need to make any changes to the linear model, just how &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\text{unit}}\)&lt;/span&gt; is defined underneath. Rather than estimating each unit intercept directly, this new model will allow them to vary around a group mean, &lt;span class=&#34;math inline&#34;&gt;\(\overline{\beta}\)&lt;/span&gt; with a standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
R_i \sim \text{Ordered-logit}(\phi_i, \kappa_k) \\
\phi_i = \beta_{\text{unit}} + \beta_{\text{age}} \ \text{age}_i \\
\kappa_k \sim \text{Normal}(0, 1) \\
\beta_{\text{unit}} \sim \text{Normal}(\color{blue}{\overline{\beta}}, \color{blue}{\sigma}) \\
\beta_{\text{age}} \sim \text{Normal}(0, 0.5) \\
\color{blue}{\overline{\beta} \sim \text{Normal}(0, 1) \\
\sigma \sim \text{Exponential}(1)}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This new definition means that we no longer set priors for &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\text{unit}}\)&lt;/span&gt; directly. Instead, our new terms are considered &lt;em&gt;hyper-priors&lt;/em&gt; or &lt;em&gt;adaptive priors&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;raccoon_03 &amp;lt;-
  ulam(
    alist(
      # model
      response_id ~ dordlogit(phi, cutpoints),
      phi &amp;lt;- b[unit_id] + b_age*age_std,
      
      # priors
      cutpoints ~ dnorm(0, 1),
      b[unit_id] ~ dnorm(b_bar, sigma),
      b_age ~ dnorm(0, 0.5),
      
      # hyper-priors
      b_bar ~ dnorm(0, 1),
      sigma ~ dexp(1)
    ),
    
    data = responses_stan,
    chains = 4,
    cores = 4
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similar to the previous model, all the true parameter values fall within the model’s 80% credible interval estimates. &lt;em&gt;And&lt;/em&gt;, we’re now accounting for the group structure with a hierarchical model! If you look at Unit E, however, it looks like the model has gotten worse — the median parameter estimate here is further away from the true value than in the previous model. This, however, is actually what we want. Because there are so few responses for unit E, the estimates are shrunken towards the group mean. In the previous model, we were a bit too &lt;em&gt;over-indexed&lt;/em&gt; on the responses we had — if this were real data, where we don’t inherently know the underlying parameter value, we’d want to be similarly cautious for a unit with few responses.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;raccoon_03@stanfit %&amp;gt;%
  
  # extract posterior parameters and summarise with 50/80% quantiles
  posterior::as_draws_df() %&amp;gt;%
  as_tibble() %&amp;gt;%
  rename_with(~str_remove(.x, &amp;quot;]&amp;quot;)) %&amp;gt;%
  rename_with(~str_replace(.x, &amp;quot;\\[&amp;quot;, &amp;quot;_&amp;quot;)) %&amp;gt;%
  mutate(A = b_1,
         B = b_2,
         C = b_3,
         D = b_4,
         E = b_5) %&amp;gt;%
  select(.draw, A, B, C, D, E, starts_with(&amp;quot;cut&amp;quot;), b_age) %&amp;gt;%
  pivot_longer(cols = -.draw,
               names_to = &amp;quot;term&amp;quot;,
               values_to = &amp;quot;estimate&amp;quot;) %&amp;gt;%
  group_by(term) %&amp;gt;%
  tidybayes::median_qi(estimate, .width = c(0.5, 0.8)) %&amp;gt;%
  
  # append with actual values used to simulate data
  mutate(term = if_else(str_detect(term, &amp;quot;cutpoint&amp;quot;), paste0(str_replace(term, &amp;quot;_&amp;quot;, &amp;quot;\\[&amp;quot;), &amp;quot;]&amp;quot;), term)) %&amp;gt;%
  left_join(unit_params, by = c(&amp;quot;term&amp;quot; = &amp;quot;unit&amp;quot;)) %&amp;gt;%
  rename(true_value = beta) %&amp;gt;%
  mutate(true_value = case_when(term == &amp;quot;cutpoints[1]&amp;quot; ~ cutpoints[1],
                                term == &amp;quot;cutpoints[2]&amp;quot; ~ cutpoints[2],
                                term == &amp;quot;b_age&amp;quot; ~ beta_age,
                                TRUE ~ true_value),
         term = fct_relevel(term, c(paste0(&amp;quot;cutpoints[&amp;quot;, 1:2, &amp;quot;]&amp;quot;),
                                    &amp;quot;b_age&amp;quot;,
                                    LETTERS[5:1]))) %&amp;gt;%
  
  # plot!
  ggplot(aes(x = term,
             y = estimate,
             ymin = .lower,
             ymax = .upper)) + 
  ggdist::geom_pointinterval(color = egypt_blu) +
  geom_point(aes(y = true_value),
             color = egypt_red,
             size = 2.5) + 
  coord_flip() + 
  labs(title = &amp;quot;**Raccoon #03** Posterior Fit&amp;quot;,
       subtitle = glue::glue(&amp;quot;Comparison of each parameter&amp;#39;s 
                             **{color_text(&amp;#39;true value&amp;#39;, egypt_red)}** 
                             to the 
                             **{color_text(&amp;#39;model\\&amp;#39;s estimate&amp;#39;, egypt_blu)}**&amp;quot;),
       x = NULL,
       y = NULL,
       caption = &amp;quot;Pointrange indicates 50/80% &amp;lt;br&amp;gt;posterior credible interval&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/index_files/figure-html/raccoon-03-post-fit-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Despite all this hierarchical goodness, this model’s diagnostics could stand to be improved. Although Stan didn’t throw any errors, each parameter’s &lt;a href=&#34;https://mc-stan.org/docs/reference-manual/effective-sample-size.html#effective-sample-size.section&#34;&gt;effective sample size&lt;/a&gt;, &lt;code&gt;n_eff&lt;/code&gt;, is low relative to the number of actual samples drawn (in this case, we used the default of 500 samples per chain for a total of 2000 samples) and the &lt;a href=&#34;https://mc-stan.org/docs/reference-manual/notation-for-samples-chains-and-draws.html#potential-scale-reduction&#34;&gt;convergence statistic&lt;/a&gt;, &lt;code&gt;Rhat4&lt;/code&gt;, is often a hair or two above the target value of &lt;code&gt;1.00&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;precis(raccoon_03, depth = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     mean         sd       5.5%     94.5%     n_eff    Rhat4
## cutpoints[1] -0.41376023 0.60190950 -1.4433861 0.5471729  373.8733 1.018584
## cutpoints[2] -0.06246023 0.60423520 -1.0766518 0.8993494  389.2378 1.018645
## b[1]          1.49374559 0.69139567  0.3692997 2.6091664  479.3414 1.014100
## b[2]          0.74360352 0.62753994 -0.2911371 1.7151677  408.1696 1.017661
## b[3]          0.33785619 0.63617237 -0.7061142 1.3145339  385.0501 1.016526
## b[4]          0.20898156 0.61467500 -0.8026615 1.1898645  366.1409 1.019565
## b[5]         -0.06365551 0.66665904 -1.1210397 0.9557759  431.5194 1.017986
## b_age         0.24898479 0.08989783  0.1096759 0.3935037 1137.4021 1.001321
## b_bar         0.48423691 0.65357409 -0.5809470 1.5103410  416.2923 1.011756
## sigma         0.78583383 0.39324244  0.3537180 1.5136274 1023.1633 1.002546&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is not-so-much an issue with the model specification, but with the computation. Stan’s sampler has a bit of difficulty estimating the shape of the posterior for each &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\text{unit}}\)&lt;/span&gt; because they are dependent on &lt;span class=&#34;math inline&#34;&gt;\(\overline{\beta}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, which are estimated separately (&lt;a href=&#34;https://benslack19.github.io/data%20science/statistics/devilsfunnel_cnc_param/&#34;&gt;this post&lt;/a&gt; provides a good visual of the “Devil’s Funnel” — a difficult shape to explore that can arise from this sort of model).&lt;/p&gt;
&lt;p&gt;Once again, I am fortunate to not be the first person to encounter this issue, and there is a relatively standard approach that we can take to address. We can respecify the model using a &lt;a href=&#34;https://mc-stan.org/docs/stan-users-guide/reparameterization.html#non-centered-parameterization&#34;&gt;non-centered parameterization&lt;/a&gt; for the &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\text{unit}}\)&lt;/span&gt; terms.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;raccoon-04&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Raccoon #04&lt;/h3&gt;
&lt;p&gt;A non-centered parameterization is mathematically equivalent to it’s &lt;a href=&#34;https://mc-stan.org/docs/stan-users-guide/reparameterization.html#centered-parameterization&#34;&gt;centered&lt;/a&gt; counterpart (which is what was used in the previous model), but makes it easier for Stan’s sampler to explore the parameter space. To convert to a non-centered parameterization, we need to respecify the model such that each parameter is sampled directly, rather than being dependent on another parameter.&lt;/p&gt;
&lt;p&gt;In our case, we want each unit’s intercept to be offset from the global mean by some amount. We can think of this offset as being &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; standard deviations away from the mean. Because the model is additive, we can simply replace the &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\text{unit}}\)&lt;/span&gt; term in the linear model with the mean &lt;span class=&#34;math inline&#34;&gt;\(\overline{\beta}\)&lt;/span&gt; and unit offset &lt;span class=&#34;math inline&#34;&gt;\(z_{\text{unit}} \ \sigma\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
R_i \sim \text{Ordered-logit}(\phi_i, \kappa_k) \\
\phi_i = \color{blue}{\underbrace{\overline{\beta} + z_{\text{unit}} \ \sigma}_{\beta_{\text{unit}} \ \text{replacement}}} + \beta_{\text{age}} \ \text{age}_i \\
\kappa_k \sim \text{Normal}(0, 1) \\
\beta_{\text{age}} \sim \text{Normal}(0, 0.5) \\
\overline{\beta} \sim \text{Normal}(0, 1) \\
\color{blue}{z_{\text{unit}} \sim \text{Normal}(0, 1) \\
\sigma \sim \text{Exponential}(1)}
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Again, this is mathematically equivalent to the previous model, but the sampler will now complain less about exploring the parameter space since each term is sampled directly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;raccoon_04 &amp;lt;- 
  ulam(
    alist(
      # model 
      response_id ~ dordlogit(phi, cutpoints),
      phi &amp;lt;- b_bar + z[unit_id]*sigma + b_age*age_std,
      
      # priors
      cutpoints ~ dnorm(0, 1),
      b_age ~ dnorm(0, 0.5),
      
      # non-centered parameters
      b_bar ~ dnorm(0, 1),
      z[unit_id] ~ dnorm(0, 1),
      sigma ~ dexp(1)
    ),
    
    data = responses_stan,
    chains = 4,
    cores = 4
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have to do a bit more work to pull out the unit estimates for this model but, as expected, the parameter estimates here are practically equivalent to the previous model’s estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;raccoon_04@stanfit %&amp;gt;%
  
  # extract parameters and summarise with 50/80% quantiles
  posterior::as_draws_df() %&amp;gt;%
  as_tibble() %&amp;gt;%
  rename_with(~str_replace(str_remove(.x, &amp;quot;]&amp;quot;), &amp;quot;\\[&amp;quot;, &amp;quot;_&amp;quot;),
              .cols = starts_with(&amp;quot;z&amp;quot;)) %&amp;gt;%
  mutate(A = b_bar + z_1 * sigma,
         B = b_bar + z_2 * sigma,
         C = b_bar + z_3 * sigma,
         D = b_bar + z_4 * sigma,
         E = b_bar + z_5 * sigma) %&amp;gt;%
  select(A, B, C, D, E, b_age, starts_with(&amp;quot;cut&amp;quot;)) %&amp;gt;%
  pivot_longer(cols = everything(),
               names_to = &amp;quot;term&amp;quot;,
               values_to = &amp;quot;estimate&amp;quot;) %&amp;gt;%
  group_by(term) %&amp;gt;%
  tidybayes::median_qi(estimate, .width = c(0.5, 0.8)) %&amp;gt;%
  
  # append with actual values used to simulate data
  left_join(unit_params, by = c(&amp;quot;term&amp;quot; = &amp;quot;unit&amp;quot;)) %&amp;gt;%
  rename(true_value = beta) %&amp;gt;%
  mutate(true_value = case_when(term == &amp;quot;cutpoints[1]&amp;quot; ~ cutpoints[1],
                                term == &amp;quot;cutpoints[2]&amp;quot; ~ cutpoints[2],
                                term == &amp;quot;b_age&amp;quot; ~ beta_age,
                                TRUE ~ true_value),
         term = fct_relevel(term, c(paste0(&amp;quot;cutpoints[&amp;quot;, 1:2, &amp;quot;]&amp;quot;),
                                    &amp;quot;b_age&amp;quot;,
                                    LETTERS[5:1]))) %&amp;gt;%
  
  # plot!
  ggplot(aes(x = term,
             y = estimate,
             ymin = .lower,
             ymax = .upper)) + 
  ggdist::geom_pointinterval(color = egypt_blu) +
  geom_point(aes(y = true_value),
             color = egypt_red,
             size = 2.5) + 
  coord_flip() +
  labs(title = &amp;quot;**Raccoon #04** Posterior Fit&amp;quot;,
       subtitle = glue::glue(&amp;quot;Comparison of each parameter&amp;#39;s 
                             **{color_text(&amp;#39;true value&amp;#39;, egypt_red)}** 
                             to the 
                             **{color_text(&amp;#39;model\\&amp;#39;s estimate&amp;#39;, egypt_blu)}**&amp;quot;),
       x = NULL,
       y = NULL,
       caption = &amp;quot;Pointrange indicates 50/80% &amp;lt;br&amp;gt;posterior credible interval&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/index_files/figure-html/raccoon-04-post-fit-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With this new parameterization, however, this inference stands on a bit sturdier ground. While still not near the actual 2000 total samples, the effective sample size of each parameter has greatly improved and the convergence statistic is better across the board.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;precis(raccoon_04, depth = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    mean         sd       5.5%     94.5%     n_eff     Rhat4
## cutpoints[1] -0.4528105 0.61734428 -1.4141411 0.5646185 1080.2276 0.9999476
## cutpoints[2] -0.1034459 0.61438529 -1.0705393 0.8963293 1085.7692 0.9997501
## b_age         0.2540631 0.09263271  0.1073939 0.4032082 1969.5320 0.9993506
## b_bar         0.4308891 0.63042797 -0.5693130 1.4471711  925.0096 1.0018934
## z[1]          1.4513916 0.65032581  0.4481160 2.5227073  862.1820 1.0065399
## z[2]          0.3795392 0.54168883 -0.4556955 1.2506734  681.4293 1.0058323
## z[3]         -0.2313011 0.56399174 -1.2076518 0.6440465  619.6718 1.0043170
## z[4]         -0.4386073 0.53846049 -1.3433651 0.3940326  592.6804 1.0029844
## z[5]         -0.7838474 0.62014640 -1.8038760 0.1606955  815.9599 1.0022972
## sigma         0.7803627 0.37862771  0.3673002 1.4990210  683.2843 1.0042488&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-the-posterior&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploring the posterior&lt;/h2&gt;
&lt;p&gt;With this finalized model, we can answer interesting &lt;a href=&#34;https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/multivariate-linear-models.html#plotting-multivariate-posteriors.&#34;&gt;counterfactual&lt;/a&gt; questions even if there isn’t data directly in the dataset. For example, what do we expect each unit’s score to be across &lt;em&gt;all ages?&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sequence of standardized ages for each unit
counterfactual_data &amp;lt;- 
  crossing(unit_id = 1:5,
           age_std = seq(-2, 2, length.out = 50))

# extract a sample of 100 cutpoints from the posterior 
cutpoints &amp;lt;- 
  extract.samples(
    raccoon_04,
    n = 100,
    pars = paste0(&amp;quot;cutpoints[&amp;quot;, 1:2, &amp;quot;]&amp;quot;)
  )

# convert to tibble &amp;amp; add sim index
cutpoints &amp;lt;- 
  tibble(
    sim = seq(1:100),
    cutpoint1 = cutpoints$`cutpoints[1]`,
    cutpoint2 = cutpoints$`cutpoints[2]`
  )
  
counterfactual_output &amp;lt;- 
  
  # extract phi &amp;amp; convert to wide tibble format
  raccoon_04 %&amp;gt;%
  link(as.list(counterfactual_data),
       post = extract.samples(., n = 100)) %&amp;gt;%
  t() %&amp;gt;%
  as_tibble() %&amp;gt;%
  
  # add unit/age data; convert to long format
  bind_cols(counterfactual_data, .) %&amp;gt;%
  rowid_to_column() %&amp;gt;%
  pivot_longer(starts_with(&amp;quot;V&amp;quot;),
               names_to = &amp;quot;sim&amp;quot;,
               values_to = &amp;quot;phi&amp;quot;) %&amp;gt;%
  mutate(sim = as.numeric(str_remove(sim, &amp;quot;V&amp;quot;))) %&amp;gt;%
  
  # estimate probabilities for each category
  left_join(cutpoints) %&amp;gt;%
  mutate(q1 = cutpoint1 - phi,
         q2 = cutpoint2 - phi,
         detractor = expit(q1),
         passive = expit(q2) - expit(q1),
         promoter = 1 - expit(q2)) %&amp;gt;%
  select(unit_id, age_std, sim, promoter, passive, detractor)

counterfactual_output %&amp;gt;%
  slice_head(n = 10) %&amp;gt;%
  mutate(across(c(promoter:detractor), ~scales::label_percent(accuracy = 1)(.x))) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;unit_id&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;age_std&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sim&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;promoter&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;passive&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;detractor&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;76%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;6%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;19%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;78%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;17%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;81%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;14%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;72%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;23%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;80%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;15%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;74%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;6%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;20%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;79%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;16%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;64%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;28%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;79%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;17%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;68%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;24%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here, we’ve taken 100 samples from &lt;code&gt;raccoon_04&lt;/code&gt; for each combination of &lt;code&gt;unit_id&lt;/code&gt; and &lt;code&gt;age_std&lt;/code&gt; and extracted the probability of selecting promoter, passive, or detractor. If we put together in a plot, we can see what the model expects of each unit across each age and how confident hte model is in that expectation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counterfactual_output %&amp;gt;%
  pivot_longer(c(promoter, passive, detractor),
               names_to = &amp;quot;nps_group&amp;quot;,
               values_to = &amp;quot;prob&amp;quot;) %&amp;gt;%
  mutate(nps_group = fct_relevel(nps_group, c(&amp;quot;detractor&amp;quot;, &amp;quot;promoter&amp;quot;, &amp;quot;passive&amp;quot;)),
         unit = paste(&amp;quot;Unit&amp;quot;, LETTERS[unit_id]),
         age = age_std * 10 + 45) %&amp;gt;%
  ggplot(aes(x = age,
             y = prob,
             color = nps_group,
             group = paste0(sim, nps_group))) +
  geom_line(alpha = 0.2) +
  facet_wrap(~unit) +
  scale_y_continuous(labels = scales::label_percent()) + 
  MetBrewer::scale_color_met_d(&amp;quot;Egypt&amp;quot;) +
  theme(legend.position = &amp;quot;none&amp;quot;) + 
  labs(title = &amp;quot;Counterfactual odds and oddities&amp;quot;,
       subtitle = glue::glue(&amp;quot;Probability of selecting 
                             **{color_text(&amp;#39;promoter&amp;#39;, egypt_blu)}**, 
                             **{color_text(&amp;#39;passive&amp;#39;, egypt_grn)}**, or 
                             **{color_text(&amp;#39;detractor&amp;#39;, egypt_red)}** 
                             as age increases&amp;quot;),
       x = NULL,
       y = NULL,
       caption = &amp;quot;Sample of 100 posterior draws&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/index_files/figure-html/plot-counterfactual-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are quite a few items to note from this plot. Firstly, across all age ranges, the early-alphabet units are more likely to have promoter responses than the late-alphabet units. Additionally, there is a relatively small chance of selecting passive at each unit across the ages. As age increases, each unit is expected to receive more favorable scores. Finally, Unit D, which had the most responses, has the tightest posterior intervals while Unit E, which had the fewest, has the widest intervals. All of this is expected, given how the data was simulated. In combination with the true/estimated parameter plot, this serves as a good confirmation that &lt;code&gt;raccoon_04&lt;/code&gt; models the process appropriately.&lt;/p&gt;
&lt;p&gt;Since each sample gives the probability of selecting promoter, passive, or detractor, we can simply plug these probabilities into the formula for NPS to get the &lt;em&gt;expected&lt;/em&gt; NPS for each unit across the ages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counterfactual_output %&amp;gt;%
  
  # get nps for each posterior sample
  mutate(nps = promoter - detractor,
         unit = paste(&amp;quot;Unit&amp;quot;, LETTERS[unit_id]),
         age = age_std * 10 + 45) %&amp;gt;%
  
  # plot!
  ggplot(aes(x = age,
             y = nps,
             group = sim)) + 
  geom_line(alpha = 0.25,
            color = RColorBrewer::brewer.pal(3, &amp;quot;Dark2&amp;quot;)[3]) +
  facet_wrap(~unit) + 
  scale_y_continuous(labels = scales::label_percent()) +
  labs(title = &amp;quot;Counterfactual odds and oddities 2: NPS drift&amp;quot;,
       subtitle = glue::glue(&amp;quot;**{color_text(&amp;#39;Expected NPS&amp;#39;, RColorBrewer::brewer.pal(3, &amp;#39;Dark2&amp;#39;)[3])}** 
                             at each unit as age increases&amp;quot;),
       x = NULL,
       y = NULL,
       caption = &amp;quot;Sample of 100 posterior draws&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-12-30-my-2022-magnum-opus/index_files/figure-html/counterfactual-nps-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;in-summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;In summary…&lt;/h2&gt;
&lt;p&gt;In this post, we’ve done the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Defined a data-generating process that links NPS to a linear model while preserving the order of the categories.&lt;/li&gt;
&lt;li&gt;Manually set the parameters of the linear model and simulated patient responses.&lt;/li&gt;
&lt;li&gt;Recovered the parameters with a series of models.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The models that were built increased in both complexity and utility:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;raccoon_01&lt;/code&gt; didn’t contain any terms and only estimated the cutpoints &lt;span class=&#34;math inline&#34;&gt;\(\kappa_k\)&lt;/span&gt; — this effectively gave us a Bayesian histogram of the data.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;raccoon_02&lt;/code&gt; added terms for the unit each patient visited and their age. This recovered the parameters we set manually, but didn’t pool any information across units and only allowed us to draw inferences from the units in the dataset.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;raccoon_03&lt;/code&gt; converted &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\text{unit}}\)&lt;/span&gt; to a hierarchical term, which addressed some of the shortcomings of &lt;code&gt;raccoon_02&lt;/code&gt;. However, the way the model was written resulted in a difficult parameter space for Stan’s sampler to explore, which gave less-than-desirable diagnostics.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;raccoon_04&lt;/code&gt; was a non-centered reparameterization of &lt;code&gt;raccoon_03&lt;/code&gt;. The two models were mathematically equivalent, but &lt;code&gt;raccoon_04&lt;/code&gt; was easier to for Stan to sample from, which gave us a larger effective sample size and smaller convergence statistic for each parameter (which are both good things!).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With the fit from &lt;code&gt;raccoon_04&lt;/code&gt;, we also were able to look at how the model expected responses to vary with age at each unit. This served as another visual confirmation that the model was doing what we expected based on how the data was simulated. These expected responses also allowed us to plot the expected NPS score at each unit as age varies.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-additional-closing-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some (additional) closing thoughts&lt;/h2&gt;
&lt;p&gt;As mentioned in the opening section, NPS is a difficult metric to model (or, at least, it was prior to picking up McElreath’s book). In the past, I’d used some hack-ish methods to model NPS, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ignoring passives and detractors by modeling promoters with a binomial.&lt;/li&gt;
&lt;li&gt;Aggregating scores at the unit-level, rescaling NPS from &lt;code&gt;[-100, 100]&lt;/code&gt; to &lt;code&gt;[0, 1]&lt;/code&gt;, tossing out any zeroes or ones, then modeling with a beta distribution.&lt;/li&gt;
&lt;li&gt;Separately modeling promoters, passives, and detractors with three binomial models.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each of these is wrong in their own way, but, fundamentally, they all ignore the data-generating process of a patient’s experience influencing an ordered response on a 0-10 scale.&lt;/p&gt;
&lt;p&gt;Speaking of a 0-10 scale, this ordinal model extends to any number of categories — we could have have directly modeled the 11 response categories using &lt;span class=&#34;math inline&#34;&gt;\(k = 10\)&lt;/span&gt; cutpoints. While NPS is more granular (and therefore, in my opinion, better) than LTR/topbox, modeling and evaluating the mean response on the 0-10 scale is &lt;em&gt;even more granular/better&lt;/em&gt; than NPS! That being said, however, it’s unlikely that much of my work at the hospital will incorporate that more-granular view. NPS is our chosen metric, so while there are more potential categories, we really only care about the three big buckets of promoter, passive, or detractor. Additionally, from a benchmarking perspective, NPS is more widely available and allows for a quick comparison to &lt;a href=&#34;https://www.qualtrics.com/news/leading-healthcare-organizations-choose-qualtrics-to-deliver-ease-and-empathy-to-patients-and-caregivers/&#34;&gt;other hospital systems that are picking up the metric&lt;/a&gt; or even &lt;a href=&#34;https://success.qualtrics.com/rs/542-FMF-412/images/Qualtrics_CX_Industry_NPS_Benchmarks_eBook.pdf&#34;&gt;other industries&lt;/a&gt; where NPS is the standard satisfaction metric.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hierarchical Hospitals</title>
      <link>https://www.thedatadiary.net/blog/2022-11-14-hierarchical-hospitals/</link>
      <pubDate>Mon, 14 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.thedatadiary.net/blog/2022-11-14-hierarchical-hospitals/</guid>
      <description>


&lt;div id=&#34;hierarchical-hospitals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical Hospitals&lt;/h2&gt;
&lt;p&gt;If the past year of working at a large hospital system has taught me one thing, it’s that hospitals are a Russian nesting doll of structure. Within the hospital system, there are several campuses. Within each campus, there are several service areas (inpatient, outpatient, emergency, day surgery, etc.). And finally, within each service area at each campus, there can be many individual hospital units.&lt;/p&gt;
&lt;p&gt;Having worked with patient satisfaction data, I know that each of these levels contains useful information that may be beneficial to include in a model. Hospital A, for example, tends to receive better reviews than Hospital B, but within Hospital A the labor &amp;amp; delivery unit tends to receive better reviews than the intensive care unit. Including every single unit as a categorical predictor isn’t a great modeling choice, since information about each unit remains separate (&lt;em&gt;no pooling&lt;/em&gt;). On the other hand, ignoring the nested structure lumps all data points together (&lt;em&gt;complete pooling&lt;/em&gt;), implicitly making the assumption that the data is independent, which can generate misleading predictions!&lt;/p&gt;
&lt;p&gt;This is where hierarchical models come into play! Hierarchical models offer a happy middle ground and allow for &lt;em&gt;partial pooling&lt;/em&gt; of information between groups. This approach allows for information to be shared across groups while still treating each group as unique (this is a pretty simplistic summary of hierarchical models; for a more detailed introduction, see &lt;a href=&#34;https://www.bayesrulesbook.com/chapter-15.html&#34;&gt;Chapter 15 of Bayes Rules!&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;While there are &lt;a href=&#34;https://github.com/lme4/lme4/&#34;&gt;non-Bayesian approaches&lt;/a&gt; to hierarchical models, they mesh well with a Bayesian framework, so in this post I’ll build a Bayesian model to predict satisfaction scores based on simulated hospital data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-hospital-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulating Hospital Data&lt;/h2&gt;
&lt;p&gt;I can’t share live data, so I’ll need to simulate some fake data for this example. Let’s start with five hospitals, each with different baseline levels of satisfaction.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(rstanarm)
library(tidybayes)
library(ggdist)
library(tidytext)

# reproducibility!
set.seed(54321)

# manually assign hospital-level intercept
hospital_prob &amp;lt;- 
  tibble(hospital = paste(&amp;quot;Hospital&amp;quot;, seq(1:5)),
         hospital_prob = seq(from = 0.6, to = 0.8, length.out = 5) ,
         hospital_sigma = rep(0.2, 5))

hospital_prob %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;hospital&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;hospital_prob&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;hospital_sigma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hospital 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.60&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hospital 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.65&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hospital 3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.70&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hospital 4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.75&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hospital 5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In this example, Hospital 1 will tend to have the lowest scores while Hospital 5 will tend to have the highest scores. Within each hospital, we want individual unit-level scores to be able to vary randomly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# simulate 5 hospitals within the system, each with 100 units (500 total)
satisfaction &amp;lt;- 
  tibble(hospital = rep(paste(&amp;quot;Hospital&amp;quot;, seq(1:5)), 100)) %&amp;gt;%
  arrange(hospital) %&amp;gt;%
  
  # add in the units at each hospital
  bind_cols(unit = rep(paste(&amp;quot;Unit&amp;quot;, seq(1:100)), 5)) %&amp;gt;%
  mutate(unit = paste(hospital, unit)) %&amp;gt;%
  
  # add in the hospital-level intercept
  left_join(hospital_prob, by = &amp;quot;hospital&amp;quot;) %&amp;gt;%
  
  # estimate a unit-level intercept 
  rowwise() %&amp;gt;%
  mutate(unit_offset = rnorm(1, 0, 0.05),
         unit_prob = gamlss.dist::rBE(1, hospital_prob + unit_offset, hospital_sigma)) %&amp;gt;%
  select(hospital, unit, unit_prob) %&amp;gt;%
  
  # generate fake responses
  mutate(n = round(rlnorm(1, log(100), 1.5)),
         topbox = rbinom(1, n, unit_prob)) %&amp;gt;%
  ungroup() %&amp;gt;%
  select(-unit_prob)

# display example at each hospital
set.seed(333)
satisfaction %&amp;gt;%
  group_by(hospital) %&amp;gt;%
  slice_sample(n = 1) %&amp;gt;%
  mutate(score = topbox/n) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;hospital&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;unit&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;topbox&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hospital 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Hospital 1 Unit 14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hospital 2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Hospital 2 Unit 41&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;137&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;107&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7810219&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hospital 3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Hospital 3 Unit 55&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;298&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;252&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8456376&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hospital 4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Hospital 4 Unit 66&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5833333&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hospital 5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Hospital 5 Unit 39&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;211&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;169&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8009479&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This unit level variation is important! Even though units within certain hospitals tend to perform worse than units in others, individual units at lower-rated hospitals can still outperform units at highly-rated hospitals! An easier way to see both the hospital-level and unit-level variation is to place all on the same plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;satisfaction %&amp;gt;%
  mutate(score = topbox/n) %&amp;gt;%
  ggplot(aes(x = hospital,
             y = score,
             size = n,
             color = hospital)) + 
  geom_boxplot() + 
  geom_jitter(alpha = 0.25) + 
  coord_flip() +
  scale_color_brewer(palette = &amp;quot;Dark2&amp;quot;) +
  scale_size_continuous(range = c(1, 15)) +
  scale_y_continuous(labels = scales::label_percent()) +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  labs(title = &amp;quot;It&amp;#39;s in the way that you Units&amp;quot;,
       subtitle = &amp;quot;Satisfaction scores vary both **across** and **within** hospitals&amp;quot;,
       x = NULL,
       y = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-11-14-hierarchical-hospitals/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, each point represents an individual unit within a hospital — larger points indicate units with more responses. There’s clearly variation &lt;em&gt;across&lt;/em&gt; hospitals, but also variation &lt;em&gt;within&lt;/em&gt; each hospital. We can generally trust that the satisfaction score for a unit with lots of responses is accurate, but a unit with few responses can provide misleading scores — some have scores of 100%! I don’t think that these units are actually perfect, it’s likelier that they got lucky.&lt;/p&gt;
&lt;p&gt;A hierarchical model will allow us to pool all this information together — when a unit has lots of responses, the model’s estimate of their true score will land pretty close to their raw score. When a unit only has a few responses, however, the model will shrink the estimate of their true score towards the hospital group-level average.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-a-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting a Model&lt;/h2&gt;
&lt;p&gt;I’ve found recently that writing out a model specification helps, so let’s write out the model and priors.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{gather}
y_{unit} \sim Binomial(\pi_{unit}, n_{unit}) \\
logit(\pi_{unit}) = \mu + \beta_{hospital} + \beta_{unit} \\
\mu \sim Normal(0, 2) \\
\beta_{hospital} \sim Normal(0, 2) \\
\beta_{unit} \sim Normal(0, 2)
\end{gather}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this case, the number of topbox responses at each unit, &lt;span class=&#34;math inline&#34;&gt;\(y_{unit}\)&lt;/span&gt;, is estimated with a binomial distribution where each patient within that visits the unit has a probability &lt;span class=&#34;math inline&#34;&gt;\(\pi_{unit}\)&lt;/span&gt; of selecting the topbox response. &lt;span class=&#34;math inline&#34;&gt;\(\pi_{unit}\)&lt;/span&gt; is allowed to vary from the global mean, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, both by hospital (&lt;span class=&#34;math inline&#34;&gt;\(\beta_{hospital}\)&lt;/span&gt;) and by unit (&lt;span class=&#34;math inline&#34;&gt;\(\beta_{unit}\)&lt;/span&gt;). This can be implemented in &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt; via the &lt;a href=&#34;https://mc-stan.org/rstanarm/index.html&#34;&gt;&lt;code&gt;{rstanarm}&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# run chains on separte cores
options(mc.cores = parallel::detectCores())

# fit a bayesian model!
satisfaction_model &amp;lt;- 
  stan_glmer(
    cbind(topbox, n - topbox) ~ (1 | hospital) + (1 | unit),
    data = satisfaction,
    family = binomial(),
    prior_intercept = normal(0, 2, autoscale = TRUE),
    prior = normal(0, 2, autoscale = TRUE),
    prior_covariance = decov(regularization = 1, concentration = 1, shape = 1, scale = 1),
    chains = 4,
    iter = 2000,
    seed = 999
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model gives us &lt;em&gt;exactly what we were looking for&lt;/em&gt; — units with many responses have posterior estimations of &lt;span class=&#34;math inline&#34;&gt;\(\pi_{unit}\)&lt;/span&gt; that are close to the raw score and have relatively small credible intervals, while the posterior estimate of &lt;span class=&#34;math inline&#34;&gt;\(\pi_{unit}\)&lt;/span&gt; for a unit with few responses is shrunken towards the hospital average with relatively wide credible intervals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(88)
satisfaction %&amp;gt;%
  
  # draw 1000 posterior predictions of pi for each unit
  tidybayes::add_epred_draws(satisfaction_model, ndraws = 1000) %&amp;gt;%
  ungroup() %&amp;gt;%
  
  # select a sample of 3 random units from each hospital to plot
  nest(preds = -c(hospital, unit, n, topbox)) %&amp;gt;%
  group_by(hospital) %&amp;gt;%
  slice_sample(n = 3) %&amp;gt;%
  mutate(unit = str_sub(unit, start = 12),
         unit = glue::glue(&amp;quot;{unit}\n(n = {scales::label_comma()(n)})&amp;quot;),
         med_pred = map_dbl(preds, ~quantile(.x$.epred, probs = 0.5))) %&amp;gt;%
  ungroup() %&amp;gt;%
  unnest(preds) %&amp;gt;%
  
  # plot!
  ggplot(aes(x = reorder_within(unit, med_pred, hospital),
             y = .epred,
             color = hospital)) +
  stat_pointinterval() +
  scale_x_reordered() +
  scale_y_continuous(labels = scales::label_percent()) +
  coord_flip() +
  facet_wrap(~hospital, scales = &amp;quot;free&amp;quot;) +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  labs(title = &amp;quot;\u03C0 in the sky&amp;quot;,
       subtitle = &amp;quot;Posterior estimations of \u03C0 for a random sampling of units&amp;quot;, 
       x = NULL,
       y = NULL,
       caption = &amp;quot;Pointrange indicates the&amp;lt;br&amp;gt;66% &amp;amp; 95% posterior credible interval&amp;quot;) +
  scale_color_brewer(palette = &amp;quot;Dark2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-11-14-hierarchical-hospitals/index_files/figure-html/explore%20fit-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With a hierarchical model, we can even make predictions for new units that didn’t appear in the original training data. If we were to introduce a new unit at each hospital, the model can still rely on the hospital-level term to estimate scores.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
tibble(hospital = paste(&amp;quot;Hospital&amp;quot;, seq(1:5))) %&amp;gt;%
  mutate(unit = glue::glue(&amp;quot;{hospital}\nNew Unit&amp;quot;)) %&amp;gt;%
  add_epred_draws(satisfaction_model) %&amp;gt;%
  ggplot(aes(x = unit,
             y = .epred,
             color = unit)) + 
  stat_pointinterval() +
  scale_y_continuous(labels = scales::label_percent()) +
  coord_flip() +
  scale_color_brewer(palette = &amp;quot;Dark2&amp;quot;) +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  labs(title = &amp;quot;New unit, who this?&amp;quot;,
       subtitle = &amp;quot;Posterior estimations of \u03C0 for hypothetical new units at each hospital&amp;quot;,
       x = NULL,
       y = NULL,
       caption = &amp;quot;Pointrange indicates the&amp;lt;br&amp;gt;66% and 95% posterior credible interval&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.thedatadiary.net/blog/2022-11-14-hierarchical-hospitals/index_files/figure-html/predicting%20for%20a%20new%20unit-1.png&#34; width=&#34;4500&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-closing-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some closing thoughts&lt;/h2&gt;
&lt;p&gt;The model I used here can be referred to as a &lt;em&gt;random intercept&lt;/em&gt; model. In this case, the intercept is allowed to vary by hospital and unit. Had I included a predictor term — age, for example — I could have put together a &lt;em&gt;random slope&lt;/em&gt; model, which would have allowed the age term to also vary by hospital and unit. This would allow for one hospital to be modeled as having better scores for young patients while another hospital could see better scores for older patients. This sort of flexibility is useful, but in my experience, simply accounting for the hierarchical structure of the data with an intercept-only model gets you 90% of where you need to go!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
