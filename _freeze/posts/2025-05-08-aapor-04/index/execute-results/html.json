{
  "hash": "eeca52efcecacbc9ab9f8aa818d4c57d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Weighting and Its Consequences\"\ndate: '2025-05-08'\ncategories: [politics]\ndescription: \"Part 4: \"\nimage: header.png\n---\n\n\n\n::: {.callout-note}\nThis is the fourth entry in a multi-part series of posts on weighting surveys. You can read the previous entries at the links below.\n\n* [Part 1: The Abstract](../2025-02-15-aapor-01/index.qmd)\n* [Part 2: A Free Lunch](../2025-04-20-aapor-02/index.qmd)\n* [Part 3: Binary Surprises](../2025-05-04-aapor-03/index.qmd)\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n* what did I do in the previous post\n* what was the cliffhanger (ooh what about forecasters)\n\n* let's simulate a population with four groups of equal size\n* there are two strata with two variables each\n* Groups are highly correlated with the response\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngroups <- read_csv(\"data/groups.csv\")\npollsters <- read_csv(\"data/pollsters.csv\")\npolls <- read_rds(\"data/polls.rds\")\n\ngroups %>%\n  select(strata_1, strata_2, group, group_mean) %>%\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|strata_1 | strata_2|group | group_mean|\n|:--------|--------:|:-----|----------:|\n|A        |        1|A1    |       0.97|\n|A        |        2|A2    |       0.90|\n|B        |        1|B1    |       0.10|\n|B        |        2|B2    |       0.03|\n\n\n:::\n:::\n\n\n\n* let's also simulate a set of pollsters\n* pollsters will have different weighting strategies\n* as well as different (logit scale) biases\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npollsters %>%\n  select(pollster, strategy, bias) %>%\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|pollster    |strategy |       bias|\n|:-----------|:--------|----------:|\n|Pollster 1  |cross    | -0.0280238|\n|Pollster 2  |cross    | -0.0115089|\n|Pollster 3  |cross    |  0.0779354|\n|Pollster 4  |cross    |  0.0035254|\n|Pollster 5  |cross    |  0.0064644|\n|Pollster 6  |cross    |  0.0857532|\n|Pollster 7  |cross    |  0.0230458|\n|Pollster 8  |cross    | -0.0632531|\n|Pollster 9  |cross    | -0.0343426|\n|Pollster 10 |cross    | -0.0222831|\n|Pollster 11 |single   |  0.0612041|\n|Pollster 12 |single   |  0.0179907|\n|Pollster 13 |single   |  0.0200386|\n|Pollster 14 |single   |  0.0055341|\n|Pollster 15 |single   | -0.0277921|\n|Pollster 16 |single   |  0.0893457|\n|Pollster 17 |single   |  0.0248925|\n|Pollster 18 |single   | -0.0983309|\n|Pollster 19 |single   |  0.0350678|\n|Pollster 20 |single   | -0.0236396|\n\n\n:::\n:::\n\n\n\n\n* those with the \"cross\" strategy will weight on all variables, including variables highly correlated with the outcome\n* those with the \"single\" strategy will only weight responses by strata_1\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngroups %>%\n  group_by(strata_2) %>%\n  summarise(strata_mean = mean(group_mean)) %>%\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n| strata_2| strata_mean|\n|--------:|-----------:|\n|        1|       0.535|\n|        2|       0.465|\n\n\n:::\n:::\n\n\n\n* We get data that looks like this\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npolls %>%\n  slice_head(n = 10) %>%\n  transmute(day = day,\n            pollster = pollster,\n            sample_size = map_int(data, ~sum(.x$K)),\n            mean = mean,\n            err = pmap_dbl(list(mean, sd), ~(qnorm(0.975, ..1, ..2) - ..1))) %>%\n  mutate(across(c(mean, err), ~scales::label_percent(accuracy = 0.1)(.x)),\n         err = paste0(\"+/-\", err)) %>%\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n| day|pollster    | sample_size|mean  |err     |\n|---:|:-----------|-----------:|:-----|:-------|\n|   1|Pollster 2  |         941|49.5% |+/-1.6% |\n|   1|Pollster 8  |         987|48.1% |+/-1.5% |\n|   1|Pollster 11 |         863|53.6% |+/-3.3% |\n|   2|Pollster 11 |         847|51.7% |+/-3.4% |\n|   2|Pollster 12 |         949|54.1% |+/-3.1% |\n|   2|Pollster 20 |         948|49.0% |+/-3.2% |\n|   3|Pollster 8  |         973|50.7% |+/-1.7% |\n|   3|Pollster 20 |        1048|48.1% |+/-3.1% |\n|   4|Pollster 10 |         933|49.4% |+/-1.7% |\n|   4|Pollster 19 |        1161|52.3% |+/-2.9% |\n\n\n:::\n:::\n\n\n\n\n* how might we fit a model to this?\n* Reasonable approach is Linzer (2013)\n* We estimate the number of responses in favor as mean * sample size\n* Then model as a binomial\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npolls %>%\n  slice_head(n = 10) %>%\n  transmute(day = day,\n            pollster = pollster,\n            sample_size = map_int(data, ~sum(.x$K)),\n            mean = mean,\n            err = pmap_dbl(list(mean, sd), ~(qnorm(0.975, ..1, ..2) - ..1))) %>%\n  mutate(err = scales::label_percent(accuracy = 0.1)(err),\n         err = paste0(\"+/-\", err),\n         Y = round(mean * sample_size),\n         K = sample_size,\n         mean = scales::label_percent(accuracy = 0.1)(mean)) %>%\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n| day|pollster    | sample_size|mean  |err     |   Y|    K|\n|---:|:-----------|-----------:|:-----|:-------|---:|----:|\n|   1|Pollster 2  |         941|49.5% |+/-1.6% | 466|  941|\n|   1|Pollster 8  |         987|48.1% |+/-1.5% | 475|  987|\n|   1|Pollster 11 |         863|53.6% |+/-3.3% | 462|  863|\n|   2|Pollster 11 |         847|51.7% |+/-3.4% | 438|  847|\n|   2|Pollster 12 |         949|54.1% |+/-3.1% | 514|  949|\n|   2|Pollster 20 |         948|49.0% |+/-3.2% | 465|  948|\n|   3|Pollster 8  |         973|50.7% |+/-1.7% | 493|  973|\n|   3|Pollster 20 |        1048|48.1% |+/-3.1% | 504| 1048|\n|   4|Pollster 10 |         933|49.4% |+/-1.7% | 461|  933|\n|   4|Pollster 19 |        1161|52.3% |+/-2.9% | 608| 1161|\n\n\n:::\n:::\n\n\n\n* Here, we model the latent mean with a timevarying parameter, $\\beta_d$\n* Don't worry about this for now, I'm modeling it using a guassian random walk\n* We also model the pollster biases as hierarchically distributed\n\n$$\n\\begin{align*}\n\\text{Y}_{d,p} &\\sim \\text{Binomial}(\\text{K}_{d,p}, \\theta_{d,p}) \\\\\n\\text{logit}(\\theta_{d,p}) &= \\alpha + \\beta_d + \\beta_p \\\\\n\\beta_p &= \\eta_p \\sigma_\\beta\n\\end{align*}\n$$\n\n* Gives a reasonable fit\n\n![](img/binomial_voteshare.png)\n\n* And a reasonable recovery of the parameter estimates\n\n![](img/binomial_parameters.png)\n\n* But this approach throws away important information\n* What if we instead model the variance directly *given* each poll's reported margin of error?\n* Here, we redefine $Y$ to be the observed mean of the poll and $\\sigma$ to be the observed sd\n* We can then rewrite the likelihood statement to be\n\n$$\n\\text{Y}_{d,g} \\sim \\text{Normal}(\\theta_{d,g}, \\sigma_{d,g})\n$$\n\n* We still get a good looking fit\n\n![](img/normal_voteshare.png)\n\n* But now our parameter estimates are more precise *among those pollsters who weight on highly correlated variables*\n\n![](img/normal_parameters.png)\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}