{
  "hash": "963c624f0ba1bebf9bbaff5519110f26",
  "result": {
    "markdown": "---\ntitle: Gauss' Goal Generation\ndate: '2023-03-19'\ncategories: [bayes, stan, healthcare, rstats]\ndescription: 'Using gaussian process models for hospital goal setting'\nimage: featured.png\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(riekelib)\n```\n:::\n\n\n\nThis past week, I've been living and breathing (and dying, a little bit) with *goals*. [Memorial Hermann's](https://www.memorialhermann.org/) mission is to provide safe, personalized, efficient, and caring healthcare and part of this endeavor is striving to improve each patient's experience (look ma, I paid attention during corporate culture day). As such, we set goals for improving metrics measuring patient satisfaction. One of my responsibilities as a consumer experience (CX) analyst is to model our [net promoter score](https://en.wikipedia.org/wiki/Net_promoter_score) (NPS) so that the goals we suggest are ambitious yet achievable. In preparation for the new fiscal year in July, I've been making updates to the goal model. I recently reached a mid-model stopping point, so now is a good time to take a mental break, recollect my thoughts, and give a peek at what I've got cooking under the hood of the model.\n\n## Foundational features: the current model\n\nThe model for this current year is serviceable, but fairly basic. For each unit, we estimate a distribution of possible outcomes based on the campus, service area (Inpatient, Outpatient, Emergency, etc.), and previous year's score. From this distribution, we're able to set three different goal levels (threshold, target, and distinguished), which can then be rolled up into summary scores at various levels (campus & service area, system & service area, system overall, etc.). This certainly works and provides a more *data-informed* methodology for goal setting than simply asserting that every location needs to improve by X% to meet goals (areas with small sample sizes can trivially crush or be crushed by goals due to expected sampling variation!), but there are ways we can improve.\n\n## Prototyping predictions: expanding the model\n\nThe (in progress) updated model builds on the foundation set by the last model --- we're still estimating a distribution of possible scores at three levels, then rolling up to summary levels. The big addition to the new model is a time series component measuring NPS at each quarter, which is implemented via a [gaussian process](https://en.wikipedia.org/wiki/Gaussian_process). There are other ways to model a time series, but I've elected to use a gaussian process for a couple reasons:\n\n* Gaussian processes allow for scores to be correlated over time. This means that scores in each quarter are likelier to be similar to the previous quarter's score rather than a score from five quarters ago. \n* Gaussian processes are stable over time (at least with the kernel I used). This is something I've seen in the context of hospital patient satisfaction scores (in the very least, the changes over time fall within an uncertainty interval roughly indistinguishable from stable). \n* Gaussian processes allow us to apply prior information to events in the future that we don't yet have data for (see [Drew Linzer's](https://civiqs.com/about) [2013 paper](https://votamatic.org/wp-content/uploads/2013/07/Linzer-JASA13.pdf) describing his election model for an example). This is important as [Memorial Hermann transitions to Epic](https://www.beckershospitalreview.com/ehrs/4-health-systems-switching-from-oracle-cerner-to-epic-ehrs.html) next year, which we expect to have a slight negative impact on patient experience.\n* You are what you read --- I read a lot of [Gelman](http://www.stat.columbia.edu/~gelman/), who is a [fan of gaussian processes](https://statmodeling.stat.columbia.edu/2012/06/14/cool-ass-signal-processing-using-gaussian-processes/), so I suppose some of that has rubbed off on me.\n* They just seem *rad as hell.*\n\nI'll get into all the fun, math-y details below, but it's worth highlighting that the model I describe did not emerge fully formed. Following Gelman's idea of a [Bayeian workflow](https://arxiv.org/pdf/2011.01808.pdf), this model was built iteratively and there were a lot (like, really, *a lot*) of failed models that came before this one. Furthermore, there's still over a quarter of data to collect before the end of the fiscal year and a few un-implemented features, so this definitely isn't the final version.\n\n## Math for days: model specification\n\nFor each hospital's service area, $i$, the number of promoter, passive, and detractor responses in a quarter, $j$, can be described with a multinomial:\n\n$$\n\\begin{align*}\nR_{ij} & \\sim \\text{Multinomial}(n_{ij}, p_{ij})\n\\end{align*}\n$$\n\n$p_{ij}$ is a probability vector with individual probabilities for each of the possible categories the patients can select:\n\n$$\n\\begin{align*}\np_{ij} & = \\langle p_{\\text{promoter}},\\ p_{\\text{passive}},\\ p_{\\text{detractor}} \\rangle_{ij}\n\\end{align*}\n$$\n\nThe categories are ordered (promoter > passive > detractor), so we can link the categorical probabilities to a single linear model, $\\phi$, via the cumulative probabilities, $q$, and $k$ cutpoints, $\\kappa$. I've described this in a bit more detail [in a previous post](https://www.thedatadiary.net/posts/2022-12-30-my-2022-magnum-opus/), so I'll just leave you with the math, here. Just note that as $\\phi$ increases, so too does the expected NPS. \n\n$$\n\\begin{align*}\np_{\\text{detractor},\\ ij} & = q_{1,\\ ij} \\\\\np_{\\text{passive},\\ ij} & = q_{2,\\ ij} - q_{1,\\ ij} \\\\\np_{\\text{promoter},\\ ij} & = 1 - q_{2,\\ ij} \\\\\n\\text{logit}(q_{kij}) & = \\kappa_k - \\phi_{ij} \\\\\n\\phi_{ij} & = \\text{some linear model}\n\\end{align*}\n$$\n\nThis is where things get interesting! Let's add some terms to the linear model. I include hierarchical terms for the campus and service area as well as an interaction between the two (the interaction allows for additional variation beyond what the campus and service terms alone would allow). These terms don't change over time and deviations from the set value over time are handled via a gaussian process parameter, $\\beta_{ij}$.\n\n$$\n\\begin{align*}\n\\phi_{ij} & = \\alpha + z_{\\text{campus}[i]}\\ \\sigma_{\\text{campus}} + \n              z_{\\text{service}[i]}\\ \\sigma_{\\text{service}} + \n              z_{\\text{interact}[i]}\\ \\sigma_{\\text{interact}} +\n              \\beta_{ij}\n\\end{align*}\n$$\n\n$\\beta_{ij}$ could be specified using a [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution), but gaussian processes are a bit finnicky, so we'll use a non-centered parameterization. \n\n$$\n\\begin{align*}\n\\beta_{ij} & = \\eta_{ij}\\ L\n\\end{align*}\n$$\n\nHere, $\\eta_{ij}$ is an offset parameter for each hospital's service area $i$ at each quarter $j$ and $L$ is the [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition) of the [covariance matrix](https://en.wikipedia.org/wiki/Covariance_matrix), which defines how similar scores are over time. \n\n$$\n\\begin{align*}\nL & = \\text{cholesky decompose}(\\Sigma)\n\\end{align*}\n$$\n\nThe covariance between each pair of quarters is defined using an [exponentiated quadratic kernel](https://mc-stan.org/docs/functions-reference/gaussian-process-covariance-functions.html#exponentiated-quadratic-kernel), which is based on two tuning parameters, $\\sigma_{\\text{amplitude}}^2$ and $\\sigma_{\\text{length}}^2$, and the distance in time, $t$, between the quarters. $\\sigma_{\\text{amplitude}}^2$ controls the range in which $\\beta_{ij}$ might end up falling while $\\sigma_{\\text{length}}^2$ controls the time period over which scores can covary (a lengthy timescale implies that scores are slow to change while a short timescale implies that scores can change quickly). For computational efficiency, I scale the $t$ to be between `0` and `1`. Notably, the parameters controlling the covariance matrix do not vary by campus or service. This is intentional! This implies that we think that satisfaction tends to change at similar rates (though scores can fluctuate more quickly due to sampling variation). \n\n$$\n\\begin{align*}\n\\Sigma_{xy} & = \\sigma_{\\text{amplitude}}^2 \\exp \\left( \\frac{|t_x - t_y|^2}{2 \\ \\sigma_{\\text{length}}^2}\\right)\n\\end{align*}\n$$\n\nAltogether, with priors, here's the full model in all it's wonky mathematical glory:\n\n$$\n\\begin{align*}\nR_{ij} & \\sim \\text{Multinomial}(n_{ij}, p_{ij}) \\\\\np_{ij} & = \\langle p_{\\text{promoter}},\\ p_{\\text{passive}},\\ p_{\\text{detractor}} \\rangle_{ij} \\\\\np_{\\text{detractor},\\ ij} & = q_{1,\\ ij} \\\\\np_{\\text{passive},\\ ij} & = q_{2,\\ ij} - q_{1,\\ ij} \\\\\np_{\\text{promoter},\\ ij} & = 1 - q_{2,\\ ij} \\\\\n\\text{logit}(q_{kij}) & = \\kappa_k - \\phi_{ij} \\\\\n\\phi_{ij} & = \\alpha + z_{\\text{campus}[i]}\\ \\sigma_{\\text{campus}} + \n              z_{\\text{service}[i]}\\ \\sigma_{\\text{service}} + \n              z_{\\text{interact}[i]}\\ \\sigma_{\\text{interact}} +\n              \\beta_{ij} \\\\\n\\beta_{ij} & = \\eta_{ij}\\ L \\\\\nL & = \\text{cholesky decompose}(\\Sigma) \\\\\n\\Sigma_{xy} & = \\sigma_{\\text{amplitude}}^2 \\exp \\left( \\frac{|t_x - t_y|^2}{2 \\ \\sigma_{\\text{length}}^2}\\right) \\\\\n\\kappa_k & \\sim \\text{Normal}(-1, 0.25) \\\\\n\\alpha & \\sim \\text{Normal}(0, 1) \\\\\nz_{\\text{campus}} & \\sim \\text{Normal}(0, 1) \\\\\nz_{\\text{service}} & \\sim \\text{Normal}(0, 1) \\\\\nz_{\\text{interact}} & \\sim \\text{Normal}(0, 1) \\\\\n\\sigma_{\\text{campus}} & \\sim \\text{Gamma}(2, 3) \\\\\n\\sigma_{\\text{service}} & \\sim \\text{Gamma}(2, 3) \\\\\n\\sigma_{\\text{interact}} & \\sim \\text{Half-Normal}(0, 0.5) \\\\\n\\eta & \\sim \\text{Normal}(0, 1) \\\\\n\\sigma_{\\text{amplitude}}^2 & \\sim \\text{Beta}(2, 25) \\\\\n\\sigma_{\\text{length}}^2 & \\sim \\text{Beta}(2, 10)\n\\end{align*}\n$$\n\nThere's a [folk theorem of statistical computing](https://statmodeling.stat.columbia.edu/2008/05/13/the_folk_theore/), which states that when you have computing problems, you likely have problems with your model. Despite having lots of data from years of patient surveys, I ran into a *lot* of computational problems with default and weak priors. I use a mix of gamma, beta, and half-normal distributions as priors for scale parameters to move some of the prior mass off `0` and restrict the scale range such that unreasonable values are rejected.\n\nThis is a lot funky math, which I love! But it can be pretty opaque to read through. In the next section, I'll simulate some fake data that will (hopefully) help provide some color to the math, then fit a model to the simulated data.\n\n## Hypothetical hospitals: simulating fake data\n\nI can't share real patient data, so instead I'll simulate responses from scratch, which will help us understand the model a bit better. First, let's get the number of quarterly responses at five hospitals' Inpatient, Outpatient, Emergency, and Day Surgery services over the course of two years. This isn't estimated in the model directly (for now!), but needed to estimate NPS. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set n-size helpers to simulate from\nhospitals <- \n  tibble(hospital = paste(\"Hospital\", LETTERS[1:5]),\n         hosp_n = seq(from = 6.5, to = 4.5, length.out = 5)) \n\nservices <- \n  tibble(service = c(\"Inpatient\", \"Outpatient\", \"Emergency\", \"Day Surgery\"),\n         service_n = c(0, 0.5, 0.25, -0.5))\n\n# simulate number of survey returns\nset.seed(30)\nresponses <- \n  crossing(hospitals,\n           services,\n           quarter = seq.Date(from = lubridate::mdy(\"1/1/21\"), \n                              to = lubridate::mdy(\"10/1/22\"),\n                              by = \"quarter\")) %>%\n  mutate(lambda = hosp_n + service_n) %>%\n  select(-ends_with(\"n\")) %>%\n  bind_cols(n = rpois(nrow(.), exp(.$lambda))) %>%\n  select(-lambda)\n\n# set color palette\npal <- MetBrewer::MetPalettes$Egypt[[1]]\ncol_ed <- pal[1]\ncol_ds <- pal[2]\ncol_ip <- pal[3]\ncol_op <- pal[4]\n\nresponses <- \n  responses %>%\n  mutate(color = case_match(service,\n                            \"Emergency\" ~ col_ed,\n                            \"Day Surgery\" ~ col_ds,\n                            \"Inpatient\" ~ col_ip,\n                            \"Outpatient\" ~ col_op))\n\n# plot!\nresponses %>%\n  ggplot(aes(x = quarter,\n             y = n,\n             color = color)) + \n  geom_line() +\n  facet_wrap(~hospital) + \n  scale_x_date(labels = scales::label_date(\"%Y\"),\n               breaks = \"years\") +\n  scale_y_continuous(labels = scales::label_comma()) + \n  scale_color_identity() + \n  theme_rieke() +\n  labs(title = \"Hypothetical Hospitals\",\n       subtitle = glue::glue(\"Quarterly \",\n                             \"**{color_text('Day Surgery', col_ds)}**, \",\n                             \"**{color_text('Inpatient', col_ip)}**, \",\n                             \"**{color_text('Outpatient', col_op)}**, and \",\n                             \"**{color_text('Emergency', col_ed)}** \",\n                             \"responses\"),\n       x = NULL,\n       y = NULL)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/simulate n-size-1.png){width=2700}\n:::\n:::\n\n\nSome hospitals have a far greater patient volume than others, but within each hospital, Outpatient units tend to see the most patients while Day Surgery units tend to see the fewest. Now that n-sizes are set, let's fix some parameters so that we can simulate scores.\n\nWe'll start by fixing the covariance matrix parameters $\\sigma_{\\text{amplitude}}^2 = 0.1$ and $\\sigma_{\\text{length}}^2 = 0.1$ (because the two years of training and one year of prediction are scaled between `0` and `1`, this implies that a slow moving covariance):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma_amplitude <- 0.1\nsigma_length <- 0.1\n\n# define a covariance function to reuse later\ncov_exp_quad <- function(x,\n                         amplitude,\n                         length,\n                         delta = 1e-9) {\n  \n  S <- matrix(nrow = length(x), ncol = length(x))\n  \n  for (i in 1:nrow(S)) {\n    for (j in 1:ncol(S)) {\n      S[i,j] <- amplitude*exp(-(0.5/length)*(x[i] - x[j])^2)\n    }\n    S[i,i] <- S[i,i] + delta\n  }\n  \n  return(S)\n  \n}\n\n# time based x for this example\nx <- seq(from = 0, to = 1, length.out = 12)\nSigma <- cov_exp_quad(x, sigma_amplitude, sigma_length)\n\ntibble(x = 1:12,\n       y = Sigma[,1]) %>%\n  ggplot(aes(x = x,\n             y = y)) + \n  geom_line(color = RColorBrewer::brewer.pal(4, \"Dark2\")[3]) +\n  theme_rieke() +\n  labs(x = \"Distance (in quarters)\",\n       y = \"Covariance\") +\n  expand_limits(y = c(0))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/covariance matrix-1.png){width=2700}\n:::\n:::\n\n\nLet's see how this covariance affects a sampling of $\\beta_{ij}$ parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# work through model generation process to get a list of betas\n# for each quarter for each hospital/service\nset.seed(31)\nresponses <- \n  responses %>%\n  nest(quarters = c(quarter, n)) %>%\n  rowwise() %>%\n  mutate(eta = list(rnorm(8, 0, 1))) %>%\n  ungroup() %>%\n  mutate(Sigma = list(cov_exp_quad(seq(0, 1, length.out = 12)[1:8],\n                                   sigma_amplitude,\n                                   sigma_length)),\n         L = map(Sigma, chol),\n         beta = pmap(list(L, eta), ~t(..1) %*% ..2)) %>%\n  select(-c(eta, Sigma, L)) %>%\n  mutate(beta = map(beta, ~.x[,1])) %>%\n  unnest(c(quarters, beta))\n\n# plot!\nresponses %>%\n  ggplot(aes(x = quarter,\n             y = beta,\n             color = color)) + \n  geom_line() + \n  scale_color_identity() + \n  scale_x_date(labels = scales::label_date(\"%Y\"),\n               breaks = \"year\") + \n  facet_wrap(~hospital) +\n  theme_rieke() +\n  labs(title = \"Hypothetical Hospitals\",\n       subtitle = glue::glue(\"Quarterly \\u03B2 for \",\n                             \"**{color_text('Day Surgery', col_ds)}**, \",\n                             \"**{color_text('Inpatient', col_ip)}**, \",\n                             \"**{color_text('Outpatient', col_op)}**, and \",\n                             \"**{color_text('Emergency', col_ed)}**\"),\n       x = NULL,\n       y = NULL)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/simulate beta params-1.png){width=2700}\n:::\n:::\n\n\nThese simulated $\\beta_{ij}$ values will allow for the NPS to slowly vary over time. Let's flesh out the linear model by adding in the time-invariant terms. Expanding the details below will let you see how the parameters were fixed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkappa_1 <- -1.5\nkappa_2 <- -0.5\nsigma_interact <- 0.25\nsigma_campus <- 0.15\nsigma_service <- 1 # setting to 1 so I can set the z_s parameters manually\n\nset.seed(33)\nresponses <- \n  responses %>%\n  \n  # apply random z-offsets to z_c/z_i\n  nest(data = -hospital) %>%\n  bind_cols(z_c = rnorm(nrow(.), 0, 1)) %>%\n  unnest(data) %>%\n  nest(data = -c(hospital, service)) %>%\n  bind_cols(z_i = rnorm(nrow(.), 0, 1)) %>%\n  unnest(data) %>%\n  \n  # fix z_s based on domain knowledge\n  mutate(z_s = case_match(service,\n                          \"Inpatient\" ~ 0,\n                          \"Outpatient\" ~ 0.75,\n                          \"Day Surgery\" ~ 1,\n                          \"Emergency\" ~ -0.5)) %>%\n  \n  # apply linear model\n  mutate(phi = z_c*sigma_campus + z_s*sigma_service + z_i*sigma_interact + beta) %>%\n  select(hospital:n, phi)\n\n# plot!\nresponses %>%\n  ggplot(aes(x = quarter,\n             y = phi,\n             color = color)) + \n  geom_line() +\n  scale_color_identity() + \n  scale_x_date(labels = scales::label_date(\"%Y\"),\n               breaks = \"years\") + \n  facet_wrap(~hospital) +\n  theme_rieke() +\n  labs(title = \"Hypothetical Hospitals\",\n       subtitle = glue::glue(\"Quarterly \\u03D5 for \",\n                             \"**{color_text('Day Surgery', col_ds)}**, \",\n                             \"**{color_text('Inpatient', col_ip)}**, \",\n                             \"**{color_text('Outpatient', col_op)}**, and \",\n                             \"**{color_text('Emergency', col_ed)}**\"),\n       x = NULL,\n       y = NULL)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/simulate phi-1.png){width=2700}\n:::\n:::\n\n\nFinally, we can push the linear model back through the cumulative probability operations to get the probabilities of each category --- promoter, passive, or detractor. NPS is the percentage of promoters minus the percentage of detractors, so we can use these categorical probabilities to get the *expected* NPS at each quarter. The actual score will deviate (possibly by quite a lot!) from this expected value due to sampling variation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresponses <- \n  responses %>%\n  mutate(q1 = kappa_1 - phi,\n         q2 = kappa_2 - phi,\n         p1 = expit(q1),\n         p2 = expit(q2) - expit(q1),\n         p3 = 1 - expit(q2)) %>%\n  select(-phi, -q1, -q2) \n\n# plot!\nresponses %>%\n  mutate(nps = p3 - p1) %>%\n  ggplot(aes(x = quarter,\n             y = nps,\n             color = color)) + \n  geom_line() + \n  facet_wrap(~hospital) +\n  scale_color_identity() + \n  scale_x_date(labels = scales::label_date(\"%Y\"),\n               breaks = \"years\") + \n  scale_y_continuous(labels = scales::label_percent(accuracy = 1)) + \n  theme_rieke() +\n  labs(title = \"Hypothetical Hospitals\",\n       subtitle = glue::glue(\"Expected NPS for \",\n                             \"**{color_text('Day Surgery', col_ds)}**, \",\n                             \"**{color_text('Inpatient', col_ip)}**, \",\n                             \"**{color_text('Outpatient', col_op)}**, and \",\n                             \"**{color_text('Emergency', col_ed)}**\"),\n       x = NULL,\n       y = NULL)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/simulate expected nps-1.png){width=2700}\n:::\n:::\n\n\nBefore moving onto the final step, where we simulate actual responses, I want to take a moment just to marvel at the flexibility of gaussian processes --- with just a few parameters and a lot math, we can simulate (and on the inverse side, fit a model to) incredibly complex, non-linear data. Let's see how the responses shake out.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(34)\nresponses <- \n  responses %>%\n  mutate(responses = pmap(list(n, p1, p2, p3),\n                          ~rmultinom(1, ..1, c(..2, ..3, ..4))),\n         detractor = map_int(responses, ~.x[1]),\n         passive = map_int(responses, ~.x[2]),\n         promoter = map_int(responses, ~.x[3])) %>%\n  select(-c(p1, p2, p3, responses))\n\nresponses %>%\n  mutate(nps = (promoter - detractor)/n) %>%\n  ggplot(aes(x = quarter,\n             y = nps,\n             color = color)) + \n  geom_point(aes(size = n),\n             alpha = 0.75) + \n  geom_line(alpha = 0.75) +\n  facet_wrap(~hospital) +\n  scale_color_identity() +\n  scale_x_date(labels = scales::label_date(\"%Y\"),\n               breaks = \"years\") + \n  scale_y_continuous(labels = scales::label_percent(accuracy = 1)) + \n  scale_size_continuous(range = c(1, 4)) + \n  theme_rieke() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Hypothetical Hospitals\",\n       subtitle = glue::glue(\"Quarterly NPS for \",\n                             \"**{color_text('Day Surgery', col_ds)}**, \",\n                             \"**{color_text('Inpatient', col_ip)}**, \",\n                             \"**{color_text('Outpatient', col_op)}**, and \",\n                             \"**{color_text('Emergency', col_ed)}**\"),\n       x = NULL,\n       y = NULL)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/simulate responses-1.png){width=2700}\n:::\n:::\n\n\nWith these responses by service area, we can also summarize with a rollup NPS for each hospital (in practice, we create two separate summary scores for services that broadly fall under Inpatient or Outpatient, but in this case we'll keep it simple and just create one summary score).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresponses %>%\n  group_by(hospital, quarter) %>%\n  summarise(n = sum(n),\n            detractor = sum(detractor),\n            passive = sum(passive),\n            promoter = sum(promoter)) %>%\n  ungroup() %>%\n  mutate(nps = (promoter - detractor)/n) %>%\n  ggplot(aes(x = quarter,\n             y = nps)) +\n  geom_point(aes(size = n),\n             alpha = 0.75,\n             color = RColorBrewer::brewer.pal(3, \"Dark2\")[3]) + \n  geom_line(alpha = 0.75,\n            color = RColorBrewer::brewer.pal(3, \"Dark2\")[3]) +\n  facet_wrap(~hospital) +\n  scale_x_date(labels = scales::label_date(\"%Y\"),\n               breaks = \"years\") +\n  scale_y_continuous(labels = scales::label_percent(accuracy = 1)) +\n  theme_rieke() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Hierarchical Hospitals\",\n       subtitle = \"Quarterly Rollup NPS at each Hospital\",\n       x = NULL,\n       y = NULL)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/rollup nps-1.png){width=2700}\n:::\n:::\n\n\nAnd just like that, we have a model for generating NPS at hospital service areas, which we can summarize with an overall NPS for each hospital! Next, I'll run through the process in reverse by fitting a model to the simulated data. Once I have the estimated parameters, I can look forward and project scores for the upcoming four quarters.\n\n## Like a glove: fitting a model\n\nI've written the model in [Stan](https://mc-stan.org/). Stan is great, because it allows you to basically write the mathematical model verbatim in code and the [algorithm running underneath the model](http://www.stat.columbia.edu/~gelman/research/published/nuts.pdf) will automatically warn you when the model is poorly specified/runs into computational issues. You can unfold the code chunks below to see the model verbatim. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add indicator variables\nresponses <- \n  responses %>%\n  \n  # add hospital indicator variable\n  nest(data = -hospital) %>%\n  bind_cols(cid = seq(1:nrow(.))) %>%\n  unnest(data) %>%\n  \n  # add service indicator variable\n  nest(data = -service) %>%\n  bind_cols(sid = seq(1:nrow(.))) %>%\n  unnest(data) %>%\n  \n  # add interaction indicator variable\n  nest(data = -c(hospital, service)) %>%\n  bind_cols(iid = seq(1:nrow(.))) %>%\n  unnest(data) %>%\n  \n  # add quarter indicator variable\n  nest(data = -quarter) %>%\n  bind_cols(qid = seq(1:nrow(.))) %>%\n  unnest(data)\n\n# create response matrix\nR <- \n  responses %>%\n  select(detractor, passive, promoter) %>%\n  as.matrix()\n\n# prep stan data\nresponses_stan <-\n  list(\n    N = nrow(responses),\n    R = R,\n    N_services = max(responses$sid),\n    N_campuses = max(responses$cid),\n    N_interact = max(responses$iid),\n    sid = responses$sid,\n    cid = responses$cid,\n    iid = responses$iid,\n    Q = max(responses$qid),\n    distances = seq(from = 0, to = 1, length.out = 12)[1:8],\n    qid = responses$qid\n  )\n```\n:::\n\n::: {.cell output.var='formatted_code'}\n\n```{.stan .cell-code}\ndata{\n  int N; // rows in the training set\n  int R[N, 3]; // number of detractors/passives/promoters at each unit\n  int N_services; // number of distinct service areas\n  int N_campuses; // number of distinct campuses\n  int N_interact; // number of interactions\n  int sid[N]; // index for service area\n  int cid[N]; // index for campus\n  int iid[N]; // index for the interaction term\n  int Q; // number of quarters considered\n  real distances[Q]; // vector of time expressed as a distance between 0/1\n  int qid[N]; // index for quarter\n}\nparameters{\n  // probability model\n  ordered[2] kappa; // cutpoints\n  real alpha; // global mean\n  real<lower = 0> sigma_s; // std dev around service\n  real<lower = 0> sigma_c; // std dev around campus\n  real<lower = 0> sigma_i; // std dev around interactions\n  vector[N_services] z_s; // service offset\n  vector[N_campuses] z_c; // campus offset\n  vector[N_interact] z_i; // interaction offset\n  \n  // gaussian process terms\n  real<lower = 0> amplitude; // 'vertical' scale of gp\n  real<lower = 0> length_scale; // 'horizontal' scale of gp\n  matrix[Q, N_interact] eta; // each interaction term gets a column of eta offsets\n}\nmodel{\n  // additional model terms\n  vector[3] p; // probability vector\n  vector[2] q; // cumulative probability vector\n  real phi; // linear model term\n  \n  // additional model terms from gp\n  matrix[Q, Q] D; // covariance matrix based on distances\n  matrix[Q, Q] L; // cholesky decomposition of covariance matrix\n  matrix[Q, N_interact] beta; // quarterly offset for each interaction term\n  \n  // probability model priors\n  kappa ~ normal(-1, 0.25);\n  alpha ~ normal(0, 1);\n  z_s ~ normal(0, 1);\n  z_c ~ normal(0, 1);\n  z_i ~ normal(0, 1);\n  sigma_s ~ gamma(2, 3);\n  sigma_c ~ gamma(2, 3);\n  sigma_i ~ normal(0, 0.5);\n  \n  // gaussian process priors\n  amplitude ~ beta(2, 25); // normal 0, 0.1\n  length_scale ~ beta(2, 10); // normal 0, 0.05\n  to_vector(eta) ~ normal(0, 1);\n  \n  // construct distance matrix & cholesky decomposition\n  D = gp_exp_quad_cov(distances, amplitude, length_scale);\n  for (i in 1:size(distances)) {\n    D[i,i] = D[i,i] + 1e-9; // not using delta\n  }\n  L = cholesky_decompose(D);\n  \n  // non-centered time-dependent parameter\n  for (i in 1:N_interact) {\n    beta[,i] = L * eta[,i];\n  }\n  \n  for (i in 1:N) {\n    // linear model (ignoring time terms)\n    phi = alpha + z_s[sid[i]]*sigma_s + z_c[cid[i]]*sigma_c + z_i[iid[i]]*sigma_i;\n    \n    // add in time terms and convert to probability matrix\n    phi = phi + beta[qid[i], iid[i]];\n    q[1] = kappa[1] - phi;\n    q[2] = kappa[2] - phi;\n    q = inv_logit(q);\n    p[1] = q[1];\n    p[2] = q[2] - q[1];\n    p[3] = 1 - q[2];\n    \n    // likelihood\n    R[i,] ~ multinomial(p);\n  }\n}\n```\n:::\n\n\nLet's fit the model & perform some quick diagnostic checks to make sure everything is sampling as expected. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# this isn't evaluated within this quarto doc\n# you can see the output in the repository for this post at the source\nresponse_fit <-\n  rstan::stan(\n    \"response_model.stan\",\n    data = responses_stan,\n    chains = 4,\n    cores = 4,\n    iter = 2000,\n    refresh = 100,\n    seed = 2024,\n    init_r = 0.1\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nparams <- rethinking::precis(response_fit, depth = 3)\nindex <- order(params[,\"Rhat4\"], decreasing = TRUE)\nparams[index[1:10],5:6]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>                  n_eff    Rhat4\n#> sigma_i       902.3297 1.003957\n#> length_scale  604.6112 1.003180\n#> z_s[2]       1890.4067 1.002883\n#> alpha        1709.7763 1.002878\n#> z_i[2]       3569.8292 1.002078\n#> amplitude     904.6963 1.002040\n#> sigma_s      2200.2597 1.001950\n#> eta[4,8]     1467.2686 1.001693\n#> z_i[8]       2879.9423 1.001668\n#> z_s[3]       1812.9102 1.001375\n```\n:::\n:::\n\n\nAs measured by the [convergence statistic](https://mc-stan.org/docs/reference-manual/notation-for-samples-chains-and-draws.html#potential-scale-reduction), `Rhat4`, these are the ten parameters that have the worst convergence. All fall well below our rule-of-thumb threshold for concern, `1.01`, so we have some pretty good evidence that the model fitting went well. Given that, we can start generating some posterior predictions. The code chunk below walks through the process of generating predictions. It's a bit lengthy, but essentially just feeds each simulated parameter through the data generating process. There's a function in there, `predict_beta()`, that forecasts out into the future by [conditioning a multivariate normal](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions) on the historical parameters. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_fit <- \n  response_fit %>%\n  posterior::as_draws_df() %>%\n  as_tibble()\n\n# function for extracting id cols as a long format tibble\nextract_draws <- function(parameter) {\n  \n  out <- \n    posterior_fit %>%\n    select(.draw, starts_with(parameter)) %>%\n    pivot_longer(starts_with(parameter),\n                 names_to = paste0(str_sub(parameter, 3), \"id\"),\n                 values_to = parameter)\n  \n  out[[2]] <-\n    as.integer(str_remove_all(out[[2]], paste0(parameter, \"\\\\[|\\\\]\")))\n  \n  return(out)\n  \n}\n\nsid_draws <- extract_draws(\"z_s\")\ncid_draws <- extract_draws(\"z_c\")\niid_draws <- extract_draws(\"z_i\")\n\n# get etas as a nested matrix alongside each draw\neta_draws <- \n  posterior_fit %>%\n  select(.draw, starts_with(\"eta\")) %>%\n  pivot_longer(starts_with(\"eta\"),\n               names_to = \"index\",\n               values_to = \"eta\") %>%\n  separate(index, c(\"row_idx\", \"col_idx\"), \",\") %>%\n  mutate(across(ends_with(\"idx\"), ~as.integer(str_remove_all(.x, \"eta\\\\[|\\\\]\")))) %>%\n  pivot_wider(names_from = col_idx,\n              values_from = eta) %>%\n  select(-row_idx) %>%\n  nest(eta = -.draw) %>%\n  mutate(eta = map(eta, as.matrix))\n\n# filter down to only the needed remaining parameters per draw\nposterior_fit <- \n  posterior_fit %>%\n  select(alpha,\n         starts_with(\"kappa[\"),\n         starts_with(\"sigma\"),\n         amplitude,\n         length_scale,\n         .draw)\n\n# function for generating a cholesky factor matrix based on a distance vector\ncholesky_distance <- function(x,\n                              amplitude = 1,\n                              length_scale = 1,\n                              delta = 1e-9) {\n  \n  d <- cov_exp_quad(x, amplitude, length_scale, delta)\n  L <- chol(d)\n  return(L)\n  \n}\n\n# function for generating a matrix of betas (past, present, and future!)\n# from an eta & cholesky matrix\npredict_beta <- function(eta, L) {\n  \n  # find the known betas to condition future betas on\n  eta_max <- nrow(eta)\n  L_sub <- L[1:eta_max, 1:eta_max]\n  x2 <- matrix(0, nrow = nrow(eta), ncol = ncol(eta))\n  \n  for (i in 1:ncol(eta)) {\n    x2[,i] <- t(L_sub) %*% eta[,i]\n  }\n  \n  # get to MVNormal Sigma from cholesky factor\n  S <- t(L) %*% L\n  \n  # separate out S matrix into partitioned matrix\n  S22 <- S[1:eta_max, 1:eta_max]\n  S11 <- S[(eta_max + 1):12, (eta_max + 1):12]\n  S21 <- S[1:eta_max, (eta_max + 1):12]\n  S12 <- t(S21)\n  \n  # get MVN distributions for x1 conditional on x2\n  mu_bar <- S12 %*% MASS::ginv(S22) %*% x2\n  sigma_bar <- S11 - S12 %*% MASS::ginv(S22) %*% S21\n  \n  # make predictions for x1 for all cols in eta\n  x1 <- matrix(0, nrow = 12 - nrow(eta), ncol = ncol(eta))\n  for (i in 1:ncol(eta)) {\n    x1[,i] <- MASS::mvrnorm(1, mu_bar[,i], sigma_bar)\n  }\n  \n  # combine x1 & x2 into a beta matrix\n  beta <- matrix(0, nrow = nrow(x2) + nrow(x1), ncol = ncol(x1))\n  for (i in 1:nrow(x2)) {\n    beta[i,] <- x2[i,]\n  }\n  for (i in 1:nrow(x1)) {\n    beta[i + nrow(x2),] <- x1[i,]\n  }\n  \n  return(beta)\n  \n}\n\n# join betas to posterior fit frame\nset.seed(35)\nposterior_fit <- \n  posterior_fit %>%\n  mutate(L = pmap(list(amplitude, length_scale),\n                  ~cholesky_distance(seq(from = 0, to = 1, length.out = 12),\n                                     ..1, ..2))) %>%\n  left_join(eta_draws) %>%\n  mutate(beta = pmap(list(eta, L), ~predict_beta(..1, ..2))) %>%\n  select(-c(amplitude,\n            length_scale,\n            L, \n            eta))\n\n# get one big-ole frame for posterior prediction\nposterior_predictions <- \n  responses %>%\n  select(-c(quarter,\n            detractor,\n            passive,\n            promoter,\n            n,\n            qid)) %>%\n  distinct() %>%\n  full_join(iid_draws, by = \"iid\", multiple = \"all\") %>%\n  left_join(cid_draws, by = c(\"cid\", \".draw\")) %>%\n  left_join(sid_draws, by = c(\"sid\", \".draw\")) %>%\n  left_join(posterior_fit, by = \".draw\")\n\n# generate predictions\nposterior_predictions <- \n  posterior_predictions %>%\n  \n  # apply time-invariant linear model\n  mutate(phi = alpha + z_s*sigma_s + z_c*sigma_c + z_i*sigma_i,\n         beta = pmap(list(beta, iid), ~..1[,..2]),\n         quarter = list(seq.Date(from = lubridate::mdy(\"1/1/21\"),\n                                 to = lubridate::mdy(\"10/1/23\"),\n                                 by = \"quarter\"))) %>%\n  unnest(c(quarter, beta)) %>%\n  \n  # apply time-variance and get probability of each category\n  mutate(phi = phi + beta,\n         q1 = `kappa[1]` - phi,\n         q2 = `kappa[2]` - phi,\n         p1 = expit(q1),\n         p2 = expit(q2) - expit(q1),\n         p3 = 1 - expit(q2)) %>%\n  \n  # join actual response data\n  select(service,\n         hospital,\n         .draw,\n         quarter,\n         p1, \n         p2,\n         p3) %>%\n  left_join(responses %>% distinct(service, hospital, quarter, detractor, passive, promoter, n),\n            by = c(\"service\", \"hospital\", \"quarter\"))\n\n# estimate n-size sans model\n# this is one of those things I want to include in the next model update\nset.seed(36)\nposterior_predictions <- \n  posterior_predictions %>%\n  mutate(log_n = log(n)) %>%\n  group_by(service, hospital) %>%\n  mutate(lambda_mean = mean(log_n, na.rm = TRUE),\n         lambda_sd = sd(log_n, na.rm = TRUE)) %>%\n  ungroup() %>%\n  bind_cols(lambda_sim = rnorm(nrow(.), .$lambda_mean, .$lambda_sd)) %>%\n  bind_cols(n_sim = rpois(nrow(.), exp(.$lambda_mean))) %>%\n  mutate(n_sim = if_else(is.na(n), n_sim, n)) %>%\n  select(-c(log_n:lambda_sim))\n\n# simulate responses\nset.seed(37)\nposterior_predictions <- \n  posterior_predictions %>%\n  mutate(score_sim = pmap(list(n_sim, p1, p2, p3), ~rmultinom(1, ..1, c(..2, ..3, ..4))),\n         nps_sim = map_dbl(score_sim, ~(.x[3] - .x[1])/sum(.x))) %>%\n  select(service, \n         hospital,\n         .draw,\n         quarter,\n         detractor,\n         passive,\n         promoter,\n         n_sim,\n         nps_sim) %>%\n  nest(sims = c(.draw, n_sim, nps_sim))\n\nposterior_predictions <- \n  posterior_predictions %>%\n  mutate(color = case_match(service,\n                            \"Inpatient\" ~ col_ip,\n                            \"Outpatient\" ~ col_op,\n                            \"Emergency\" ~ col_ed,\n                            \"Day Surgery\" ~ col_ds))\n```\n:::\n\n\nWe now have a distribution of possible scores for each service area at each hospital for each quarter. From this distribution, we can set the three goal levels (threshold, target, and distinguished) based on quantiles of the distribution. For conveniences sake, I've set these quantiles to 10%, 50%, and 90% for threshold, target, and distinguished, respectively. This means that, according to this model, an area falling below threshold or rising above distinguished is somewhat unlikely to be a fluke and it conveniently lines up with an 80% credible interval. Plotted below, we can see how the scores are expected to shift over time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot!\nposterior_predictions %>%\n  mutate(.pred_nps = map_dbl(sims, ~quantile(.x$nps_sim, probs = 0.5)),\n         .pred_nps_lower = map_dbl(sims, ~quantile(.x$nps_sim, probs = 0.1)),\n         .pred_nps_upper = map_dbl(sims, ~quantile(.x$nps_sim, probs = 0.9)),\n         n = promoter + passive + detractor,\n         nps = (promoter - detractor)/n) %>%\n  ggplot(aes(x = quarter,\n             y = .pred_nps,\n             ymin = .pred_nps_lower,\n             ymax = .pred_nps_upper)) +\n  geom_vline(xintercept = lubridate::mdy(\"1/1/23\"),\n             linetype = \"dashed\",\n             color = \"gray60\",\n             alpha = 0.5) + \n  geom_ribbon(aes(fill = color),\n              alpha = 0.5) +\n  geom_line(aes(color = color),\n            linewidth = 0.25) +\n  scale_fill_identity() +\n  scale_color_identity() + \n  scale_y_continuous(labels = scales::label_percent(accuracy = 1)) + \n  facet_wrap(~hospital) +\n  theme_rieke() +\n  labs(title = \"Hypothetical Hospitals\",\n       subtitle = glue::glue(\"Predicted NPS for \",\n                             \"**{color_text('Day Surgery', col_ds)}**, \",\n                             \"**{color_text('Inpatient', col_ip)}**, \",\n                             \"**{color_text('Outpatient', col_op)}**, and \",\n                             \"**{color_text('Emergency', col_ed)}**\"),\n       x = NULL,\n       y = NULL,\n       caption = paste(\"Posterior prediction from 4,000 MCMC samples\",\n                       \"Shaded range represents 80% credible interval\",\n                       sep = \"<br>\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot posterior service areas-1.png){width=2700}\n:::\n:::\n\n\nWe can also summarize the predictions in rollup scores at each hospital.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_predictions %>%\n  unnest(sims) %>%\n  select(hospital, quarter, .draw, n_sim, nps_sim) %>%\n  group_by(hospital, quarter, .draw) %>%\n  summarise(nps = sum((nps_sim * n_sim))/sum(n_sim)) %>%\n  group_by(hospital, quarter) %>%\n  summarise(.pred_nps = quantile(nps, probs = 0.5),\n            .pred_nps_lower = quantile(nps, probs = 0.1),\n            .pred_nps_upper = quantile(nps, probs = 0.9)) %>%\n  ungroup() %>%\n  ggplot(aes(x = quarter,\n             y = .pred_nps,\n             ymin = .pred_nps_lower,\n             ymax = .pred_nps_upper)) + \n  geom_vline(xintercept = lubridate::mdy(\"1/1/23\"),\n             linetype = \"dashed\",\n             color = \"gray60\",\n             alpha = 0.5) + \n  geom_ribbon(fill = RColorBrewer::brewer.pal(3, \"Dark2\")[3],\n              alpha = 0.5) +\n  geom_line(color = RColorBrewer::brewer.pal(3, \"Dark2\")[3],\n            linewidth = 0.25) +\n  facet_wrap(~hospital) +\n  scale_y_continuous(labels = scales::label_percent(accuracy = 1)) + \n  theme_rieke() +\n  labs(title = \"Hypothetical Hospitals\",\n       subtitle = \"Predicted Rollup NPS at each Hospital\",\n       x = NULL,\n       y = NULL,\n       caption = paste(\"Posterior prediction from 4,000 MCMC samples\",\n                       \"Shaded range represents 80% credible interval\",\n                       sep = \"<br>\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot posterior rollup-1.png){width=2700}\n:::\n:::\n\n\nThe $\\sigma_{length}^2$ parameter used to simulate data implies that scores take quite a bit of time to return to their stabilized value --- here it appears four quarters in 2023 isn't lengthy enough to reach stability. Despite this, the gaussian process allows us to predict how scores will progress through the future as they decay towards their stabilized value. In our actual goal predictions file, we also summarize the scores at an annual level and present the results in a table. It's a bit late on a Sunday afternoon and I've been staring at code for the better part of a day, so I'll instead I'll just leave it at the plot. \n\n## Some closing thoughts\n\nAndrew Gelman has a fun phrase that he uses often --- [\"big data means big model.\"](https://statmodeling.stat.columbia.edu/2014/05/22/big-data-needs-big-model/) With years of patient satisfaction data to sift through, we certainly have \"big data,\" and gaussian processes provide the means for fitting a big, complex model to the data. This complexity comes at a cost. Taming the real goal model took a lot of time and effort, and there's still more work to do (which probably means I'll end up needing to refactor the model again). That being said, the juice here is worth the squeeze --- gaussian processes are a powerful, flexible tool that allow us to express complex ideas with just a lil bit of matrix math.\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}