{
  "hash": "da60668a9f6dadfe722552934b5d0e3f",
  "result": {
    "markdown": "---\ntitle: The Math Behind workboots\ndate: '2022-07-05'\ncategories: [rstats, workboots, tidymodels]\nimage: featured.png\ncode-fold: show\n---\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot2::theme_set(\n  ggplot2::theme_minimal() +\n    ggplot2::theme(plot.title.position = \"plot\",\n                   plot.background = ggplot2::element_rect(fill = \"white\", color = \"white\"))\n)\n```\n:::\n\n\nGenerating prediction intervals with workboots hinges on a few core concepts: bootstrap resampling, estimating prediction error for each resample, and aggregating the resampled prediction errors for each observation. The [`bootstraps()` documentation from {rsample}](https://rsample.tidymodels.org/reference/bootstraps.html) gives a concise definition of bootstrap resampling:\n\n> A bootstrap sample is a sample that is the same size as the original data set that is made using replacement. This results in analysis samples that have multiple replicates of some of the original rows of the data. The assessment set is defined as the rows of the original data that were not included in the bootstrap sample. This is often referred to as the \"out-of-bag\" (OOB) sample.\n\nThis vignette will walk through the details of estimating and aggregating prediction errors --- additional resources can be found in Davison and Hinkley's book, [*Bootstrap Methods and their Application*](https://www.cambridge.org/core/books/bootstrap-methods-and-their-application/ED2FD043579F27952363566DC09CBD6A), or Efron and Tibshirani's paper, [*Improvements on Cross-Validation: The Bootstrap .632+ Method*](https://www.jstor.org/stable/2965703).\n\n### The Bootstrap .632+ Method\n\n:::{.alert-info}\nWhat follows here is largely a summary of [this explanation](https://stats.stackexchange.com/questions/96739/what-is-the-632-rule-in-bootstrapping/96750#96750) of the .632+ error rate by Benjamin Deonovic.\n:::\n\nWhen working with bootstrap resamples of a dataset, there are two error estimates we can work with: the bootstrap training error and the out-of-bag (oob) error. Using the [Sacramento housing dataset](https://modeldata.tidymodels.org/reference/Sacramento.html), we can estimate the training and oob error for a single bootstrap.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n# setup our data\ndata(\"Sacramento\")\n\nSacramento <-\n  Sacramento %>%\n  select(sqft, type, price) %>%\n  mutate(across(c(sqft, price), log10)) \n\nset.seed(987)\nsacramento_split <- initial_split(Sacramento)\nsacramento_train <- training(sacramento_split)\nsacramento_test <- testing(sacramento_split)\n\n# setup bootstrapped dataset for .632+ example\nsacramento_boots <- bootstraps(sacramento_train, times = 1)\n\nsacramento_boots\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Bootstrap sampling \n# A tibble: 1 Ã— 2\n  splits            id        \n  <list>            <chr>     \n1 <split [699/261]> Bootstrap1\n```\n:::\n:::\n\n\nUsing a [k-nearest-neighbor regression model](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#k-NN_regression) and [rmse](https://en.wikipedia.org/wiki/Root-mean-square_deviation#:~:text=The%20root%2Dmean%2Dsquare%20deviation,estimator%20and%20the%20values%20observed.) as our error metric, we find that the training and oob error differ, with the training error lesser than the oob error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# setup a workflow to predict price using a knn regressor\nsacramento_recipe <-\n  recipe(price ~ ., data = sacramento_train) %>%\n  step_dummy(all_nominal())\n\nsacramento_spec <-\n  nearest_neighbor() %>%\n  set_mode(\"regression\")\n\nsacramento_wf <-\n  workflow() %>%\n  add_recipe(sacramento_recipe) %>%\n  add_model(sacramento_spec)\n\nset.seed(876)\nsacramento_fit <-\n  sacramento_wf %>%\n  fit(training(sacramento_boots$splits[[1]])) \n\n# get bootstrap training error\nsacramento_train_err <- \n  Metrics::rmse(\n    training(sacramento_boots$splits[[1]])$price,\n    sacramento_fit %>% predict(training(sacramento_boots$splits[[1]])) %>% pull(.pred)\n  )\n\n# get oob error\nsacramento_oob_err <-\n  Metrics::rmse(\n    testing(sacramento_boots$splits[[1]])$price,\n    sacramento_fit %>% predict(testing(sacramento_boots$splits[[1]])) %>% pull(.pred)\n  )\n\nsacramento_train_err\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.08979873\n```\n:::\n\n```{.r .cell-code}\nsacramento_oob_err\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1661675\n```\n:::\n:::\n\n\nThe training error is overly optimistic in the model's performance and likely to under-estimate the prediction error. We are interested in the model's performance on new data. The oob error, on the other hand, is likely to over-estimate the prediction error! This is due to non-distinct observations in the bootstrap sample that results from sampling with replacement. Given that [the average number of distinct observations in a bootstrap training set is about `0.632 * total_observations`](https://stats.stackexchange.com/questions/88980/why-on-average-does-each-bootstrap-sample-contain-roughly-two-thirds-of-observat?lq=1), Efron and Tibshirani proposed a blend of the training and oob error with the 0.632 estimate:\n\n$$\n\\begin{align*}\nErr_{.632} & = 0.368 Err_{train} + 0.632 Err_{oob}\n\\end{align*}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsacramento_632 <- 0.368 * sacramento_train_err + 0.632 * sacramento_oob_err\nsacramento_632\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1380638\n```\n:::\n:::\n\n\nIf, however, the model is highly overfit to the bootstrap training set, the training error will approach 0 and the 0.632 estimate will *under estimate* the prediction error.\n\nAn example from [*Applied Predictive Modeling*](http://appliedpredictivemodeling.com/) shows that as model complexity increases, the reported resample accuracy by the 0.632 estimate continues to increase whereas other resampling strategies report diminishing returns:\n\n![](https://user-images.githubusercontent.com/5731043/157986232-9c32c1c2-a7ed-4f9f-b28e-7d8ccb7ac41c.png)\n\nAs an alternative to the 0.632 estimate, Efron & Tibshirani also propose the 0.632+ estimate, which re-weights the blend of training and oob error based on the model overfit rate:\n\n$$\n\\begin{align*}\nErr_{0.632+} & = (1 - w) Err_{train} + w Err_{oob} \\\\\n\\\\\nw & = \\frac{0.632}{1 - 0.368 R} \\\\\n\\\\\nR & = \\frac{Err_{oob} - Err_{train}}{\\gamma - Err_{train}}\n\\end{align*}\n$$\n\nHere, $R$ represents the overfit rate and $\\gamma$ is the no-information error rate, estimated by evaulating all combinations of predictions and actual values in the bootstrap training set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# estimate the no-information error rate\npreds_train <- \n  predict(sacramento_fit, training(sacramento_boots$splits[[1]])) %>% \n  pull(.pred)\n\nactuals_train <- \n  training(sacramento_boots$splits[[1]]) %>% \n  pull(price)\n\nall_combinations <- \n  crossing(actuals_train, preds_train)\n\nrmse_ni <- \n  Metrics::rmse(all_combinations$actuals_train, all_combinations$preds_train)\n\n# estimate the overfit rate\noverfit <- \n  (sacramento_oob_err - sacramento_train_err)/(rmse_ni - sacramento_train_err)\n\n# estimate weight\nw <- 0.632 / (1 - 0.368 * overfit)\n\nsacramento_632_plus <- (1 - w) * sacramento_train_err + w * sacramento_oob_err\nsacramento_632_plus\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1450502\n```\n:::\n:::\n\n\nWhen there is no overfitting (i.e., $R = 0$) the 0.632+ estimate will equal the 0.632 estimate. In this case, however, the model is overfitting the training set and the 0.632+ error estimate is pushed a bit closer to the oob error.\n\n### Prediction intervals with many bootstraps\n\n[For an unbiased estimator, rmse is the standard deviation of the residuals](https://en.wikipedia.org/wiki/Root-mean-square_deviation#Formula). With this in mind, we can modify our predictions to include a sample from the residual distribution (for more information, see Algorithm 6.4 from Davison and Hinkley's *Bootstrap Methods and their Application*):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(999)\nresid_train_add <- rnorm(length(preds_train), 0, sacramento_632_plus)\npreds_train_mod <- preds_train + resid_train_add\n```\n:::\n\n\nThus far, we've been working with a single bootstrap resample. When working with a single bootstrap resample, adding this residual term gives a pretty poor estimate for each observation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\ntibble(.pred = preds_train_mod) %>%\n  bind_cols(training(sacramento_boots$splits[[1]])) %>%\n  mutate(across(c(.pred, price), ~10^.x)) %>%\n  ggplot(aes(x = .pred, y = price)) +\n  geom_point(alpha = 0.25,\n             size = 2.5,\n             color = \"midnightblue\") +\n  geom_abline(linetype = \"dashed\",\n              size = 1, \n              color = \"gray\") +\n  labs(title = \"Predicted sale price of home in Sacramento\",\n       subtitle = \"Adding a single error estimate produces poor predictions of price\",\n       x = \"Predicted price\",\n       y = \"Actual price\") +\n  scale_x_log10(labels = scales::label_dollar(scale_cut = cut_short_scale())) +\n  scale_y_log10(labels = scales::label_dollar(scale_cut = cut_short_scale())) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=2700}\n:::\n:::\n\n\nWith workboots, however, we can repeat this process over many bootstrap datasets to generate a prediction distribution for each observation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(workboots)\n\n# fit and predict price in sacramento_test from 100 models\n# the default number of resamples is 2000 - dropping here to speed up knitting\nset.seed(555)\nsacramento_pred_int <-\n  sacramento_wf %>%\n  predict_boots(\n    n = 100,\n    training_data = sacramento_train,\n    new_data = sacramento_test\n  )\n\nsacramento_pred_int %>%\n  summarise_predictions() %>%\n  bind_cols(sacramento_test) %>%\n  mutate(across(c(.pred:.pred_upper, price), ~ 10^.x)) %>%\n  ggplot(aes(x = .pred,\n             y = price,\n             ymin = .pred_lower,\n             ymax = .pred_upper)) +\n  geom_point(alpha = 0.25,\n             size = 2.5,\n             color = \"midnightblue\") +\n  geom_errorbar(alpha = 0.25,\n                color = \"midnightblue\",\n                width = 0.0125) +\n  scale_x_log10(labels = scales::label_dollar(scale_cut = cut_short_scale())) +\n  scale_y_log10(labels = scales::label_dollar(scale_cut = cut_short_scale())) +\n  geom_abline(linetype = \"dashed\",\n              size = 1,\n              color = \"gray\") +\n  labs(title = \"Predicted sale price of home in Sacramento\",\n       subtitle = \"Using many resamples allows us to generate prediction intervals\",\n       x = \"Predicted price\",\n       y = \"Actual price\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=2700}\n:::\n:::\n\n\nThis methodology produces prediction distributions that are [consistent with what we might expect from linear models](https://markjrieke.github.io/workboots/articles/Estimating-Linear-Intervals.html) while making no assumptions about model type (i.e., we can use a non-parametric model; in this case, a k-nearest neighbors regression). ",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}