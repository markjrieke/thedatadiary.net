{
  "hash": "49333147100e6631965ab520690fee50",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Sufficiency go brrrrrr\"\ndate: '2026-02-15'\ncategories: [stan, sufficiency]\ndescription: \"One of the many ways to parameterize a sufficient Gamma\"\nimage: header.png\nfilters:\n  - add-code-files\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(ggdist)\nlibrary(riekelib)\n\n# Setup a color palette to be used for groups A/B\npal <- NatParksPalettes::NatParksPalettes$Acadia[[1]][c(2,8)]\n```\n:::\n\n\nSay you're modeling some time-to-event data with a wide range of possible outcomes. Some events occur quickly (<2s) and some take a while (>35s). Events can't occur after negative seconds, so you reach for a [gamma](https://en.wikipedia.org/wiki/Gamma_distribution) likelihood. In [Stan](https://mc-stan.org/docs/functions-reference/positive_continuous_distributions.html#gamma-distribution) and [PyMC](https://www.pymc.io/projects/docs/en/latest/api/distributions/generated/pymc.Gamma.html), the built-in gamma density functions only consider individual observations. This is probably fine for a relatively small number of observations but doesn't scale well. For better scaling, you'll need to roll your own [sufficient](https://en.wikipedia.org/wiki/Sufficient_statistic) formulation of the gamma distribution.\n\nIf you want to skip the derivation that follows in the rest of the article, this sufficient gamma density function can just be dropped into a function block at the top of a Stan model. Note that the gamma distribution has, like, a thousand different parameterizations. This, and other gamma distributions throughout this article, make use of the shape ($\\alpha$) / scale ($\\theta$) parameterization.\n\n```stan\nreal sufficient_gamma_lpdf(\n  real Xsum,\n  real Xlogsum,\n  real n,\n  real alpha,\n  real theta\n) {\n  real lp = 0.0;\n  lp += -n * (lgamma(alpha) + alpha * log(theta));\n  lp += (alpha - 1) * Xlogsum;\n  lp += -Xsum / theta;\n  return lp;\n}\n```\n\n## Individual Inference\n\nTo demonstrate the benefits of the sufficient gamma, I'll fit two models to the same dataset --- first using Stan's default gamma density function and then using our custom sufficient density function. I'll simulate 5,000 observations from two different gamma-distributed groups for a total of 10,000 observations in the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate two groups with a mean of 16\nset.seed(2026)\ngamma_data <-\n  tibble(group = c(\"A\", \"B\"),\n         alpha = c(4, 2),\n         theta = c(4, 8)) %>%\n  mutate(y = pmap(list(alpha, theta), ~rgamma(5000, shape = ..1, scale = ..2)))\n\ngamma_data %>%\n  unnest(y) %>%\n  ggplot(aes(x = y,\n             y = group,\n             fill = group)) +\n  stat_histinterval(breaks = 60,\n                    slab_alpha = 0.75) +\n  scale_fill_manual(values = pal) +\n  theme_rieke()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=2700}\n:::\n:::\n\n\nWe'll fit a simple gamma likelihood to these observations where each group, $g$, has separate shape and scale parameters.\n\n$$\n\\begin{align*}\ny_i &\\sim \\text{Gamma}(\\alpha_g, \\theta_g) \\\\\n\\alpha_g &\\sim \\text{Gamma}(1, 5) \\\\\n\\theta_g &\\sim \\text{Gamma}(1, 5)\n\\end{align*}\n$$\n\n\n::: {.cell add-from='individual-gamma.stan' source-lang='stan'}\n\n```{.r .cell-code  code-summary=\"Stan Model\"}\n# comment\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindividual_model <- cmdstan_model(\"individual-gamma.stan\")\n\nmodel_data <-\n  gamma_data %>%\n  mutate(gid = rank(group)) %>%\n  unnest(y)\n\n# Stan makes use of the shape/rate parameterization\n# So the scale prior of 5 is converted to a rate prior of 1/5\nstan_data <-\n  list(\n    N = nrow(model_data),\n    Y = model_data$y,\n    G = max(model_data$gid),\n    gid = model_data$gid,\n    alpha_alpha = 1,\n    beta_alpha = 0.2,\n    alpha_theta = 1,\n    beta_theta = 0.2\n  )\n\nstart_time <- Sys.time()\nindividual_fit <-\n  individual_model$sample(\n    data = stan_data,\n    seed = 1234,\n    iter_warmup = 1000,\n    iter_sampling = 1000,\n    chains = 10,\n    parallel_chains = 10\n  )\nrun_time_individual <- as.numeric(Sys.time() - start_time)\n```\n:::\n\n\nThis model recovers the parameters as expected and we end up with a nice-looking posterior.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_posterior <- function(fit) {\n  \n  # Get the true parameters used in simulation\n  truth <-\n    gamma_data %>%\n    select(group, alpha, theta) %>%\n    pivot_longer(-group,\n                 names_to = \"parameter\",\n                 values_to = \"draw\") %>%\n    mutate(parameter = if_else(parameter == \"alpha\", \"\\u03b1\", \"\\u03b8\"))\n  \n  fit$draws(c(\"alpha\", \"theta\"), format = \"df\") %>%\n    as_tibble() %>%\n    select(starts_with(\"alpha\"), starts_with(\"theta\")) %>%\n    pivot_longer(everything(),\n                 names_to = \"parameter\",\n                 values_to = \"draw\") %>%\n    nest(data = -parameter) %>%\n    separate(parameter, c(\"parameter\", \"group\"), \"\\\\[\") %>%\n    mutate(group = str_remove(group, \"\\\\]\"),\n           group = if_else(group == \"1\", \"A\", \"B\"),\n           parameter = if_else(parameter == \"alpha\", \"\\u03b1\", \"\\u03b8\")) %>%\n    unnest(data) %>%\n    ggplot(aes(x = draw,\n               y = parameter,\n               fill = group)) +\n    stat_histinterval(breaks = 60,\n                      slab_alpha = 0.75) +\n    geom_point(data = truth,\n               color = \"red\") +\n    scale_fill_manual(values = pal) +\n    theme_rieke()\n  \n}\n\nplot_posterior(individual_fit) +\n  labs(title = \"**Fit against individual observations**\",\n       subtitle = \"Can be used as a baseline for evaluating the sufficient formulation\",\n       x = NULL,\n       y = NULL)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=2700}\n:::\n:::\n\n\nBut! It takes **<span style='color:royalblue'>12.6 seconds</span>** to fit this model. This is because the model evaluates the gamma likelihood 10,000 times, separately --- once for each observation in the dataset. We can speed up our model by exploiting sufficiency!\n\n## Density Derivations\n\nThe $\\text{Gamma}(\\alpha, \\theta)$ sampling statement from the individual model is just a function for computing the probability density. For a single observation, that density is:\n\n$$\n\\Pr(y_i) = \\frac{1}{\\Gamma(\\alpha)\\theta^\\alpha}y_i^{\\alpha-1}e^{-y_i/\\theta}\n$$\n\nWhen fitting a model, we multiply the probability density for each individual observation to get a total proability density for the entire set of observations.\n\n$$\n\\begin{align*}\n\\Pr(y_1, y_2, \\dots, y_n) &= \\Pr(y_1)\\Pr(y_2)\\dots\\Pr(y_n) \\\\\n\\Pr(y) &= \\prod_i^N\\Pr(y_i)\n\\end{align*}\n$$\n\nStan (and other probabilistic programming languages) evaluate the model's density on the log scale. Log-scale math is a bit easier and we can think in terms of sums instead of products.\n\n$$\n\\log(\\Pr(y)) = \\sum_i^N\\log(\\Pr(y_i))\n$$\n\nIf we take the log of the gamma density function from above, the log probability of an individual observation is this:\n\n$$\n\\log(\\Pr(y_i|\\alpha,\\theta)) = -\\log(\\Gamma(\\alpha)) - \\alpha\\log(\\theta) + (\\alpha-1)\\log(y_i) - y_i/\\theta\n$$\n\nIf we think about a case where $y$ is a vector with two observations, we can just add these two log probabilities together to get the total log probability.\n\n$$\n\\begin{align*}\n\\log(\\Pr(y_1,y_2|\\alpha,\\theta)) = &-\\log(\\Gamma(\\alpha)) - \\alpha\\log(\\theta) + (\\alpha-1)\\log(y_1) - y_1/\\theta \\\\\n&-\\log(\\Gamma(\\alpha)) - \\alpha\\log(\\theta) + (\\alpha-1)\\log(y_2) - y_2/\\theta\n\\end{align*}\n$$\n\nAlready, we can see how doing this repeatedly for additional observations is ripe for combining. $\\Gamma(\\alpha)$ and $\\alpha\\log(\\theta)$ are added for each observation in $y$, so in the aggregate can just be multiplied by the total number of observations, $n$. $(\\alpha-1)$ is multiplied by each $\\log(y_i)$ and can be factored out. Similarly, each new observation adds an additional $y_i/\\theta$ term, which can just be replaced with a sum.\n\n$$\n\\log(\\Pr(y,n|\\alpha,\\theta)) = -n\\left(\\log(\\Gamma(\\alpha))+\\alpha\\log(\\theta)\\right) + (\\alpha-1)\\sum\\log(y) - \\sum y/\\theta\n$$\n\nThe log probability function is what actually gets used to estimate the model, but we can also un-log to show the natural probability density function. In summary, the probability density of a set of gamma distributed observations, $y$, has the following sufficient formulation given the product, sum, and count of observations in $y$.\n\n$$\n\\Pr\\left(\\prod y, \\sum y, n | \\alpha, \\theta\\right) = \\frac{1}{\\Gamma(\\alpha)^n\\theta^{n\\alpha}}\\left(\\prod y\\right)^{\\alpha-1}e^{-\\sum y/\\theta}\n$$\n\n## Sufficient Speedups\n\nWith this new sufficient formulation, we can reduce the size of the dataset passed to the model from 10,000 rows to just 2, _without losing information about the full distribution_.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsufficient_model <- cmdstan_model(\"sufficient-gamma.stan\")\n\nmodel_data <-\n  gamma_data %>%\n  mutate(gid = rank(group),\n         n = map_int(y, length),\n         Ysum = map_dbl(y, sum),\n         Ylogsum = map_dbl(y, ~sum(log(.x))))\n\nmodel_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 2 Ã— 8\n#>   group alpha theta y               gid     n   Ysum Ylogsum\n#>   <chr> <dbl> <dbl> <list>        <dbl> <int>  <dbl>   <dbl>\n#> 1 A         4     4 <dbl [5,000]>     1  5000 81089.  13285.\n#> 2 B         2     8 <dbl [5,000]>     2  5000 79287.  12472.\n```\n\n\n:::\n:::\n\n\nThe model itself stays largely the same, we just swap out the likelihood function and pass in the pre-computed summary information.\n\n$$\n\\begin{align*}\n\\prod y, \\sum y, n &\\sim \\text{Sufficient-Gamma}(\\alpha_g, \\theta_g) \\\\\n\\alpha_g &\\sim \\text{Gamma}(1, 5) \\\\\n\\theta_g &\\sim \\text{Gamma}(1, 5)\n\\end{align*}\n$$\n\n\n::: {.cell add-from='sufficient-gamma.stan' source-lang='stan'}\n\n```{.r .cell-code  code-summary=\"Stan Model\"}\n# comment\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_data <-\n  list(\n    N = nrow(model_data),\n    Ysum = model_data$Ysum,\n    Ylogsum = model_data$Ylogsum,\n    n_obs = model_data$n,\n    G = max(model_data$gid),\n    gid = model_data$gid,\n    alpha_alpha = 1,\n    beta_alpha = 0.2,\n    alpha_theta = 1,\n    beta_theta = 0.2\n  )\n\nstart_time <- Sys.time()\nsufficient_fit <-\n  sufficient_model$sample(\n    data = stan_data,\n    seed = 1234,\n    iter_warmup = 1000,\n    iter_sampling = 1000,\n    chains = 10,\n    parallel_chains = 10\n  )\nrun_time_sufficient <- as.numeric(Sys.time() - start_time)\n```\n:::\n\n\nBut this parameterization is _much faster_. Fitting the sufficient formulation to the same dataset only takes **<span style='color:royalblue'>0.7 seconds</span>**, a **<span style='color:royalblue'>18.3x</span>** speedup compared to the fit against individual observations. And we recover the same posterior that would've resulted from the individual fit!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_posterior(sufficient_fit)  +\n  labs(title = \"**Fit using the sufficient gamma**\",\n       subtitle = \"Recovers the same posterior from the fit to individual observations!\",\n       x = NULL,\n       y = NULL)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=2700}\n:::\n:::\n\n\nTL;DR, exploit sufficiency, go zoom zoom.\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}