{
  "hash": "7a0bb2d164cd98bccf055ad53e64c2da",
  "result": {
    "markdown": "---\ntitle: \"Introducing {workboots}\"\ndate: '2022-03-14'\ncategories: [rstats, tidymodels, workboots]\ndescription: \"Generate bootstrap prediction intervals from a tidymodel workflow!\"\nimage: featured.png\ncode-fold: show\n---\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# setup themes\nextrafont::loadfonts(device = \"win\")\nggplot2::theme_set(\n  ggplot2::theme_minimal(base_family = \"Roboto Slab\", \n                         base_size = 14) +\n    ggplot2::theme(plot.title.position = \"plot\",\n                   plot.background = ggplot2::element_rect(fill = \"white\", color = \"white\"))\n)\n```\n:::\n\n\n\nSometimes, we want a model that generates a range of possible outcomes around each prediction and may opt for a model that can generate a prediction interval, like a linear model. Other times, we just care about point predictions and may opt to use a more powerful model like XGBoost. But what if we want the best of both worlds: getting a range of predictions while still using a powerful model? That's where [`{workboots}`](https://github.com/markjrieke/workboots) comes to the rescue! `{workboots}` uses bootstrap resampling to train many models which can be used to generate a range of outcomes --- regardless of model type.\n\n![](pics/workboots.png)\n\n# Installation\n\nVersion 0.1.0 of `{workboots}` is available on [CRAN](https://cran.r-project.org/web/packages/workboots/index.html). Given that the package is still in early development, however, I'd recommend installing the development version from [github](https://github.com/markjrieke/workboots):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install from CRAN\ninstall.packages(\"workboots\")\n\n# or install the development version\ndevtools::install_github(\"markjrieke/workboots\")\n```\n:::\n\n\n# Usage\n\n`{workboots}` builds on top of the [`{tidymodels}`](https://www.tidymodels.org/) suite of packages and is intended to be used in conjunction with a [tidymodel workflow](https://workflows.tidymodels.org/). Teaching how to use `{tidymodels}` is beyond the scope of this post, but some helpful resources are linked at the bottom for further exploration. \n\nWe'll walk through two examples that show the benefit of the package: estimating a linear model's prediction interval and generating a prediction interval for a boosted tree model.\n\n## Estimating a prediction interval\n\nLet's get started with a model we know can generate a prediction interval: a basic linear model. In this example, we'll use the [Ames housing dataset](https://modeldata.tidymodels.org/reference/ames.html) to predict a home's price based on its square footage.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n\n# setup our data\ndata(\"ames\")\names_mod <- ames %>% select(First_Flr_SF, Sale_Price)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# relationship between square footage and price\names_mod %>%\n  ggplot(aes(x = First_Flr_SF, y = Sale_Price)) +\n  geom_point(alpha = 0.25) +\n  scale_y_continuous(labels = scales::dollar_format(), trans = \"log10\") +\n  scale_x_continuous(labels = scales::comma_format(), trans = \"log10\") +\n  labs(title = \"Relationship between Square Feet and Sale Price\",\n       subtitle = \"Linear relationship between the log transforms of square footage and price\",\n       x = NULL,\n       y = NULL)\n```\n:::\n\n\n![](pics/plot_01.png)\n\nWe can use a linear model to predict the log transform of `Sale_Price` based on the log transform of `First_Flr_SF`. In this example, we'll train a linear model then plot our predictions against a holdout set with a prediction interval.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# log transform\names_mod <- \n  ames_mod %>%\n  mutate(across(everything(), log10))\n\n# split into train/test data\nset.seed(918)\names_split <- initial_split(ames_mod)\names_train <- training(ames_split)\names_test <- testing(ames_split)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# train a linear model\nset.seed(314)\nmod <- lm(Sale_Price ~ First_Flr_SF, data = ames_train)\n\n# predict on new data with a prediction interval\names_preds <-\n  mod %>%\n  predict(ames_test, interval = \"predict\") %>%\n  as_tibble()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot!\names_preds %>%\n  \n  # re-scale predictions to match the original dataset's scale\n  bind_cols(ames_test) %>%\n  mutate(across(everything(), ~10^.x)) %>%\n  \n  # add geoms\n  ggplot(aes(x = First_Flr_SF)) +\n  geom_point(aes(y = Sale_Price),\n             alpha = 0.25) +\n  geom_line(aes(y = fit),\n            size = 1) +\n  geom_ribbon(aes(ymin = lwr,\n                  ymax = upr),\n              alpha = 0.25) +\n  scale_y_continuous(labels = scales::dollar_format(), trans = \"log10\") +\n  scale_x_continuous(labels = scales::comma_format(), trans = \"log10\") +\n  labs(title = \"Linear Model of Sale Price predicted by Square Footage\",\n       subtitle = \"Shaded area represents the 95% prediction interval\",\n       x = NULL,\n       y = NULL) \n```\n:::\n\n\n![](pics/plot_02.png)\nWith `{workboots}`, we can approximate the linear model's prediction interval by passing a workflow built on a linear model to the function `predict_boots()`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(workboots)\n\n# setup a workflow with a linear model\names_wf <-\n  workflow() %>%\n  add_recipe(recipe(Sale_Price ~ First_Flr_SF, data = ames_train)) %>%\n  add_model(linear_reg())\n\n# generate bootstrap predictions on ames_test\nset.seed(713)\names_preds_boot <-\n  ames_wf %>%\n  predict_boots(\n    n = 2000,\n    training_data = ames_train,\n    new_data = ames_test\n  )\n```\n:::\n\n\n\n\n`predict_boots()` works by creating 2000 [bootstrap resamples](https://rsample.tidymodels.org/reference/bootstraps.html) of the training data, fitting a linear model to each resample, then generating 2000 predictions for each home's price in the holdout set. We can then use `summarise_predictions()` to generate upper and lower intervals for each prediction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_preds_boot %>%\n  summarise_predictions()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 733 × 5\n   rowid .preds               .pred .pred_lower .pred_upper\n   <int> <list>               <dbl>       <dbl>       <dbl>\n 1     1 <tibble [2,000 × 2]>  5.44        5.17        5.71\n 2     2 <tibble [2,000 × 2]>  5.27        4.98        5.55\n 3     3 <tibble [2,000 × 2]>  5.25        4.97        5.52\n 4     4 <tibble [2,000 × 2]>  5.40        5.11        5.67\n 5     5 <tibble [2,000 × 2]>  5.44        5.15        5.71\n 6     6 <tibble [2,000 × 2]>  5.21        4.93        5.49\n 7     7 <tibble [2,000 × 2]>  4.94        4.67        5.22\n 8     8 <tibble [2,000 × 2]>  5.13        4.85        5.40\n 9     9 <tibble [2,000 × 2]>  5.14        4.87        5.42\n10    10 <tibble [2,000 × 2]>  5.41        5.13        5.69\n# … with 723 more rows\n```\n:::\n:::\n\n\nBy overlaying the intervals on top of one another, we can see that the prediction interval generated by `predict_boots()` is a good approximation of the theoretical interval generated by `lm()`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\names_preds_boot %>%\n  summarise_predictions() %>%\n  bind_cols(ames_preds) %>%\n  bind_cols(ames_test) %>%\n  mutate(across(c(.pred:Sale_Price), ~10^.x)) %>%\n  ggplot(aes(x = First_Flr_SF)) +\n  geom_point(aes(y = Sale_Price),\n             alpha = 0.25) +\n  geom_line(aes(y = fit),\n            size = 1) +\n  geom_ribbon(aes(ymin = lwr,\n                  ymax = upr),\n              alpha = 0.25) +\n  geom_point(aes(y = .pred),\n             color = \"blue\",\n             alpha = 0.25) +\n  geom_errorbar(aes(ymin = .pred_lower,\n                    ymax = .pred_upper),\n                color = \"blue\",\n                alpha = 0.25,\n                width = 0.0125) +\n  scale_y_continuous(labels = scales::dollar_format(), trans = \"log10\") +\n  scale_x_continuous(labels = scales::comma_format(), trans = \"log10\") +\n  labs(title = \"Linear Model of Sale Price predicted by Square Footage\",\n       subtitle = \"Bootstrap prediction interval closely matches theoretical prediction interval\",\n       x = NULL,\n       y = NULL)\n```\n:::\n\n\n![](pics/plot_03.png)\n\nBoth `lm()` and `summarise_predictions()` use a 95% prediction interval by default but we can generate other intervals by passing different values to the parameter `conf`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_preds_boot %>%\n  \n  # generate 95% prediction interval\n  summarise_predictions(conf = 0.95) %>%\n  rename(.pred_lower_95 = .pred_lower,\n         .pred_upper_95 = .pred_upper) %>%\n  select(-.pred) %>%\n  \n  # generate 80% prediction interval\n  summarise_predictions(conf = 0.80) %>%\n  rename(.pred_lower_80 = .pred_lower,\n         .pred_upper_80 = .pred_upper) %>%\n  bind_cols(ames_test) %>%\n  mutate(across(c(.pred_lower_95:Sale_Price), ~10^.x)) %>%\n  \n  # plot!\n  ggplot(aes(x = First_Flr_SF)) +\n  geom_point(aes(y = Sale_Price),\n             alpha = 0.25) +\n  geom_line(aes(y = .pred),\n            size = 1,\n            color = \"blue\") +\n  geom_ribbon(aes(ymin = .pred_lower_95,\n                  ymax = .pred_upper_95),\n              alpha = 0.25,\n              fill = \"blue\") +\n  geom_ribbon(aes(ymin = .pred_lower_80,\n                  ymax = .pred_upper_80),\n              alpha = 0.25,\n              fill = \"blue\") +\n  scale_y_continuous(labels = scales::dollar_format(), trans = \"log10\") +\n  scale_x_continuous(labels = scales::comma_format(), trans = \"log10\") +\n  labs(title = \"Linear Model of Sale Price predicted by Square Footage\",\n       subtitle = \"Predictions alongside 95% and 80% bootstrap prediction interval\",\n       x = NULL,\n       y = NULL)\n```\n:::\n\n\n![](pics/plot_04.png)\n\nAs this example shows, `{workboots}` can approximate linear prediction intervals pretty well! But this isn't very useful, since we can just generate a linear prediction interval from a linear model directly. The real benefit of `{workboots}` comes from generating prediction intervals from *any* model!\n\n## Bootstrap prediction intervals with non-linear models\n\n[XGBoost](https://xgboost.readthedocs.io/en/stable/) is one of my favorite models. Up until now, however, in situations that require a prediction interval, I've had to opt for a simpler model. With `{workboots}`, that's no longer an issue! In this example, we'll use XGBoost and `{workboots}` to generate predictions of a penguins weight from the [Palmer Penguins dataset](https://modeldata.tidymodels.org/reference/penguins.html).\n\nTo get started, let's build a workflow and train an individual model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load and prep data\ndata(\"penguins\")\n\npenguins <-\n  penguins %>%\n  drop_na()\n\n# split data into training and testing sets\nset.seed(123)\npenguins_split <- initial_split(penguins)\npenguins_test <- testing(penguins_split)\npenguins_train <- training(penguins_split)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a workflow\npenguins_wf <-\n  workflow() %>%\n  \n  # add preprocessing steps\n  add_recipe(\n    recipe(body_mass_g ~ ., data = penguins_train) %>%\n      step_dummy(all_nominal_predictors()) \n  ) %>%\n  \n  # add xgboost model spec\n  add_model(\n    boost_tree(\"regression\")\n  )\n\n# fit to training data & predict on test data\nset.seed(234)\npenguins_preds <-\n  penguins_wf %>%\n  fit(penguins_train) %>%\n  predict(penguins_test)\n```\n:::\n\n\nAs mentioned above, XGBoost models can only generate point predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins_preds %>%\n  bind_cols(penguins_test) %>%\n  ggplot(aes(x = body_mass_g,\n             y = .pred)) +\n  geom_point() +\n  geom_abline(linetype = \"dashed\",\n              color = \"gray\") +\n  labs(title = \"XGBoost Model of Penguin Weight\",\n       subtitle = \"Individual model can only output individual predictions\")\n```\n:::\n\n\n![](pics/plot_05.png)\n\nWith `{workboots}`, however, we can generate a prediction interval from our XGBoost model for each penguin's weight!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create 2000 models from bootstrap resamples and make predictions on the test set\nset.seed(345)\npenguins_preds_boot <-\n  penguins_wf %>%\n  predict_boots(\n    n = 2000,\n    training_data = penguins_train,\n    new_data = penguins_test\n  )\n\npenguins_preds_boot %>%\n  summarise_predictions()\n```\n:::\n\n\n\n\nHow does our bootstrap model perform?\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins_preds_boot %>%\n  summarise_predictions() %>%\n  bind_cols(penguins_test) %>%\n  ggplot(aes(x = body_mass_g,\n             y = .pred,\n             ymin = .pred_lower,\n             ymax = .pred_upper)) +\n  geom_abline(linetype = \"dashed\",\n              color = \"gray\") +\n  geom_errorbar(alpha = 0.5,\n                color = \"blue\") +\n  geom_point(alpha = 0.5,\n             color = \"blue\") +\n  labs(title = \"XGBoost Model of Penguin Weight\",\n       subtitle = \"Bootstrap models can generate prediction intervals\")\n```\n:::\n\n\n![](pics/plot_06.png)\n\nThis particular model may be in need of some tuning for better performance, but the important takeaway is that we were able to generate a prediction distribution for the model! This method works with other regression models as well --- just create a workflow then let `{workboots}` take care of the rest!\n\n# Tidymodel Resources\n\n* [Getting Started with Tidymodels](https://www.tidymodels.org/start/)\n* [Tidy Modeling with R](https://www.tmwr.org/)\n* [Julia Silge's Blog](https://juliasilge.com/blog/) provides use cases of tidymodels with weekly [#tidytuesday](https://github.com/rfordatascience/tidytuesday) datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}