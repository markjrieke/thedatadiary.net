{
  "hash": "57f0fb98d510661a60afc7fa41bbc79b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Sufficiently Normal\"\ndate: '2026-02-19'\ncategories: [stan, sufficiency]\ndescription: \"Oops I've been obsessed with sufficiency lately\"\nimage: header.png\nfilters:\n  - add-code-files\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(ggdist)\nlibrary(riekelib)\n\n# Setup a color palette to be used for groups A/B\npal <- NatParksPalettes::NatParksPalettes$Acadia[[1]][c(2,8)]\n```\n:::\n\n\nAs a somewhat unexpected follow-up to my [previous post on the sufficient gamma](../2026-02-15-sufficient-gamma/index.qmd), I've decided to take a similarly motivated look at implementing the sufficient normal. I'd previously [implemented](https://github.com/markjrieke/2025-march-madness/blob/main/stan/recovery.stan) the sufficient normal as a part of my [March Madness predictions](https://www.thedatadiary.net/projects/2025-march-madness/mens) last year,^[My current employer is a subsidiary of LV Sands, so it is unlikely that I put together a model for March Madness this year.] but if I'm being honest, I got that working without getting an intuition for _why_ it worked. That's bugged me for the better part of a year, so here we are.\n\nLike last time, if you want to skip the derivation and just get right to implementation, here's a sufficient normal density function that you can drop right at the top of your Stan model:\n\n```stan\nreal sufficient_normal_lpdf(\n  real Xsum,\n  real Xsumsq,\n  real n,\n  real mu,\n  real sigma\n) {\n  real lp = 0.0;\n  real s2 = sigma^2;\n  real d = (2 * s2)^(-1);\n  lp += -n * 0.5 * log(2 * pi() * s2);\n  lp += -d * Xsumsq;\n  lp += 2 * d * mu * Xsum;\n  lp += -n * d * mu^2;\n  return lp;\n}\n```\n\n## Notably Normal\n\nI'll simulate 5,000 observations from two normally distributed groups for a total of 10,000 observations in the dataset. I'll then fit two models to these 10,000 observations --- one making use of Stan's built-in normal likelihood and another with our custom sufficient normal density function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2026)\nnormal_data <-\n  tibble(group = c(\"A\", \"B\"),\n         mu = c(0, 2),\n         sigma = c(1, 0.5)) %>%\n  mutate(y = pmap(list(mu, sigma), ~rnorm(5000, ..1, ..2)))\n\nnormal_data %>%\n  unnest(y) %>%\n  ggplot(aes(x = y,\n             y = group,\n             fill = group)) +\n  stat_histinterval(breaks = 60,\n                    slab_alpha = 0.75) +\n  scale_fill_manual(values = pal) +\n  theme_rieke()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=2700}\n:::\n:::\n\n\nWe'll fit a simple normal likelihood to these observations where each group, $g$, has a separate mean, $\\mu$, and standard deviation, $\\sigma$. Unsurprisingly, this model fits with no issues.\n\n$$\n\\begin{align*}\ny_i &\\sim \\mathcal{N}(\\mu_g, \\sigma_g) \\\\\n\\mu_g &\\sim \\mathcal{N}(0, 5) \\\\\n\\sigma_g &\\sim \\text{Exponential}(1)\n\\end{align*}\n$$\n\n\n::: {.cell add-from='individual-normal.stan' source-lang='stan'}\n\n```{.r .cell-code  code-summary=\"Stan Model\"}\n# comment\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindividual_model <- cmdstan_model(\"individual-normal.stan\")\n\nmodel_data <-\n  normal_data %>%\n  mutate(gid = rank(group)) %>%\n  unnest(y)\n\nstan_data <-\n  list(\n    N = nrow(model_data),\n    Y = model_data$y,\n    G = max(model_data$gid),\n    gid = model_data$gid,\n    mu_mu = 0,\n    sigma_mu = 5,\n    lambda_sigma = 1\n  )\n\nstart_time <- Sys.time()\nindividual_fit <-\n  individual_model$sample(\n    data = stan_data,\n    seed = 1234,\n    iter_warmup = 1000,\n    iter_sampling = 1000,\n    chains = 10,\n    parallel_chains = 10\n  )\nrun_time_individual <- as.numeric(Sys.time() - start_time)\n\nplot_posterior <- function(fit) {\n  \n  # Get the true parameters used in simulation\n  truth <-\n    normal_data %>%\n    select(group, mu, sigma) %>%\n    pivot_longer(-group,\n                 names_to = \"parameter\",\n                 values_to = \"draw\") %>%\n    mutate(parameter = if_else(parameter == \"mu\", \"\\u03bc\", \"\\u03c3\"))\n  \n  fit$draws(c(\"mu\", \"sigma\"), format = \"df\") %>%\n    as_tibble() %>%\n    select(starts_with(\"mu\"), starts_with(\"sigma\")) %>%\n    pivot_longer(everything(),\n                 names_to = \"parameter\",\n                 values_to = \"draw\") %>%\n    nest(data = -parameter) %>%\n    separate(parameter, c(\"parameter\", \"group\"), \"\\\\[\") %>%\n    mutate(group = str_remove(group, \"\\\\]\"),\n           group = if_else(group == \"1\", \"A\", \"B\"),\n           parameter = if_else(parameter == \"mu\", \"\\u03bc\", \"\\u03c3\")) %>%\n    unnest(data) %>%\n    ggplot(aes(x = draw,\n               y = parameter,\n               fill = group)) +\n    stat_histinterval(breaks = 60,\n                      slab_alpha = 0.75) +\n    geom_point(data = truth,\n               color = \"red\") +\n    scale_fill_manual(values = pal) +\n    theme_rieke()\n  \n}\n\nplot_posterior(individual_fit) +\n  labs(title = \"**Fit against individual observations**\",\n       subtitle = \"Can be used as a baseline for evaluating the sufficient formulation\",\n       x = NULL,\n       y = NULL)\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=2700}\n:::\n:::\n\n\nWhile it only takes **<span style='color:royalblue'>4.1 seconds</span>** to fit this model,^[As an aside, _wow_ Stan's implementation of the normal likelihood is incredibly fast.] we have some room for improvement by summarizing the dataset with a sufficient formulation.\n\n## Density Derivations\n\nWhen fitting a model, we repeatedly multiply the probability density for each individual observation to get a total probability density for the entire set of observations. A sufficient exploit would allow us to calculate this _product_ in one step, rather than repeatedly calculating the individual probability densities.\n\n$$\n\\begin{align*}\n\\Pr(y_1,y_2,\\dots,y_n) &= \\Pr(y_1)\\Pr(y_2)\\dots\\Pr(y_n) \\\\\n\\Pr(y) &= \\prod_i^N \\Pr(y_i)\n\\end{align*}\n$$\n\nThe probability density of the normal distribution with mean $\\mu$ and standard deviation $\\sigma$ for a single observation $y_i$ is:\n\n$$\n\\Pr(y_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{\\frac{1}{2\\sigma^2}(y_i-\\mu)^2}\n$$\n\nOur goal is to find a variant of this density function that allows us to evaluate $\\prod\\Pr(y)$ given summary information from $y$, rather than individual observations. Bayesian models evaluate log probabilities and it's generally easier to work on the log scale --- the log of the normal density function for a single observation is:\n\n$$\n\\log(\\Pr(y_i)) = -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(y_i-\\mu)^2}{2\\sigma^2}\n$$\n\nWe want to setup $y_i$ in such a way that we can easily replace it with summary statistics when we move from a single observation to multiple observations. Here, that means we need to expand $(y_i-\\mu)^2$.\n\n$$\n\\begin{align*}\n\\log(\\Pr(y_i)) &= -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{y_i^2 - 2\\mu y_i + \\mu^2}{2\\sigma^2} \\\\\n&= -\\frac{1}{2}\\log(2\\pi\\sigma^2) - (2\\sigma^2)^{-1}(y_i^2 - 2\\mu y_i + \\mu^2)\n\\end{align*}\n$$\n\nThe last step of this expansion is to multiply $(2\\sigma^2)^{-1}$ by each of the quadratic terms. For legibility's sake (and to save some keystrokes), I set $d=(2\\sigma^2)^{-1}$. We end up with the following equation for the log-probability of a single observation, $y_i$.\n\n$$\n\\begin{align*}\n\\log(\\Pr(y_i)) &= -\\frac{1}{2}\\log(2\\pi\\sigma^2) - dy_i^2 + 2d\\mu y_i - d\\mu^2 \\\\\n\\end{align*}\n$$\n\nThe simplest case beyond a single observation is to consider two observations. Because we're on the log scale we can simply add the two together to get the log of the product.\n\n$$\n\\begin{align*}\n\\log(\\Pr(y_1,y_2)) = &- \\frac{1}{2}\\log(2\\pi\\sigma^2) - dy_1^2 + 2d\\mu y_1 - d\\mu^2 \\\\\n&-\\frac{1}{2}\\log(2\\pi\\sigma^2) - dy_2^2 + 2d\\mu y_2 - d\\mu^2\n\\end{align*}\n$$\n\nThis framing makes it clear how this product expands to $n$ observations. For each observation, we have another set of $\\frac{1}{2}\\log(2\\pi\\sigma^2)$ and $d\\mu^2$ terms, so we can simply multiply these terms by $n$. Each $y_i^2$ term is multiplied by $d$, so we replace $d(y_1^2+y_2^2\\dots+y_n^2)$ with $d\\sum y^2$ (and can do the same operation to get to $2d\\mu\\sum y$). Altogether, we arrive at a new log probability function based on the count, sum, and sum of square terms in $y$.\n\n$$\n\\log(\\Pr(y)) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - d\\sum y^2 + 2d\\mu\\sum y - nd\\mu^2\n$$\n\nThe log probability is what Stan uses during sampling and is how I defined the function in the intro of this article. For completeness' sake, we can unlog to show that the probability density of a normally distributed set of observations, $y$, has a sufficient formulation given the count, sum, and sum of squares in $y$.\n\n$$\n\\Pr\\left(\\sum y^2, \\sum y, n | \\mu, \\sigma\\right) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}} e^{(-2\\sigma^2)^{-1}\\left(\\sum y^2 - 2\\mu\\sum y + n\\mu^2\\right)}\n$$\n\n## Sufficient Speedups\n\nWith this sufficient formulation, we can collapse our original 10,000 row dataset into just two rows --- one per group.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsufficient_model <- cmdstan_model(\"sufficient-normal.stan\")\n\nmodel_data <-\n  normal_data %>%\n  mutate(gid = rank(group),\n         n = map_int(y, length),\n         Ysum = map_dbl(y, sum),\n         Ysumsq = map_dbl(y, ~sum(.x^2)))\n\nmodel_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 2 Ã— 8\n#>   group    mu sigma y               gid     n    Ysum Ysumsq\n#>   <chr> <dbl> <dbl> <list>        <dbl> <int>   <dbl>  <dbl>\n#> 1 A         0   1   <dbl [5,000]>     1  5000   -40.3  4927.\n#> 2 B         2   0.5 <dbl [5,000]>     2  5000 10039.  21429.\n```\n\n\n:::\n:::\n\n\nWe can keep the same priors and just swap out our original likelihood function for our new sufficient likelihood.\n\n$$\n\\begin{align*}\n\\sum y^2, \\sum y, n &\\sim \\text{Sufficient-Normal}(\\mu_g, \\sigma_g) \\\\\n\\mu_g &\\sim \\mathcal{N}(0, 5) \\\\\n\\sigma_g &\\sim \\text{Exponential}(1)\n\\end{align*}\n$$\n\n\n::: {.cell add-from='sufficient-normal.stan' source-lang='stan'}\n\n```{.r .cell-code  code-summary=\"Stan Model\"}\n# comment\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_data <-\n  list(\n    N = nrow(model_data),\n    Ysum = model_data$Ysum,\n    Ysumsq = model_data$Ysumsq,\n    n_obs = model_data$n,\n    G = max(model_data$gid),\n    gid = model_data$gid,\n    mu_mu = 0,\n    sigma_mu = 5,\n    lambda_sigma = 1\n  )\n\nstart_time <- Sys.time()\nsufficient_fit <-\n  sufficient_model$sample(\n    data = stan_data,\n    seed = 1234,\n    iter_warmup = 1000,\n    iter_sampling = 1000,\n    chains = 10,\n    parallel_chains = 10\n  )\nrun_time_sufficient <- as.numeric(Sys.time() - start_time)\n```\n:::\n\n\nFitting the sufficient formulation to the same dataset only takes **<span style='color:royalblue'>0.7 seconds</span>**, a **<span style='color:royalblue'>5.6x</span>** speedup compared to the fit against individual observations.^[Because the normal likelihood in Stan is already so optimized, this isn't as large of a speedup as we saw in the sufficient gamma. But in both cases, the speedup from sufficiency is largely determined by the original dataset's size. For example, fitting the original model to 200,000 individual observations takes ~90s, whereas it still only takes ~0.7 in the sufficient model.] And we recover the same posterior that we see in the fit against individual observations!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_posterior(sufficient_fit)  +\n  labs(title = \"**Fit using the sufficient normal**\",\n       subtitle = \"Recovers the same posterior from the fit to individual observations!\",\n       x = NULL,\n       y = NULL)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=2700}\n:::\n:::\n\n\nOnce again, exploit sufficiency, go zoom zoom. I simply ~~must be~~ **cannot be** stopped.\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}