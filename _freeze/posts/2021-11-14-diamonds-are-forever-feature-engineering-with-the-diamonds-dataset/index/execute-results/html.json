{
  "hash": "c376d879f166e293b2facb2242f7e0b4",
  "result": {
    "markdown": "---\ntitle: 'Diamonds are Forever'\ndescription: Feature Engineering with the Diamonds Dataset\ndate: '2021-11-14'\ncategories: [tidymodels]\nimage: featured.png\ncode-fold: show\n---\n\n\nAre y'all ready for some charts?? This week, I did a bit of machine learning practice with the [`diamonds dataset`](https://ggplot2.tidyverse.org/reference/diamonds.html). This dataset is interesting and good for practice for a few reasons:\n\n* there are lots of observations (50,000+);\n* it includes a mix of numeric and categorical variables;\n* there are some data oddities to deal with (log scales, interactions, non-linear relations)\n\nI'll be doing a bit of feature engineering prior to fitting an tuning a linear model that predicts the each diamond's `price` with the [`glmnet`](https://glmnet.stanford.edu/index.html) package. This will give a good end-to-end glimpse into the data exploration and model fitting process! Before we get into that, let's load some packages and get a preview of the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(vip)\n\ntheme_set(theme_minimal())\n\ndiamonds %>%\n  slice_head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 10\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n```\n:::\n:::\n\n\nSince we're predicting price, let's look at its distribution first.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds %>%\n  ggplot(aes(x = price)) +\n  geom_histogram()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=2100}\n:::\n:::\n\n\nWe're definitely gonna want to apply a transformation to the price when modeling - let's look at the distribution on a log-10 scale.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds %>%\n  ggplot(aes(x = price)) +\n  geom_histogram() +\n  scale_x_log10()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=2100}\n:::\n:::\n\n\nThat's a lot more evenly distributed, if not perfect. That's a fine starting point, so now we'll look through the rest of the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds %>%\n  ggplot(aes(x = carat)) +\n  geom_histogram()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=2100}\n:::\n\n```{.r .cell-code}\ndiamonds %>%\n  ggplot(aes(x = cut,\n             y = price)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-2.png){width=2100}\n:::\n\n```{.r .cell-code}\ndiamonds %>%\n  count(cut) %>%\n  ggplot(aes(x = cut,\n             y = n)) +\n  geom_col()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-3.png){width=2100}\n:::\n\n```{.r .cell-code}\ndiamonds %>%\n  ggplot(aes(x = color,\n             y = price)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-4.png){width=2100}\n:::\n\n```{.r .cell-code}\ndiamonds %>%\n  count(color) %>%\n  ggplot(aes(x = color,\n             y = n)) +\n  geom_col()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-5.png){width=2100}\n:::\n\n```{.r .cell-code}\ndiamonds %>%\n  ggplot(aes(x = clarity,\n             y = price)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-6.png){width=2100}\n:::\n\n```{.r .cell-code}\ndiamonds %>%\n  count(clarity) %>%\n  ggplot(aes(x = clarity,\n             y = n)) +\n  geom_col()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-7.png){width=2100}\n:::\n\n```{.r .cell-code}\ndiamonds %>%\n  ggplot(aes(x = depth)) +\n  geom_histogram()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-8.png){width=2100}\n:::\n\n```{.r .cell-code}\ndiamonds %>%\n  ggplot(aes(x = table)) +\n  geom_histogram() \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-9.png){width=2100}\n:::\n\n```{.r .cell-code}\ndiamonds %>%\n  ggplot(aes(x = x)) +\n  geom_histogram()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-10.png){width=2100}\n:::\n\n```{.r .cell-code}\ndiamonds %>%\n  ggplot(aes(x = y)) +\n  geom_histogram() \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-11.png){width=2100}\n:::\n\n```{.r .cell-code}\ndiamonds %>% \n  ggplot(aes(x = z)) +\n  geom_histogram()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-12.png){width=2100}\n:::\n:::\n\n\nIt looks like there may be a good opportunity to try out a few normalization and resampling techniques, but before we get into any of that, let's build a baseline linear model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# splits\ndiamonds_split <- initial_split(diamonds)\ndiamonds_train <- training(diamonds_split)\ndiamonds_test <- testing(diamonds_split)\n\n# resamples (don't want to use testing data!)\ndiamonds_folds <- vfold_cv(diamonds_train)\n\n# model spec\nmod01 <-\n  linear_reg() %>%\n  set_engine(\"lm\")\n\n# recipe\nrec01 <-\n  recipe(price ~ ., data = diamonds_train) %>%\n  step_dummy(all_nominal_predictors())\n\n# controls\nctrl_preds <- \n  control_resamples(save_pred = TRUE)\n\n# create a wf\nwf01 <-\n  workflow() %>%\n  add_model(mod01) %>%\n  add_recipe(rec01)\n\n# parallel processing\ndoParallel::registerDoParallel()\n\n# fit\nrs01 <- \n  fit_resamples(\n    wf01,\n    diamonds_folds,\n    control = ctrl_preds\n  )\n\n# metrics!\ncollect_metrics(rs01)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  .metric .estimator     mean     n std_err .config             \n  <chr>   <chr>         <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   1135.       10 8.10    Preprocessor1_Model1\n2 rsq     standard      0.919    10 0.00129 Preprocessor1_Model1\n```\n:::\n:::\n\n\nAnd right off the bat, we can see a fairly high value for `rsq`! However, `rsq` doesn't tell the whole story, so we should check our predictions and residuals plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(rs01) %>%\n  ggplot(aes(x = price,\n             y = .pred)) +\n  geom_point(alpha = 0.01) +\n  geom_abline(linetype = \"dashed\",\n              size = 0.1,\n              alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=2100}\n:::\n:::\n\n\nThis is *definitely* not what we want to see! It looks like there's an odd curve/structure to the graph and we're actually predicting quite a few negative values. The residuals plot doesn't look too great either.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(rs01) %>%\n  ggplot(aes(x = price,\n             y = .resid)) +\n  geom_point(alpha = 0.01) +\n  geom_hline(yintercept = 0,\n             linetype = \"dashed\",\n             alpha = 0.5,\n             size = 0.1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=2100}\n:::\n:::\n\n\nWhat we'd like to see is a 0-correlation plot with errors normally distributed; what we're seeing instead, however, is a ton of structure. \n\nThat being said, that's okay! we expected this first pass to be pretty rough! And the price is *clearly* on a log-10 scale. To make apples-apples comparisons with models going forward, I'll retrain this basic linear model to predict the `log10(price)`. This'll involve a bit of data re-manipulation!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# log transform price\ndiamonds_model <-\n  diamonds %>%\n  mutate(price = log10(price),\n         across(cut:clarity, as.character))\n\n# bad practice copy + paste lol\n\n# splits\nset.seed(999)\ndiamonds_split <- initial_split(diamonds_model)\ndiamonds_train <- training(diamonds_split)\ndiamonds_test <- testing(diamonds_split)\n\n# resamples (don't want to use testing data!)\nset.seed(888)\ndiamonds_folds <- vfold_cv(diamonds_train)\n\n# model spec\nmod01 <-\n  linear_reg() %>%\n  set_engine(\"lm\")\n\n# recipe\nrec01 <-\n  recipe(price ~ ., data = diamonds_train) %>%\n  step_dummy(all_nominal_predictors())\n\n# controls\nctrl_preds <- \n  control_resamples(save_pred = TRUE)\n\n# create a wf\nwf01 <-\n  workflow() %>%\n  add_model(mod01) %>%\n  add_recipe(rec01)\n\n# parallel processing\ndoParallel::registerDoParallel()\n\n# fit\nset.seed(777)\nrs01 <- \n  fit_resamples(\n    wf01,\n    diamonds_folds,\n    control = ctrl_preds\n  )\n\n# metrics!\ncollect_metrics(rs01)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.0793    10 0.00557 Preprocessor1_Model1\n2 rsq     standard   0.966     10 0.00494 Preprocessor1_Model1\n```\n:::\n:::\n\n\nAnd wow, that *one* transformation increased our `rsq` to 0.96! Again, that's not the whole story, and we're going to be evaluating models based on the `rmse`. Let's look at how our prediction map has updated:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrs01 %>%\n  augment() %>%\n  ggplot(aes(x = price,\n             y = .pred)) +\n  geom_point(alpha = 0.01) +\n  geom_abline(linetype = \"dashed\",\n              size = 0.1,\n              alpha = 0.5) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=2100}\n:::\n:::\n\n\nNow *that* is a much better starting place to be at! Let's look at our coefficients\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(666) # :thedevilisalive:\nwf01 %>%\n  fit(diamonds_train) %>%\n  pull_workflow_fit() %>%\n  vip::vi() %>%\n  mutate(Variable = fct_reorder(Variable, Importance)) %>%\n  ggplot(aes(x = Variable,\n             y = Importance,\n             fill = Sign)) +\n  geom_col() +\n  coord_flip() + \n  theme(plot.title.position = \"plot\") +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Diamonds are forever\",\n       subtitle = \"Variable importance plot of a basic linear regression predicting diamond price\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=2100}\n:::\n:::\n\n\nAnother way of looking at it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(666)\nwf01 %>%\n  fit(diamonds_train) %>%\n  pull_workflow_fit() %>%\n  vip::vi() %>%\n  mutate(Importance = if_else(Sign == \"NEG\", Importance * -1, Importance),\n         Variable = fct_reorder(Variable, Importance)) %>%\n  ggplot(aes(x = Variable,\n             y = Importance,\n             fill = Sign)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Diamonds are forever\",\n       subtitle = \"Variable importance plot of a basic linear regression predicting diamond price\",\n       x = NULL,\n       y = NULL) +\n  theme(plot.title.position = \"plot\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=2100}\n:::\n:::\n\n\nThis is a good, but definitely improvable, starting point. We can likely decrease our overall error with a bit of feature engineering and drop unimportant features by tuning a regularized model. There are some oddities in this initial model that will need to be improved upon; for one, we can definitively say that the `carat` feature ought to be *positively* associated with price\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds_train %>%\n  ggplot(aes(x = carat,\n             y = price)) +\n  geom_point(alpha = 0.01) +\n  labs(title = \"A clear positive (albeit nonlinear) relationship between `carat` and `price`\") +\n  theme(plot.title.position = \"plot\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=2100}\n:::\n:::\n\n\nAnother few things that are interesting to note in this plot! It looks like there are clusterings of carat ratings around round-ish numbers. My hypothesis here is that carat ratings tend to get rounded up to the next size. There's also a clear abscence of diamonds priced at \\$1,500 (~3.17 on the log10 scale). I suppose there is some industry-specific reason to avoid a diamond price of \\$,1500? \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds_train %>%\n  ggplot(aes(x = carat,\n             y = price)) +\n  geom_point(alpha = 0.01) +\n  labs(title = \"A clear positive (albeit nonlinear) relationship between `carat` and `price`\") +\n  theme(plot.title.position = \"plot\") +\n  geom_hline(yintercept = log10(1500),\n             linetype = \"dashed\",\n             size = 0.9,\n             alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=2100}\n:::\n:::\n\n\nHow to address all these things? With some feature engineering! Firstly, let's add some recipe steps to balance classes & normalize continuous variables. \n\nBut before I get into *that*, I'll save the resample metrics so that we can compare models!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrics <- collect_metrics(rs01) %>% mutate(model = \"model01\")\n\nmetrics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 7\n  .metric .estimator   mean     n std_err .config              model  \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>                <chr>  \n1 rmse    standard   0.0793    10 0.00557 Preprocessor1_Model1 model01\n2 rsq     standard   0.966     10 0.00494 Preprocessor1_Model1 model01\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# spec will be the same as model01\nmod02 <- mod01\n\n# recipe!\nrec02 <- \n  recipe(price ~ ., data = diamonds_train) %>%\n  step_other(cut, color, clarity) %>% \n  step_dummy(all_nominal_predictors(), -cut) %>%\n  \n  # use smote resampling to balance classes\n  themis::step_smote(cut) %>% \n    \n  # normalize continuous vars\n  bestNormalize::step_best_normalize(carat, depth, table, x, y, z)\n```\n:::\n\n\nLet's [bake](https://recipes.tidymodels.org/reference/bake.html) our recipe to verify that everything looks up-to-snuff in the preprocessed dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbaked_rec02 <- \n  rec02 %>%\n  prep() %>%\n  bake(new_data = NULL)\n\nbaked_rec02\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 80,495 × 20\n     carat cut        depth  table       x       y      z price color_E color_F\n     <dbl> <fct>      <dbl>  <dbl>   <dbl>   <dbl>  <dbl> <dbl>   <dbl>   <dbl>\n 1 -0.706  Premium    0.138 -0.760 -0.709  -0.738  -0.695  3.01       0       1\n 2  0.356  Very Good  0.570  0.835  0.342   0.251   0.344  3.63       0       0\n 3  0.214  Premium   -0.308  0.835  0.293   0.263   0.166  3.58       0       0\n 4 -1.08   other      1.04  -0.310 -1.30   -1.40   -0.995  2.70       0       0\n 5 -0.641  Ideal     -0.602 -0.760 -0.595  -0.560  -0.622  2.97       0       0\n 6 -0.0759 Premium   -0.602  0.494 -0.0349 -0.0460 -0.114  3.38       0       0\n 7 -0.149  Premium   -1.16   0.103 -0.0565 -0.0842 -0.246  3.44       1       0\n 8  0.170  Very Good -0.371  0.494  0.178   0.313   0.130  3.56       0       1\n 9 -0.736  Ideal     -0.110 -0.760 -0.709  -0.738  -0.723  3.09       0       0\n10  0.782  Ideal     -0.602 -0.310  0.819   0.846   0.732  4.02       0       0\n# … with 80,485 more rows, and 10 more variables: color_G <dbl>, color_H <dbl>,\n#   color_I <dbl>, color_J <dbl>, clarity_SI2 <dbl>, clarity_VS1 <dbl>,\n#   clarity_VS2 <dbl>, clarity_VVS1 <dbl>, clarity_VVS2 <dbl>,\n#   clarity_other <dbl>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbaked_rec02 %>%\n  count(cut) %>%\n  ggplot(aes(x = cut,\n             y = n)) +\n  geom_col()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=2100}\n:::\n\n```{.r .cell-code}\nbaked_rec02 %>%\n  ggplot(aes(x = carat)) +\n  geom_histogram()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-2.png){width=2100}\n:::\n\n```{.r .cell-code}\nbaked_rec02 %>%\n  ggplot(aes(x = depth)) +\n  geom_histogram()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-3.png){width=2100}\n:::\n\n```{.r .cell-code}\nbaked_rec02 %>%\n  ggplot(aes(x = table)) +\n  geom_histogram()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-4.png){width=2100}\n:::\n\n```{.r .cell-code}\nbaked_rec02 %>%\n  ggplot(aes(x = x)) +\n  geom_histogram()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-5.png){width=2100}\n:::\n\n```{.r .cell-code}\nbaked_rec02 %>%\n  ggplot(aes(x = y)) +\n  geom_histogram() \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-6.png){width=2100}\n:::\n\n```{.r .cell-code}\nbaked_rec02 %>%\n  ggplot(aes(x = z)) +\n  geom_histogram()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-7.png){width=2100}\n:::\n:::\n\n\nEverything looks alright with the exception of the `table` predictor. I wonder if there are a lot of repeated values in the `table` variable - that may be why we're seeing a \"chunky\" histogram. Let's check\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbaked_rec02 %>%\n  count(table) %>%\n  arrange(desc(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10,406 × 2\n    table     n\n    <dbl> <int>\n 1  0.103 12167\n 2 -0.310 11408\n 3 -0.760 11031\n 4  0.494  9406\n 5 -1.28   6726\n 6  0.835  6165\n 7  1.15   3810\n 8 -1.85   2789\n 9  1.42   2182\n10  1.64    972\n# … with 10,396 more rows\n```\n:::\n:::\n\nOoh - okay yeah that's definitely the issue! I'm not *quite* sure how to deal with it, so we're just going to ignore for now! Let's add a new model & see how it compares against the baseline transformed model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf02 <-\n  workflow() %>%\n  add_model(mod02) %>%\n  add_recipe(rec02)\n\n# stop parallel to avoid error!\n# need to replace with PSOCK clusters\n# see github issue here: https://github.com/tidymodels/recipes/issues/847\nforeach::registerDoSEQ()\n\nset.seed(666) # spoopy\nrs02 <-\n  fit_resamples(\n    wf02,\n    diamonds_folds,\n    control = ctrl_preds\n  )\n\ncollect_metrics(rs02)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.115    10 0.00143 Preprocessor1_Model1\n2 rsq     standard   0.932    10 0.00161 Preprocessor1_Model1\n```\n:::\n:::\n\n\nOof - that's actually slightly worse than our baseline model!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrs02 %>%\n  augment() %>%\n  ggplot(aes(x = price,\n             y = .pred)) +\n  geom_point(alpha = 0.01) +\n  geom_abline(linetype = \"dashed\",\n              size = 0.1,\n              alpha = 0.5) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=2100}\n:::\n:::\n\n\nIt looks like we've introduced structure into the residual plot! \n\n\n::: {.cell}\n\n```{.r .cell-code}\nrs02 %>%\n  augment() %>%\n  ggplot(aes(x = price,\n             y = .resid)) +\n  geom_point(alpha = 0.01) +\n  geom_hline(yintercept = 0,\n             linetype = \"dashed\",\n             size = 0.1,\n             alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=2100}\n:::\n:::\n\n\nYeah that's fairly wonky! I'm wondering if it's due to the SMOTE upsampling method we introduced? To counteract, I'll build & train new models after each set of recipe steps (e.g., resampling, normalizing, interactions) to buil up a better performing model one step at a time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrics <- \n  metrics %>%\n  bind_rows(collect_metrics(rs02) %>% mutate(model = \"model02\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# same model spec\nmod03 <- mod02\n\n# rebuild rec+wf & retrain\nrec03 <- \n  recipe(price ~ ., data = diamonds_train) %>%\n  step_other(cut, color, clarity) %>%\n  step_dummy(all_nominal_predictors(), -cut) %>%\n  themis::step_smote(cut)\n\nwf03 <- \n  workflow() %>%\n  add_model(mod03) %>%\n  add_recipe(rec03)\n\n# do paralllel\ndoParallel::registerDoParallel()\n\n# refit!\nset.seed(123)\nrs03 <-\n  fit_resamples(\n    wf03,\n    diamonds_folds,\n    control = ctrl_preds\n  )\n\ncollect_metrics(rs03)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.0918    10 0.00502 Preprocessor1_Model1\n2 rsq     standard   0.956     10 0.00502 Preprocessor1_Model1\n```\n:::\n:::\n\n\nInteresting! Improved relative to `rs02`, but still not as good as our first model! Let's try using `step_downsample()` to balance classes & see how we fare.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cleanup some large-ish items eating up memory\nrm(mod01, mod02, rec01, rec02, wf01, wf02, rs01, rs02)\n\n# save metrics\nmetrics <- \n  metrics %>%\n  bind_rows(collect_metrics(rs03) %>% mutate(model = \"model03\"))\n\n# new mod\nmod04 <- mod03\n\n# new rec\nrec04 <-\n  recipe(price ~ ., data = diamonds_train) %>%\n  step_other(cut, color, clarity) %>%\n  step_dummy(all_nominal_predictors(), -cut) %>%\n  themis::step_downsample(cut)\n\nwf04 <-\n  workflow() %>%\n  add_model(mod04) %>%\n  add_recipe(rec04) \n\nset.seed(456) \nrs04 <-\n  fit_resamples(\n    wf04,\n    diamonds_folds,\n    control = ctrl_preds\n  )\n\ncollect_metrics(rs04)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.113    10  0.0120 Preprocessor1_Model1\n2 rsq     standard   0.930    10  0.0153 Preprocessor1_Model1\n```\n:::\n:::\n\n\nWow - still a bit worse! I'll try upsampling & if there is no improvement, we'll move on without resampling!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrics <-\n  metrics %>%\n  bind_rows(collect_metrics(rs04) %>% mutate(model = \"model04\"))\n\nmod05 <- mod04\n\nrec05 <-\n  recipe(price ~ ., data = diamonds_train) %>%\n  step_other(cut, color, clarity) %>%\n  step_dummy(all_nominal_predictors(), -cut) %>%\n  themis::step_upsample(cut)\n\nwf05 <- \n  workflow() %>%\n  add_model(mod05) %>%\n  add_recipe(rec05) \n\nset.seed(789)\nrs05 <-\n  fit_resamples(\n    wf05,\n    diamonds_folds,\n    control = ctrl_preds\n  )\n\ncollect_metrics(rs05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.101    10 0.00486 Preprocessor1_Model1\n2 rsq     standard   0.946    10 0.00527 Preprocessor1_Model1\n```\n:::\n:::\n\n\nOkay - resampling gets stricken off our list of recipe steps! Let's look at how the models compare so far\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrics <-\n  metrics %>%\n  bind_rows(collect_metrics(rs05) %>% mutate(model = \"model05\"))\n\nmetrics %>%\n  ggplot(aes(x = model)) +\n  geom_point(aes(y = mean)) +\n  geom_errorbar(aes(ymin = mean - std_err,\n                    ymax = mean + std_err)) +\n  facet_wrap(~.metric, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-26-1.png){width=2100}\n:::\n:::\n\n\nThe first simple linear model was the best as measured by both metrics! Let's see if we can improve with some normalization of the continuous vars.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(mod03, mod04, rec03, rec04, rs03, rs04, wf03, wf04)\n\nmod06 <- mod05\n\nrec06 <-\n  recipe(price ~ ., data = diamonds_train) %>%\n  step_other(cut, color, clarity) %>%\n  bestNormalize::step_best_normalize(all_numeric_predictors()) %>%\n  step_dummy(all_nominal_predictors())\n\nwf06 <-\n  workflow() %>%\n  add_model(mod06) %>%\n  add_recipe(rec06)\n\nforeach::registerDoSEQ()\nset.seed(101112)\nrs06 <-\n  fit_resamples(\n    wf06,\n    diamonds_folds,\n    control = ctrl_preds\n  )\n\ncollect_metrics(rs06)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.127    10 0.00115 Preprocessor1_Model1\n2 rsq     standard   0.916    10 0.00136 Preprocessor1_Model1\n```\n:::\n:::\n\n\nWell - that was quite a bit for no improvement! I guess that normalizing the continuous vars in this case isn't helping. Moving on to adding some interactions - first let's explore potential interactions a bit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrics <-\n  metrics %>% \n  bind_rows(collect_metrics(rs06) %>% mutate(model = \"model06\"))\n\ndiamonds_train %>%\n  ggplot(aes(x = carat,\n             y = price,\n             color = cut)) +\n  geom_point(alpha = 0.05) + \n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-28-1.png){width=2100}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(splines)\ndiamonds_train %>%\n  ggplot(aes(x = carat,\n             y = price,\n             color = cut)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(method = lm,\n              formula = y ~ ns(x, df = 5),\n              se = FALSE) +\n  facet_wrap(~cut, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-29-1.png){width=2100}\n:::\n:::\n\n\n5 spline terms might not be sufficient here - capturing the lower bound well but *really* not doing well with the higher carat diamonds.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds_train %>%\n  ggplot(aes(x = carat,\n             y = price,\n             color = cut)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(method = lm,\n              formula = y ~ ns(x, df = 10),\n              se = FALSE) +\n  facet_wrap(~cut, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-30-1.png){width=2100}\n:::\n:::\n\n\nHmmmm, 10 might be too many. It looks lie we'll just lose a bit of confidence for the Premium & Very Good diamonds at higher carats. Relative to the total number, I'm not too concerned.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds_train %>%\n  ggplot(aes(x = carat,\n             y = price,\n             color = cut)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(method = lm,\n              formula = y ~ ns(x, df = 7),\n              se = FALSE) +\n  facet_wrap(~cut, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-31-1.png){width=2100}\n:::\n:::\n\n\n7 terms feels like the best we're going to do here - I think this is tuneable, but we'll leave as is (now & in the final model). \n\nNext, we'll look at creating interactions between the `color` and `carat` variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds_train %>%\n  ggplot(aes(x = carat, \n             y = price,\n             color = color)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(method = lm, \n              formula = y ~ ns(x, df = 15),\n              se = FALSE) +\n  facet_wrap(~color)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-32-1.png){width=2100}\n:::\n:::\n\n\nAdding interactive spline terms with `df` of 15 seems to add some useful information! \n\nWe have three shape parameters, `x`, `y`, and `z` - I wonder if creating a stand-in for volume by multiplying them all together will provide any useful information?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds_train %>%\n  mutate(volume_param = x * y * z) %>%\n  ggplot(aes(x = volume_param,\n             y = price)) +\n  geom_point(alpha = 0.05)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-33-1.png){width=2100}\n:::\n:::\n\n\nOoh, looks like we're getting some good info here, but we may want to use `log10` to scale this back.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds_train %>%\n  mutate(volume_param = log10(x * y * z)) %>%\n  ggplot(aes(x = volume_param, \n             y = price)) +\n  geom_point(alpha = 0.05)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-34-1.png){width=2100}\n:::\n:::\n\n\nLet's see if this ought to interact with any other paramaters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds_train %>%\n  mutate(volume_param = log10(x * y * z)) %>%\n  ggplot(aes(x = volume_param, \n             y = price,\n             color = cut)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-35-1.png){width=2100}\n:::\n\n```{.r .cell-code}\ndiamonds_train %>%\n  mutate(volume_param = log10(x * y * z)) %>%\n  ggplot(aes(x = volume_param, \n             y = price,\n             color = color)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-35-2.png){width=2100}\n:::\n\n```{.r .cell-code}\ndiamonds_train %>%\n  mutate(volume_param = log10(x * y * z)) %>%\n  ggplot(aes(x = volume_param, \n             y = price,\n             color = clarity)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-35-3.png){width=2100}\n:::\n:::\n\n\nHmm, it doesn't really look like we're capturing too great of interactions, so I'll leave out for now. It looks like the *size* of the rock is more important than anything else! I could continue to dig further, but I'll stop there. I'm likely getting diminishing returns, & I'd like to get back into modeling!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod07 <- mod06\n\nrec07 <-\n  recipe(price ~ ., data = diamonds_train) %>%\n  step_other(cut, color, clarity) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_interact(~carat:starts_with(\"cut_\")) %>%\n  step_interact(~carat:starts_with(\"color_\")) %>%\n  step_mutate_at(c(x, y, z),\n                 fn = ~if_else(.x == 0, mean(.x), .x)) %>%\n  step_mutate(volume_param = log10(x * y * z)) %>%\n  step_ns(starts_with(\"carat_x_cut\"), deg_free = 7) %>%\n  step_ns(starts_with(\"carat_x_color\"), deg_free = 15) \n\nrec07\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          9\n\nOperations:\n\nCollapsing factor levels for cut, color, clarity\nDummy variables from all_nominal_predictors()\nInteractions with carat:starts_with(\"cut_\")\nInteractions with carat:starts_with(\"color_\")\nVariable mutation for c(x, y, z)\nVariable mutation for log10(x * y * z)\nNatural splines on starts_with(\"carat_x_cut\")\nNatural splines on starts_with(\"carat_x_color\")\n```\n:::\n\n```{.r .cell-code}\nwf07 <-\n  workflow() %>%\n  add_model(mod07) %>%\n  add_recipe(rec07)\n\ndoParallel::registerDoParallel()\nset.seed(9876)\nrs07 <-\n  fit_resamples(\n    wf07,\n    diamonds_folds,\n    control = ctrl_preds\n  )\n```\n:::\n\n\nThis is definitely going to *way* overfit our data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrs07 %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.0750    10 0.00377 Preprocessor1_Model1\n2 rsq     standard   0.970     10 0.00342 Preprocessor1_Model1\n```\n:::\n:::\n\n\nWell we (finally) made a modes improvement! Let's see how the predictions/residuals plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrs07 %>%\n  augment() %>%\n  ggplot(aes(x = price,\n             y = .pred)) +\n  geom_point(alpha = 0.05) +\n  geom_abline(linetype = \"dashed\",\n              alpha = 0.5,\n              size = 0.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-38-1.png){width=2100}\n:::\n:::\n\n\nThat's pretty good! We do have one value that's ***way*** off, so let's see if regulization can help. This will require setting a new baseline model, and we'll tune our way to the best regularizaion parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrics <- \n  rs07 %>%\n  collect_metrics() %>%\n  mutate(model = \"model07\") %>%\n  bind_rows(metrics)\n\n# add normalization step\nrec08 <- \n  rec07 %>% \n  step_zv(all_numeric_predictors()) %>%\n  step_normalize(all_numeric_predictors(),\n                 -cut_Ideal, -cut_Premium, -cut_Very.Good, -cut_other,\n                 -color_E, -color_F, -color_G, -color_H, -color_I, -color_J,\n                 -clarity_SI2, -clarity_VS1, -clarity_VS2, -clarity_VVS1, -clarity_VVS2, -clarity_other)\n\nrm(mod05, mod06, mod07, rec05, rec06, rec07, wf05, wf06, wf07, rs05, rs06, rs07)\n\nmod08 <-\n  linear_reg(penalty = tune(), mixture = tune()) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"regression\") \n\nwf08 <-\n  workflow() %>%\n  add_model(mod08) %>%\n  add_recipe(rec08)\n\ndiamonds_grid <- \n  grid_regular(penalty(), mixture(), levels = 20)\n\ndoParallel::registerDoParallel()\nset.seed(5831)\nrs08 <-\n  tune_grid(\n    wf08,\n    resamples = diamonds_folds,\n    control = ctrl_preds,\n    grid = diamonds_grid\n  )\n```\n:::\n\n\nSome notes but let's explore our results...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrs08 %>%\n  collect_metrics() %>%\n  ggplot(aes(x = penalty,\n             y = mean,\n             color = as.character(mixture))) +\n  geom_point() +\n  geom_line(alpha = 0.75) +\n  facet_wrap(~.metric, scales = \"free\") +\n  scale_x_log10()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-40-1.png){width=2100}\n:::\n:::\n\n\nLooks like we were performing pretty well with the unregularized model, oddly enough! Let's select the best and finalize our workflow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_metrics <- \n  rs08 %>%\n  select_best(\"rmse\")\n\nwf_final <- \n  finalize_workflow(wf08, best_metrics)\n\nrm(mod08, rec07, rec08, rs08, wf08)\n\nset.seed(333)\nfinal_fit <- \n  wf_final %>%\n  fit(diamonds_train)\n\nfinal_fit %>%\n  predict(diamonds_test) %>%\n  bind_cols(diamonds_test) %>%\n  select(price, .pred) %>%\n  ggplot(aes(x = price, \n             y = .pred)) +\n  geom_point(alpha = 0.05) + \n  geom_abline(alpha = 0.5,\n              linetype = \"dashed\",\n              size = 0.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-41-1.png){width=2100}\n:::\n:::\n\n\nWhat are the most important variables in this regularized model?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit %>%\n  pull_workflow_fit() %>%\n  vi(lambda = best_metrics$penalty) %>%\n  mutate(Variable = fct_reorder(Variable, Importance)) %>%\n  ggplot(aes(x = Variable,\n             y = Importance, \n             fill = Sign)) +\n  geom_col() +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-42-1.png){width=2100}\n:::\n:::\n\n\nAs expected, most of our terms get regularized away, which is what we want! Our chart is a little unreadable; let's plot just the most important variables in a few ways:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit %>%\n  pull_workflow_fit() %>%\n  vi(lambda = best_metrics$penalty) %>%\n  arrange(desc(Importance)) %>%\n  slice_head(n = 10) %>%\n  mutate(Variable = fct_reorder(Variable, Importance)) %>%\n  ggplot(aes(x = Variable,\n             y = Importance,\n             fill = Sign)) +\n  geom_col() +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-43-1.png){width=2100}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit %>%\n  pull_workflow_fit() %>%\n  vi(lambda = best_metrics$penalty) %>%\n  arrange(desc(Importance)) %>% \n  slice_head(n = 10) %>%\n  mutate(Importance = if_else(Sign == \"NEG\", -Importance, Importance),\n         Variable = fct_reorder(Variable, Importance)) %>%\n  ggplot(aes(x = Variable,\n             y = Importance,\n             fill = Sign)) + \n  geom_col() +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-44-1.png){width=2100}\n:::\n:::\n\n\nAnd look at that! Our most important variable was one that came from feature engineering! The size of the rock had the biggest impact on price. \n\nWe've gone through a lot of steps, so it may be good to look back on what was done:\n\n* Explored our dataset via some simple exploratory data analysis;\n* Fit a simple linear model to predict the log-transform of price;\n* Attempted (and failed) to improve upon the simple model with fancier normalization and resampling techniques;\n* Explored the dataset further to find meaningful interactions and potential new features;\n* Fit a new model with feature engineering;\n* Tuned regularization parameters on our model with feature engineering to arrive at the final model.\n\nOur models' performances, ranked from best to worst, show that the final tuned model did indeed perform the best on the test dataset!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_preds <-\n  final_fit %>%\n  predict(diamonds_train) %>%\n  bind_cols(diamonds_train) %>%\n  select(price, .pred)\n\nbind_rows(final_preds %>% rmse(price, .pred),\n          final_preds %>% rsq(price, .pred)) %>%\n  rename(mean = .estimate) %>%\n  select(-.estimator) %>%\n  mutate(model = \"model_final\") %>%\n  bind_rows(metrics %>% select(.metric, mean, model)) %>%\n  pivot_wider(names_from = .metric,\n              values_from = mean) %>%\n  mutate(model = fct_reorder(model, desc(rmse))) %>%\n  pivot_longer(rmse:rsq,\n               names_to = \"metric\",\n               values_to = \"value\") %>%\n  ggplot(aes(x = model,\n             y = value)) +\n  geom_point() +\n  facet_wrap(~metric, scales = \"free\") +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-45-1.png){width=2100}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}