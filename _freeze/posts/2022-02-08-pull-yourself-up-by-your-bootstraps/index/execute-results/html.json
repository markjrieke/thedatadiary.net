{
  "hash": "5d35f3425831e8f8d524bc11f5f54747",
  "result": {
    "markdown": "---\ntitle: \"Pull Yourself Up by Your Bootstraps\"\ndate: '2022-02-08'\ncategories: [rstats, tidymodels]\ncaption: Using bootstrap resamples to generate confidence intervals from non-linear\n  models\nimage: featured.png\ncode-fold: show\n---\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(MetBrewer)\n\nextrafont::loadfonts(device = \"win\")\n\ntheme_set(theme_minimal(base_family = \"Roboto Slab\") +\n            theme(plot.background = element_rect(fill = \"white\", color = \"white\")))\n```\n:::\n\n\n:::{.alert-warning}\n**Note (3/14/22): This article was written prior to the release of the [{workboots}](https://github.com/markjrieke/workboots) package. Since the release of that package, I've discovered some errors with the methodology described here and would recommend instead referencing the [post associated with the release](https://thedatadiary.net/posts/2022-03-14-introducing-workboots).**\n:::\n\nStatistical modeling sometimes presents conflicting goals. Oftentimes, building a model involves a mix of objectives that don't necessarily mesh well together: super-accurate point predictions, explainability, fast performance, or an expression of confidence in the prediction. In my work as an analyst, I generally am focused on how explainable the model is while being able to express a confidence interval around each prediction. For that, simple linear models do the trick. If, however, I want to regularize via `{glmnet}` (which --- with good reason --- [doesn't provide confidence intervals](https://stats.stackexchange.com/questions/224796/why-are-confidence-intervals-and-p-values-not-reported-as-default-for-penalized)) or use a non-linear model like `{xgboost}`, I have to drop the confidence interval around predictions. Or so I had previously thought! As it turns out, building a series of models from bootstrap resamples provides an alternative method of generating a confidence interval around a prediction.\n\n### Setting a baseline with penguins\n\nFirst, let's build out a baseline linear model with the Palmer Penguins dataset. This dataset contains information on 344 penguins across three species types and three islands. For this example, we'll use the penguin information to predict `body_mass_g`.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the data in from the tidytuesdayR package\npenguins_src <- tidytuesdayR::tt_load(2020, week = 31)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tDownloading file 1 of 2: `penguins.csv`\n\tDownloading file 2 of 2: `penguins_raw.csv`\n```\n:::\n\n```{.r .cell-code}\n# extract out the penguins dataset\npenguins <- penguins_src$penguins\nrm(penguins_src)\n\npenguins\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <chr>   <chr>              <dbl>         <dbl>      <dbl>   <dbl> <chr> <dbl>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n```\n:::\n:::\n\n\nWe'll need to do some lite preprocessing before we start modeling --- it looks like there are some `NAs` in `body_mass_g` and in `sex`. If I were creating a more serious model, I might keep the rows with `NAs` for `sex`, but since there are so few and this is an explainer, I'll just filter them out. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# remove NA from body_mass_g and sex\npenguins <- \n  penguins %>%\n  filter(!is.na(body_mass_g),\n         !is.na(sex))\n\npenguins\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 333 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <chr>   <chr>              <dbl>         <dbl>      <dbl>   <dbl> <chr> <dbl>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 5 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 6 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 7 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 8 Adelie  Torgersen           41.1          17.6        182    3200 fema…  2007\n 9 Adelie  Torgersen           38.6          21.2        191    3800 male   2007\n10 Adelie  Torgersen           34.6          21.1        198    4400 male   2007\n# … with 323 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n```\n:::\n:::\n\n\nIt's always good practice to explore the dataset prior to fitting a model, so let's jump into some good ol' fashioned EDA.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# how are species/island related to body mass?\npenguins %>%\n  ggplot(aes(x = species,\n             y = body_mass_g,\n             color = species)) +\n  geom_boxplot() +\n  geom_point(alpha = 0.25,\n             position = position_jitter()) +\n  facet_wrap(~island)\n```\n:::\n\n\n![](pics/plot_01.png)\n\nInteresting! It looks like the Gentoo and Chinstrap species are only found on the Biscoe and Dream islands, respectively, whereas the Adelie species can be found on all three islands. At first glance, there's not a meaningful difference that Island has on the weight of the Adelie penguins, so I think we're safe to toss out the `island` feature and just keep `species`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# how does sex relate to body mass?\npenguins %>%\n  ggplot(aes(x = sex,\n             y = body_mass_g,\n             color = sex)) +\n  geom_boxplot() +\n  geom_point(alpha = 0.25,\n             position = position_jitter())\n```\n:::\n\n\n![](pics/plot_02.png)\n\nUnsurprisingly, male penguins are typically heavier than female penguins.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# are penguins getting heavier or lighter as years progress?\npenguins %>%\n  mutate(year = as.character(year)) %>%\n  ggplot(aes(x = year,\n             y = body_mass_g)) +\n  geom_boxplot() +\n  geom_point(alpha = 0.25,\n             position = position_jitter())\n```\n:::\n\n\n![](pics/plot_03.png)\n\nIt doesn't look like there is significant signal being drawn from `year`, so we'll toss that out as well.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# how do other body measurements compare with the total body mass?\npenguins %>%\n  select(bill_length_mm:body_mass_g) %>%\n  pivot_longer(ends_with(\"mm\"),\n               names_to = \"measurement\",\n               values_to = \"value\") %>%\n  ggplot(aes(x = value,\n             y = body_mass_g,\n             color = measurement)) +\n  geom_point(alpha = 0.5) + \n  facet_wrap(~measurement, scales = \"free_x\") +\n  geom_smooth(method = \"lm\",\n              se = FALSE)\n```\n:::\n\n\n![](pics/plot_04.png)\n\nFor bill and flipper length, there's a pretty clear relationship, but it looks like bill depth has a *classic* case of [Simpson's paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox). Let's explore that further to find a meaningful interaction to apply.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# which feature interacts with bill depth to produce simpson's pardox?\npenguins %>%\n  ggplot(aes(x = bill_depth_mm,\n             y = body_mass_g,\n             color = species)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\",\n              se = FALSE)\n```\n:::\n\n\n![](pics/plot_05.png)\n\nSo, very clearly, the Gentoo species has a very different relationship between bill depth and body mass than the Adelie/Chinstrap species. We'll add this as an interactive feature to the model.\n\nWith all that completed, let's (finally) setup and build the baseline linear model with confidence intervals around the prediction!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# remove features\npenguins <- \n  penguins %>%\n  select(-island, -year)\n\n# split into testing and training datasets\nset.seed(123)\npenguins_split <- initial_split(penguins)\npenguins_test <- testing(penguins_split)\npenguins_train <- training(penguins_split)\n\n# setup a pre-processing recipe\npenguins_rec <- \n  recipe(body_mass_g ~ ., data = penguins_train) %>%\n  step_dummy(all_nominal()) %>% \n  step_interact(~starts_with(\"species\"):bill_depth_mm)\n\n# fit a workflow\npenguins_lm <- \n  workflow() %>%\n  add_recipe(penguins_rec) %>%\n  add_model(linear_reg() %>% set_engine(\"lm\")) %>%\n  fit(penguins_train)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# predict on training data with confidence intervals\nbind_cols(penguins_lm %>% predict(penguins_train),\n          penguins_lm %>% predict(penguins_train, type = \"conf_int\", level = 0.95),\n          penguins_train) %>%\n  ggplot(aes(x = body_mass_g,\n             y = .pred)) +\n  geom_point(alpha = 0.5) +\n  geom_segment(aes(x = body_mass_g,\n                   xend = body_mass_g,\n                   y = .pred_lower,\n                   yend = .pred_upper),\n               alpha = 0.25) +\n  labs(title = \"Predicting the Palmer Penguins - Training\",\n       subtitle = \"Linear model predicting a penguin's weight in grams\",\n       x = \"Actual weight (g)\",\n       y = \"Predicted weight (g)\",\n       caption = \"Errorbars represent the a 95% confidence interval\") +\n  theme(plot.title.position = \"plot\")\n```\n:::\n\n\n![](pics/plot_06.png)\n\nThis model does generally okay, but the confidence interval around each prediction is pretty [clearly too confident](https://mc-stan.org/rstanarm/articles/rstanarm.html)! Let's solve this with bootstrapping.\n\n### What's a bootstrap?\n\nBefore progressing any further, it's probably important to define what exactly a bootstrap is/what bootstrapping is. Bootstrapping is a resampling method that lets us take one dataset and turn it into many datasets. Bootstrapping accomplishes this by repeatedly pulling a random row from the source dataset and, importantly, bootstrapping allows for rows to be repeated! Let's look at an example for a bit more clarity.\n\n\n::: {.cell}\n::: {.cell-output-display}\n| rowid|  x1|  x2|\n|-----:|---:|---:|\n|     1| 0.8| 104|\n|     2| 0.7| 102|\n|     3| 0.9|  88|\n|     4| 0.4| 124|\n|     5| 0.3|  79|\n:::\n:::\n\n\nLet's say we want to make bootstrap resamples of this dataset. We'll draw five random rows from the dataset and, sometimes, we'll have the same row show up in our new bootstrapped dataset multiple times:\n\n\n::: {.cell}\n::: {.cell-output-display}\n| rowid|  x1|  x2|\n|-----:|---:|---:|\n|     1| 0.8| 104|\n|     3| 0.9|  88|\n|     4| 0.4| 124|\n|     4| 0.4| 124|\n|     2| 0.7| 102|\n:::\n:::\n\n\nAnother bootstrap dataset might look like this:\n\n\n::: {.cell}\n::: {.cell-output-display}\n| rowid|  x1|  x2|\n|-----:|---:|---:|\n|     2| 0.7| 102|\n|     3| 0.9|  88|\n|     5| 0.3|  79|\n|     3| 0.9|  88|\n|     5| 0.3|  79|\n:::\n:::\n\n\nBootstrap datasets allow us to create many datasets from the original dataset and evaluate models across these bootstraps. Models that are well informed will give similar outputs across each dataset, despite of the randomness within each dataset, whereas less confident models will have a wider variation across the bootstrapped datasets.\n\n### Generating some confident penguins\n\nLet's say we want to use `{xgboost}` to predict penguin weight and we'll use bootstrapping to generate a confidence interval. Firstly, we'll create the bootstrap datasets from our training set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins_boot <- penguins_train %>% bootstraps()\n\npenguins_boot\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Bootstrap sampling \n# A tibble: 25 × 2\n   splits           id         \n   <list>           <chr>      \n 1 <split [249/92]> Bootstrap01\n 2 <split [249/90]> Bootstrap02\n 3 <split [249/91]> Bootstrap03\n 4 <split [249/87]> Bootstrap04\n 5 <split [249/98]> Bootstrap05\n 6 <split [249/84]> Bootstrap06\n 7 <split [249/91]> Bootstrap07\n 8 <split [249/95]> Bootstrap08\n 9 <split [249/94]> Bootstrap09\n10 <split [249/86]> Bootstrap10\n# … with 15 more rows\n```\n:::\n:::\n\n\nBy default, the `bootstraps()` function will create 25 bootstrap datasets, but we could theoretically create as many as we want. Now that we have our bootstraps, let's create a function that will fit a model to each of the bootstraps and save to disk. We'll use the default parameters for our `{xgboost}` model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define a basic xgboost model\npenguins_xgb <-\n  boost_tree() %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"xgboost\")\n\n# function that will fit a model and save to a folder\nfit_bootstrap <- function(index) {\n  \n  # pull out individual bootstrap to fit\n  xgb_boot <- penguins_boot$splits[[index]] %>% training()\n  \n  # fit to a workflow\n  workflow() %>%\n    add_recipe(penguins_rec) %>%\n    add_model(penguins_xgb) %>%\n    fit(xgb_boot) %>%\n    write_rds(paste0(\"models/model_\", index, \".rds\"))\n  \n}\n```\n:::\n\n\nThis function will create a new model for each bootstrap, so we'll end up with 25 separate models. Let's fit!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit to 25 bootstrapped datasets\nfor (i in 1:25) {\n  \n  fit_bootstrap(i)\n  \n}\n```\n:::\n\n\nNow let's define a function that will predict based on these 25 bootstrapped models, then predict on our training data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict_bootstrap <- function(new_data, index){\n  \n  read_rds(paste0(\"models/model_\", index, \".rds\")) %>%\n    predict(new_data) %>%\n    rename(!!sym(paste0(\"pred_\", index)) := .pred)\n  \n}\n\n# predict!\ntraining_preds <- \n  seq(1, 25) %>%\n  map_dfc(~predict_bootstrap(penguins_train, .x))\n\ntraining_preds\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 249 × 25\n   pred_1 pred_2 pred_3 pred_4 pred_5 pred_6 pred_7 pred_8 pred_9 pred_10\n    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n 1  5552.  5638.  5555.  5703.  5726.  5783.  5404.  5566.  5493.   5547.\n 2  3470.  3340.  3334.  3350.  3311.  3303.  3315.  3421.  3692.   3436.\n 3  5309.  5274.  5241.  5286.  5206.  5084.  5506.  5531.  5274.   5309.\n 4  4160.  4013.  3988.  4111.  4075.  4073.  4284.  4050.  4033.   4033.\n 5  4003.  3931.  4096.  3968.  4008.  3918.  3941.  4093.  3941.   3880.\n 6  3967.  4039.  4095.  4047.  4021.  4055.  3980.  4115.  4067.   4084.\n 7  4647.  4551.  4750.  4555.  4690.  4396.  4235.  4686.  4764.   4659.\n 8  5240.  5288.  5291.  5276.  5308.  5508.  5570.  5375.  5340.   5268.\n 9  4138.  4111.  4106.  4236.  4135.  4219.  4218.  4211.  4160.   4071.\n10  4728.  4723.  4715.  4823.  4765.  4727.  4836.  4777.  4765.   4633.\n# … with 239 more rows, and 15 more variables: pred_11 <dbl>, pred_12 <dbl>,\n#   pred_13 <dbl>, pred_14 <dbl>, pred_15 <dbl>, pred_16 <dbl>, pred_17 <dbl>,\n#   pred_18 <dbl>, pred_19 <dbl>, pred_20 <dbl>, pred_21 <dbl>, pred_22 <dbl>,\n#   pred_23 <dbl>, pred_24 <dbl>, pred_25 <dbl>\n```\n:::\n:::\n\n\nNow we have a column of predictions for each model --- we can summarise our point prediction for each row with the average across all models and set the confidence interval based on the standard deviation of the predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntraining_preds %>%\n  bind_cols(penguins_train) %>%\n  rowid_to_column() %>%\n  pivot_longer(starts_with(\"pred_\"),\n               names_to = \"model\",\n               values_to = \".pred\") %>%\n  group_by(rowid) %>%\n  summarise(body_mass_g = max(body_mass_g),\n            .pred_mean = mean(.pred),\n            std_dev = sd(.pred)) %>%\n  riekelib::normal_interval(.pred_mean, std_dev) %>%\n  ggplot(aes(x = body_mass_g,\n             y = .pred_mean)) +\n  geom_point(alpha = 0.5) +\n  geom_segment(aes(x = body_mass_g, \n                   xend = body_mass_g,\n                   y = ci_lower,\n                   yend = ci_upper),\n               alpha = 0.25) +\n  labs(title = \"Predicting the Palmer Penguins - Training\",\n       subtitle = \"XGBoost model predicting a penguin's weight in grams\",\n       x = \"Actual weight (g)\",\n       y = \"Predicted weight (g)\",\n       caption = \"Errorbars represent the a 95% confidence interval\") +\n  theme(plot.title.position = \"plot\")\n```\n:::\n\n\n![](pics/plot_07.png)\n\nAnd just like that, we've trained a series of models with `{xgboost}` that let us apply a confidence interval around a point prediction! Now that we've done so on the training set, let's look at performance on the test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseq(1, 25) %>%\n  map_dfc(~predict_bootstrap(penguins_test, .x)) %>%\n  bind_cols(penguins_test) %>%\n  rowid_to_column() %>%\n  pivot_longer(starts_with(\"pred_\"),\n               names_to = \"model\",\n               values_to = \".pred\") %>%\n  group_by(rowid) %>%\n  summarise(body_mass_g = max(body_mass_g),\n            .pred_mean = mean(.pred),\n            std_dev = sd(.pred)) %>%\n  riekelib::normal_interval(.pred_mean, std_dev) %>%\n  ggplot(aes(x = body_mass_g,\n             y = .pred_mean)) +\n  geom_point(alpha = 0.5) +\n  geom_segment(aes(x = body_mass_g, \n                   xend = body_mass_g,\n                   y = ci_lower,\n                   yend = ci_upper),\n               alpha = 0.25) +\n  labs(title = \"Predicting the Palmer Penguins - Testing\",\n       subtitle = \"XGBoost model predicting a penguin's weight in grams\",\n       x = \"Actual weight (g)\",\n       y = \"Predicted weight (g)\",\n       caption = \"Errorbars represent the a 95% confidence interval\") +\n  theme(plot.title.position = \"plot\")\n```\n:::\n\n\n![](pics/plot_08.png)\n\nThe performance on the test data is slightly less accurate than on the training data, but that is to be expected. Importantly, we've used bootstrap resampling to generate a confidence interval from a model that otherwise normally returns a simple point prediction. \n\n### Some noteworthy caveats\n\nThe prediction interval above is all well and good, but it comes with some *hefty* caveats. Firstly, the confidence interval in the Testing plot is generated from the mean and standard deviation from each prediction. This assumes that the predictions are distributed normally, which may not necessarily be the case.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntraining_preds %>%\n  slice_head(n = 1) %>%\n  pivot_longer(starts_with(\"pred\")) %>%\n  ggplot(aes(x = value)) +\n  geom_density()\n```\n:::\n\n\n![](pics/plot_09.png)\n\nThis density plot for one of the predictions shows that there's definitely some non-normal behavior! There's a few ways of addressing this.\n\n1. Create many, many, more bootstraps and models so that the prediction distribution approaches normality (with only 25 points, we really shouldn't even expect normality from this example). \n2. Report out the actual values of the percentiles in the distribution (e.g., the 2.5% percentile is below X, 97.5% is above Y, and the mean is at Z).\n3. Report out the actual distribution as the result.\n\nIdeally, you should do all three.\n\nThe second major caveat is that this is not one model, but a whole host of models and these take up a large amount of disk space. In this example, our 25 models take up 25 times more space than our original model and it takes some time to read in, fit, and wrangle the results. We can trade disk space for computation time by writing a function that fits and predicts without saving a model, but again, that's a tradeoff between speed and space. For linear models, it may be a better route to have STAN simulate thousands of results via `{rstanarm}` or `{brms}`, but for non-linear models, boostrapping is the best way to go for now!\n\n### Polling Bites\n\nCurrently, the Generic Ballot is holding steady with a slight sliver more Americans wanting Republicans in Congress than Democrats (**50.7%** to **49.3%**, respectively). Joe Biden's net approval continues to slide, currently sitting at **-11.4%** (**41.8%** approve, **53.1%** disapprove).\n\n![](pics/generic_ballot_current.png)\n\n![](pics/approval_disapproval_current.png)\n\n![](pics/net_approval_current.png)",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}