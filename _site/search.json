[
  {
    "objectID": "posts/2021-01-17-r-ggplot2-plotly/index.html",
    "href": "posts/2021-01-17-r-ggplot2-plotly/index.html",
    "title": "R, ggplot2, & plotly",
    "section": "",
    "text": "Editor’s note: I’ve gone back & read my previous post, & have one general thought - yeeeesh… Everything is so… “matter of fact” in tone & doesn’t really sound like me. I’ll give myself a pass, since it was the first post, was written around midnight (which is very late for me!), and probably won’t ever be read by anyone besides me/my mom (hi mom!). That being said, I’m going to try to make a more conscious effort going forward of having my voice be expressed in my writing. This is my blog, after all, so it should sound like my voice when read back.\n\n\nPlotting in R\nI’ve started getting more familiar with R & wrangling my way through a few plotting examples (Youtube university is, once again, my best friend), but I thought it would be worthwhile to work towards plotting my own dataset. Luckily enough, plotting with R is pretty intuitive, once you get ahold of the basic syntax. I had put together a dataset on past US presidential elections for a separate personal project, and was able to convert to a .csv to use for some basic plotting practice.\nIt’s generally well understood that the Republican party has a structural advantage in presidential elections due to the winner-take-all nature of the electoral college, but I wanted to see if I could quantify this advantage in an understandable format using R. I used the ggplot2, dplyr, and plotly packages to put this together.\n\n\nPutting Together the Plot\nI imported the entire dataset and assigned it to a dataframe (R’s version of, say, an excel table). Using the base R plot() function, I plotted the entire dataframe:\n\nWithout specifying which variables I want to look at, plot() will output a set of summary plots, with every variable plotted against every other variable. This isn’t great for gathering any meaningful insight, but helps get a “lay of the land” view of the dataframe. In this case, I’m interested in how the percentage of the popular vote a candidate wins is related to the percentage of the electoral vote they win. Using plot(), I can graph popular_vote_pct and electoral_vote_pct variables from the dataframe:\n\nFigure 2 shows the basic data I want to represent, but there’s a lot of noise. The dataset includes every major candidate in every election since Washington’s run for re-election in 1788. I’m really only interested in the modern two-party system, so I filtered out the elections prior to 1952, as well as any candidate that won 0 electoral votes (i.e., third parties). The new, filtered plot is shown in Figure 3, below:\n\nNow, with all the data manipulation squared away, I can start with the fun part: making it look good! ggplot’s base plot is, right off the bat, just a bit nicer looking than base R’s plot() function:\n\nggplot color maps very easily, and with a little googling (aka - jumping through multiple color converters online to convert colors from hexadecimal to rgb to hcl), I was able to make some formatting changes that give the below static chart:\n\nDespite some frustrations (more on that, below), I was able to add a simple linear regression to the plot, with the shaded bands representing the 95% confidence interval for each regression. Since the Republican regression is above the Democratic regression, the chart implies that, for a given share of popular vote, Republican candidates on average win a larger share of the electoral college than their Democratic counterparts.\n\nThis finding matches the general consensus and my prior expectations, but I also found something mildly surprising. Ideally, I would imagine, we’d want a candidate’s share of the electoral vote to match their share of the popular vote. However, if I add a reference line of y = x, we can see that both parties tend to underperform in the electoral college when they lose the popular vote, but overperform in the electoral college when they win the popular vote!\n\nI plotted the absolute vote share from published election results, rather than the two-party vote share (2PV) (i.e., the percentage of vote won if only the top two candidates are considered). Converting to the 2PV would shift the plot to the right, but keep the same overall trend, as Democrats and Republicans have won 100% of the electoral college since 1952. This exercise was more about exploring R’s ggplot() function, so going back into the dataset to account for this marginal change doesn’t seem like an efficient use of time, but I did want to make a note of it, for transparency’s sake.\nFinally, I used the ggplotly() to convert from a static ggplot to an interactive plotly, and exported as an html file. For reasons I’ve yet to figure out, the custom textbox formatting did not want to play nice with the linear regression lines, so I had to take those out. I plan on figuring this wrinkle out eventually, but for now I’m happy with the final plot. You can hover over individual datapoints below to see more details!\n\nThis is an archive of a post previously hosted on Squarespace. You can view the original interactive content here.\n\n\n\nSome Notes\nI’ve published all of my work to github, and I’ll continue to do so in the future. The files I uploaded are, to say the least, in pretty rough shape. That being said, I don’t really plan on tweaking them for this post - I’ve used them as training tools, and it might be interesting in the future to look at the progress I’ve made.\nAlso, I just realized that the interactive plotly chart looks like junk when viewed via mobile (the data points and labels don’t resize automatically). I’ll have to figure out a way to fix this in the future… Maybe I can set the point size as a ratio of the plot width? It’s something to think about, but again, I’m happy with my first run using ggplot & plotly.\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {R, Ggplot2, \\& Plotly},\n  date = {2021-01-17},\n  url = {https://www.thedatadiary.net/posts/2021-01-17-r-ggplot2-plotly},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “R, Ggplot2, & Plotly.” January 17,\n2021. https://www.thedatadiary.net/posts/2021-01-17-r-ggplot2-plotly."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "the data diary",
    "section": "",
    "text": "Diamonds are Forever\n\n\n\ntidymodels\n\n\n\nFeature Engineering with the Diamonds Dataset\n\n\n\nMark Rieke\n\n\nNov 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElection Night: Some Closing Thoughts on the VA Governor Race\n\n\n\npolitics\n\n\n\n\n\n\n\nMark Rieke\n\n\nNov 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolling Average of the VA Governor’s Race using purrr::map functions\n\n\n\npolitics\n\n\n\n\n\n\n\nMark Rieke\n\n\nOct 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio’s Call for Documentation\n\n\n\narchive\n\n\nrstats\n\n\n\n\n\n\n\nMark Rieke\n\n\nOct 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCatching Up (again)\n\n\n\narchive\n\n\n\n\n\n\n\nMark Rieke\n\n\nSep 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidymodels and the Titanic\n\n\n\narchive\n\n\nrstats\n\n\ntidymodels\n\n\n\n\n\n\n\nMark Rieke\n\n\nAug 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Gas Price Fallacy\n\n\n\narchive\n\n\npolitics\n\n\n\n\n\n\n\nMark Rieke\n\n\nJul 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlexas?\n\n\n\narchive\n\n\npolitics\n\n\n\n\n\n\n\nMark Rieke\n\n\nJul 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID Cases Improve with Introduction of Vaccines\n\n\n\narchive\n\n\n\n\n\n\n\nMark Rieke\n\n\nJun 3, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPresident of the Polls\n\n\n\narchive\n\n\npolitics\n\n\n\n\n\n\n\nMark Rieke\n\n\nMay 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Lukewarm Case for DC Statehood\n\n\n\narchive\n\n\npolitics\n\n\n\n\n\n\n\nMark Rieke\n\n\nApr 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCatching Up\n\n\n\narchive\n\n\n\n\n\n\n\nMark Rieke\n\n\nApr 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeasonality is a Weak Predictor of Border Crossings\n\n\n\narchive\n\n\npolitics\n\n\n\n\n\n\n\nMark Rieke\n\n\nApr 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecent Works\n\n\n\narchive\n\n\n\n\n\n\n\nMark Rieke\n\n\nMar 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naRtwork!\n\n\n\narchive\n\n\n\n\n\n\n\nMark Rieke\n\n\nMar 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoug Collins Saved Raphael Warnock’s Senate Bid\n\n\n\narchive\n\n\npolitics\n\n\n\n\n\n\n\nMark Rieke\n\n\nFeb 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Kind of) Projecting the 2020 Election\n\n\n\narchive\n\n\npolitics\n\n\n\n\n\n\n\nMark Rieke\n\n\nFeb 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScorecasting\n\n\n\narchive\n\n\npolitics\n\n\n\n\n\n\n\nMark Rieke\n\n\nFeb 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome Worthwhile Links\n\n\n\narchive\n\n\n\n\n\n\n\nMark Rieke\n\n\nFeb 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Min-Wage Debate\n\n\n\narchive\n\n\npolitics\n\n\n\n\n\n\n\nMark Rieke\n\n\nJan 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Doctor Probably Isn’t an Idiot\n\n\n\narchive\n\n\nbayes\n\n\n\n\n\n\n\nMark Rieke\n\n\nJan 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR, ggplot2, & plotly\n\n\n\narchive\n\n\n\n\n\n\n\nMark Rieke\n\n\nJan 17, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBaby Steps\n\n\n\narchive\n\n\n\n\n\n\n\nMark Rieke\n\n\nJan 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHello World!\n\n\n\narchive\n\n\n\n\n\n\n\nMark Rieke\n\n\nJan 9, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-01-10-baby-steps/index.html",
    "href": "posts/2021-01-10-baby-steps/index.html",
    "title": "Baby Steps",
    "section": "",
    "text": "An Uphill Battle\nToday marks the beginning of what I imagine will be a long journey, with a significant number of hiccups & frustrations along the way. The end goal I have is to build out an election forecast model (in the spirit of FiveThirtyEight & The Economist), but along the way, I plan on getting better at a few things:\n\nWriting : while I’ve never been a poor writer, I’ve always gravitated more toward math & science, & in my career as an engineer, a good amount of my communication has been visual. If possible, I’ve always preferred to leave words off the page & have a chart speak for me. While I still think that, in general, its better the explain a concept visually (a picture vs. a thousand words, and all), I also believe supplementing a visual with my a well written analysis & opinion can improve its reception.\nStats : in my current job, almost everything I model is deterministic, rather than probabilistic (physical systems tend to behave the way that natural laws expect them to). It has been a long time since I’ve done any sort of stats work (& it was all basic introductory analyses), so I expect an uphill battle with getting my head wrapped around Bayes’ Theorem, Monte Carlo simulations, Brier’s Tests, etc.\nCoding : most of my experience in coding is with VBA - typically just for manipulating data in Excel. I’ve built out a good number of forms using VBA, but don’t typically tend to dig into anything far beyond simple manipulation. The front end of Excel is effectively setup as a visual programming tool, so there’s not a great incentive to do much array manipulation outside of the spreadsheet. Excel is a wonderful tool, & is sufficient for my current job, but I’ve definitely found where its limits lie. I’m excited to dig into R, the stats based program that seems almost to be designed specifically for what I’d like to do.\n\n\n\nThe First Plot\nR has a few sample datasets, and r:base includes a pretty basic plot function. With the sample set, cars, I made a quick plot of the stopping distance vs. the speed.\n\n\nCode\nplot(cars)\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {Baby {Steps}},\n  date = {2021-01-10},\n  url = {https://www.thedatadiary.net/posts/2021-01-10-baby-steps},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “Baby Steps.” January 10, 2021. https://www.thedatadiary.net/posts/2021-01-10-baby-steps."
  },
  {
    "objectID": "posts/2021-01-09-hello-world/index.html",
    "href": "posts/2021-01-09-hello-world/index.html",
    "title": "Hello World!",
    "section": "",
    "text": "I’m currently setting up the website & will have an official post soon(ish).\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {Hello {World!}},\n  date = {2021-01-09},\n  url = {https://www.thedatadiary.net/posts/2021-01-09-hello-world},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “Hello World!” January 9, 2021. https://www.thedatadiary.net/posts/2021-01-09-hello-world."
  },
  {
    "objectID": "posts/2021-01-24-your-doctor-probably-isn-t-an-idiot/index.html",
    "href": "posts/2021-01-24-your-doctor-probably-isn-t-an-idiot/index.html",
    "title": "Your Doctor Probably Isn’t an Idiot",
    "section": "",
    "text": "There’s a great standup routine about doctors (which I spent about an hour trying to find online with no luck!) with a punchline to the effect of, “getting a positive cancer test then requesting a second opinion is basically the medical equivalent of telling your doctor you think they’re an idiot to their face.” The joke makes sense in context (and is a lot funnier to hear, rather than read), and is logical at a general level (why would a patient, who probably isn’t a medical expert, be able to say that a doctor’s assessment is wrong?), but there’s a hidden nugget of nuance about cancer screenings missed by the joke that make them such an interesting introduction to one of the foundational equations in statistics, Bayes’ Theorem.\n\nWhat is Bayes’ Theorem?\nTo understand Bayes’ Theorem, it’s probably best to get a grasp on frequency-based probability, which is often considered the contrast of bayesian/uncertainty-based probability. Consider a perfectly ideal coin flip. I know that, for any given coin flip, the probabilities of landing either heads-up or tails-up are exactly the same: 50%. Moreover, this is true no matter how many times I flip the coin. If the coin lands heads-up on the first flip, I still have a 50% chance of the coin landing heads-up on the next flip (and the next flip, and the next flip, etc.).\nBut what if I’m told the coin has a manufacturing defect, and is slightly heavier on one side (lets say, to favor tails-up)? I might assume, starting out, that the defect is small, and has a negligible effect on my odds of the coin landing heads-up. But every flip that lands tails-up changes my certainty about the true odds. That’s the heart of Bayes’ Theorem, and the biggest difference between bayesian statistics and frequentist statistics. With frequentist statistics, when the data updates, the uncertainty stays the same, whereas with bayesian statistics, when the data updates, the uncertainty updates as well.\nSo why is this important? Well, at a base level, just about every forecasting model needs to make use of Bayes’ Theorem. So with the long term goal of building a predictive election model, it’s pretty dang important to have a firm understanding of the foundation the model will rest on. But in a more general sense, I think it’s important to understand how views can (and should) update in the light of new data. From what I can tell, the majority of opinions (especially political opinions…) are either based on assumptions or cherry-picked data points, and typically don’t update - even when presented with new evidence. In contrast, to think like a bayesian is to weigh the severity of new information against the history of data and update your views accordingly.\n\n\nUgly vs. Pretty Bayes\n\nEquation 1 above shows Bayes’ Theory. As expressed in statistical language, it reads, “the probability of A, given event B, is equal to the probability of B, given event A, and the probability of A divided by the probability of B.” As written, Bayes’ Theorem is… unintuitive… Even reading the description back to myself stirs up memories of fumbling through my high school stats class. The way the equation is written doesn’t lend itself towards being accessible, but, as Grant Sanderson points out in this video, reframing the equation into a geometry problem makes Bayes’ Theorem much easier to understand (much of the rest of this post is applying Bayes’ Theorem via Grant’s geometrical framing).\nTo understand how a geometrical framing can help make sense of Bayes’ Theorem, it’s best to run through an example. So let’s get back to the doctor’s office & cancer screening.\nLet’s first consider a population group of 10,000, arranged on a grid of 100 x 100. If we know that 1% of the population has cancer, we can divide the group into two segments, cancer-free and cancerous, as shown below:\n\nIf we avoid any compounding risk factors (like age, sex, diet, etc.) & assume that the likelihood of having cancer is the same for everyone, any person in the population can say that they are 99% certain that they do not have cancer. But how might this certainty update for someone who gets a positive result from a cancer screening test?\nThe reliability of cancer screening tests are measured by their sensitivity & specificity. A test’s sensitivity is the the proportion of cancer-positive patients it correctly identifies (i.e., “true positives”). For example, if a screening test has a sensitivity of 90% and is used to test 100 cancerous patients, we’d expect 90 of these patients to receive a correct positive result and the remaining 10 patients to receive an incorrect negative result. The specificity, on the other hand, is the proportion of cancer-free patients the test correctly identifies. If the screening test’s specificity is 92% and is used to test 100 cancer-free patients, we’d expect 92 patients to correctly receive a negative result and 8 patients to incorrectly receive a positive result (a “false positive”). I tend to find it easier just to think about the positive cases, so from now on, I’ll refer to a test’s true positive rate and false positive rate, rather than the sensitivity & specificity.\nLet’s assume that a cancer screening test has the true positive & false positive rates above, 90% & 8%, respectively. How certain should someone who tests positive be that they truly have cancer? A test that gives a true positive 90% of the time seems pretty damning, but the 8% false positive rate isn’t insignificant. An important step is to realize that this person is no longer looking for the probability of having cancer, but instead looking for the probability of having cancer given that they have a positive test result.\n\nFor someone who receives a positive test result, it may be helpful to think about the hypothetical question: “What if everyone took this test?” This reframing can help us think in terms of people, rather than probabilities.\nGiven that 1% of the 10,000 person population has cancer, we can divide the group into two segments: 100 people with cancer, and 9,900 people without cancer.\n\nIf everyone takes the screening test, we’d expect 90 true positives (90% x 100 = 90) and a whopping 792 false positives (8% x 9,900 = 792)!\n\nAlthough the test is fairly accurate in terms of sensitivity & specificity (90% and 92% are both considered A’s, by most grading standards) the sheer number of non-cancerous people in the population results in a large number of false positives. Of all the 882 positive cases (792 + 90 = 882), only 90 are true positives, meaning that the probability of having cancer given a positive test result is about 10% (90 / 882 = 10.2%). While this is a significant increase from the prior assumption, 1%, it’s still far likelier that someone with one positive test result doesn’t have cancer. This is why it’s important to get a second opinion. It’s not that your first doctor is an idiot, just that your uncertainty has changed!\nNow what would happen if all the people who received a positive result took another test? Well, of this subset of 882 patients, we expect that 792 are cancer free and 90 are cancerous:\n\nWhen these 882 patients take the second test, we still expect the test to hold the same true positive & false positive rates - meaning we can expect 81 true positives (90% x 90 = 81) and about 63 false positives (8% x 792 = 63). This means that, of the 144 positive cases (81 + 63 = 144), 81 are true positives, and the probability of having cancer given two positive results is about 56% (81 / 144 = 56%).\n\nWith this new round of data from test results, those who receive a second positive result once again update their prior assumption from 10% to 56%. In fact, every test result, positive or negative, should either support or refute the prior. Each positive test result, appropriately, increases your likelihood of actually having cancer. Similarly, each negative result decreases your likelihood.\n\nThat’s the beauty of Bayes’ Theorem in a nutshell. Gathering new information allows you to update your prior belief!\n\n\nIn Summary…\nI may have gotten too in the weeds with this post, though, to be fair, walking through Bayes’ Theorem in detail also helps my understanding. I likely wont dive as deep into stats topics in the future. In part, they’re a bit of a slog to read through if you’re not as excited as I am by this kind of stuff. But it also took me a good chunk of time to write this post. Going forward, I’d like to better balance my time between the three goals of coding, writing, and learning stats.\nAs always, I’ve posted my work to github, though this file just includes an excel workbook & a few pictures. Next week, I’ll dig more into plotting with ggplot - likely with a much shorter post!\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {Your {Doctor} {Probably} {Isn’t} an {Idiot}},\n  date = {2021-01-24},\n  url = {https://www.thedatadiary.net/posts/2021-01-24-your-doctor-probably-isn-t-an-idiot},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “Your Doctor Probably Isn’t an Idiot.”\nJanuary 24, 2021. https://www.thedatadiary.net/posts/2021-01-24-your-doctor-probably-isn-t-an-idiot."
  },
  {
    "objectID": "posts/2021-01-31-the-min-wage-debate/index.html",
    "href": "posts/2021-01-31-the-min-wage-debate/index.html",
    "title": "The Min-Wage Debate",
    "section": "",
    "text": "On January 26, House Democrats introduced a bill to raise the federal minimum wage from $7.25 per hour to $15 per hour by 2025. In the weeks leading up to the introduction, there’s been an influx on twitter of bad faith attacks, outright factually incorrect statements, and the type of fact-free arguments that pundits love (to be fair, my twitter timeline is biased towards my left-leaning friends, so the majority of what I see are poorly formulated left-leaning takes, but it’s pretty easy to find similarly bad right-leaning takes). The slew of emotion-driven arguments muddies the water around the minimum wage discussion by avoiding references to data. In an effort to find the signal in all this noise, I dug into publicly available databases to hopefully provide at least one opinion grounded in empiricism, rather than emotion.\n\nMy Priors\nFor transparency’s sake, prior to taking a look into the data behind the minimum wage debate, I was of the belief that the minimum wage should increase and that the proposed $15 per hour seemed reasonable. I didn’t have an empirically driven reason for this belief, just a vague sense that the minimum wage hadn’t risen in a while and had therefore effectively been deflating. Anecdotally, I also hadn’t seen a good defense of keeping the minimum wage static (in fact, most of the arguments against raising the minimum wage that I’d seen were laughably bad). My prior was built on little data, so the analysis I set out to do would either strongly confirm or refute it.\n\n\nThe History of the Minimum Wage\nIn 1947, the federal government introduced a minimum wage for hourly-compensated labor as part of an amendment to the Fair Labor Standards Act (FLSA). First set at $0.40 per hour, the minimum wage has risen periodically throughout its 80 year history - most recently rising to $7.25 per hour in 2009. Currently, 29 states and D.C. have state minimum wages greater than the minimum wage, and several state minimum wages are increasing in 2021 (either due to ballot initiatives or automatic increases based on cost of living).\nI’ve seen a theory float around that, had the minimum wage risen with inflation, it would be $22 per hour today, rather than $7.25 per hour. This is pretty overtly false.* Using the consumer price index (CPI) as the standard measure of inflation, the nominal minimum wage throughout the years can be adjusted to real (aka, inflation-adjusted) dollars:\nIn inflation-adjusted terms, the minimum wage peaked around 1968 at $12.20 per hour ($1.60 per hour, unadjusted). The current minimum wage of $7.25 per hour, while on the lower end of inflation-adjusted historical values, is not the lowest it’s ever been, and a $15 per hour minimum wage would set a new record for both adjusted and unadjusted minimum wage. Critics of the proposed increase may point to this as reason to keep the minimum wage at its current value - why increase the minimum wage beyond its historical high if it hasn’t even reached its historical low? Supporters, on the other hand, may argue that the minimum wage has never been enough.\n\n\nThe Minimum Wage’s Dance Between Two Thresholds\nIn 1978, the Census Bureau and Department of Health and Human Services set a baseline administrative threshold to determine eligibility for financial assistance from the federal government. Dubbed the “poverty threshold,” this value was created based on an approximation of the annual budget various family units require to meet basic food & shelter needs (for example, a single adult with no children would need to earn less per hour to meet his needs than, say, a nuclear family of two working adults and two children). The Census Bureau updates the threshold annually by simply adjusting for inflation.\nMany groups, however, argue that the poverty threshold doesn’t meet basic needs and instead advocate for a living wage. The living wage is similar in concept to the poverty threshold, but generally much higher, as it includes an expanded food budget (the poverty threshold is based on USDA’s “thrifty” food plan), a budget cap on rent (generally set at ~30% of a monthly budget), healthcare, and childcare, amongst other additions.\nBased on MIT Lab’s Living Wage Calculator, we can see that, for a family of four with two working adults, the minimum wage has historically been marginally greater than the poverty threshold, but never reaches (or approaches) the living wage:\n\nFigure 2, above, shows the national minimum and living wages, but these values vary by location. Figure 3 below shows the minimum and living wages for each state. Even states with minimum wages greater than the federal minimum do not meet the local living wage:\n\n\n\nThe Proposed Increase\nSo how does the proposed increase to the minimum wage fare against the living wage and poverty threshold in the future? If inflation continues to hold at about 1.6%, the increased minimum wage will get significantly closer to, but not greater than, the living wage in 2025.\n\nIs this enough? Arguably, no, as the proposed increase is less than the estimated living wage. But it is certainly a step in the right direction, and if passed, would likely keep the minimum wage above the poverty threshold for quite some (the poverty threshold rises roughly at a rate of $1 per hour every 10 years).\n\n\nMy Posterior\nIn general, my prior was confirmed by the analysis - my belief that the minimum wage should be increased has strengthened, and there’s a believable argument that the $15 per hour increase could be raised even higher. There are some additional points that I didn’t discuss above, however, that are still worth noting:\n\nThere are many different groups that have estimated living wages, and results can vary (I used MIT Lab’s data because it was the most readily available & also provided state, county, and metropolitan level data). In general, however, living wage estimates agree that the poverty threshold is far too low, and typically show living wages in excess of $15 per hour.\nThe percentage of hourly workers earning the federal minimum wage (or less) has decreased over the years and currently rests at about 1.9%. This is in part due to many states having minimum wages greater than the federal minimum. If the minimum wage increases to $15 per hour, the total number of workers earning the federal minimum wage will likely increase (the linked chart shows spikes that correspond to minimum wage increases in the ’90s and 2009).\nSome critics of the increased minimum wage argue that the minimum wage is only meant for teenagers working summer jobs. The data doesn’t support this argument - over 80% of federal minimum wage workers are 20+ years old.\nIn 2019, the Congressional Budget Office released a report that estimated that job losses due to an increased minimum wage could range between 4.7 million and 0, with a median of 1.3 million lost jobs. The report also notes, however, that about 17 million workers would benefit from the wage increase.\n\n\nThe linked NYT opinion article does give itself the caveat that the $22 per hour figure is based on “inflation and productivity,” though others have repeated this $20+ per hour wage argument without this caveat. The opinion article doesn’t mention any sources or databases, though I expect that the author is using GDP as a proxy for inflation - several articles point out that if the minimum wage had kept pace with GDP growth since 1968, it’d be significantly greater than even the proposed $15 per hour minimum wage. Correlating this to inflation, however, is at best a significant oversight by the author/editor, and at worst intentionally deceptive.\n\n\n\nSome Other Notes\nAs always, source data for this post can be found on github. I made an overt attempt to keep the code a bit cleaner this time around (having a “test area” in the code helped out). It’s still not perfect, and I suspect I’ll just naturally get better at writing neatly formatted scripts as the language becomes more intuitive.\nI think I’m going to take a break for a couple weeks from some of these longer posts - I’m currently finishing this a bit after midnight (which is very late for me!) - and between writing this & the Bayes’ Theorem post, I’ve had little time to dedicate to stats. Next week, I’ll put together a less intensive piece to give myself time to push forward with the stats class. Maybe after that I can write a quick post about the accuracy of prediction models, then get into another deep dive. We’ll see how I feel about it in the next couple weeks.\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {The {Min-Wage} {Debate}},\n  date = {2021-01-31},\n  url = {https://www.thedatadiary.net/posts/2021-01-31-the-min-wage-debate},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “The Min-Wage Debate.” January 31, 2021.\nhttps://www.thedatadiary.net/posts/2021-01-31-the-min-wage-debate."
  },
  {
    "objectID": "posts/2021-02-08-some-worthwhile-links/index.html",
    "href": "posts/2021-02-08-some-worthwhile-links/index.html",
    "title": "Some Worthwhile Links",
    "section": "",
    "text": "Rather than diving deep into a topic this week, I took a bit of a break to focus on playing catchup with the stats course I’m taking. Instead, I’ve listed out below a number of creators that I follow on various platforms. If you’re interested in data or critical thinking, each is well worth your time and attention.\n\nNewsletters\n\nFiveThirtyEight : While it’s not technically a newsletter, the FiveThirtyEight site, created by statistician Nate Silver prior to the 2008 presidential election, is the original source for data-driven news (or, at least, one of the first sites to popularize data as a news resource). Nate created the site specifically because the narrative created by pundits, that the 2008 election was super close, was pretty easily refutable when you looked at the polling data, which showed that Obama was going to win handily. Now, FiveThirtyEight is a powerhouse of data analysis for politics, sports, and science, and often serves as a good reality check against the narratives espoused by talking heads on the major news networks.\n\nG. Elliott Morris’ Newsletter : G. Elliott Morris is a data journalist for the Economist and created their forecast for the 2020 presidential election. In addition to regularly writing for the Economist, Elliott also writes a weekly newsletter in which he comments on polls that caught his eye (he also has a subscriber newsletter, for those who want to get his thoughts on even more topics). Elliott is also writing a book on the history of public polls, their limitations, and their future in American politics, which I am looking forward to reading when it releases later this year. As an aside, Elliott and I are the same age, and he was a large part of the inspiration for me to start diving into statistics again (i.e., if he can do it, so can I). Elliot is pretty bearish on the future of American democracy, especially following Donald Trump’s repeated attempts to overturn the overwhelmingly clear and overwhelmingly fair results of the election and the attempted (but, thankfully, woefully unorganized) coup by insurrectionists on Jan. 6th. Despite all this, I’m a bit more hopeful for the future of democracy, and hope to be able to provide a more positive opinion alongside Elliott’s (provided it’s supported by the data!).\nInfinite Monkeys : Started by a collection of college students who met via #ElectionTwitter, the Infinite Monkeys newsletter (named such from the theory that, an infinite collection of monkeys hitting keys on typewriters will eventually write the entirety of Shakespeare’s work by random chance) take a look at geographical trends and their relation to current political headlines. It’s a relatively recent startup, and I’m looking forward to the development of the newsletter & its coalition of authors over the coming years.\nVisual in the Noise : The Visual in the Noise is a weekly newsletter focused on data visualization. Most often looking at sports (particularly, NBA) data, the Visual in the Noise is a great touch-point for the importance of visualization and how it can help make data more insightful.\n\n\n\nPodcasts\n\nThe Daily : The Daily is, appropriately, a daily (Mon. - Fri.) podcast hosted by the New York Times, covering important topics in the American landscape. The podcast generally focuses in on individuals, and how national stories can affect people personally (for example, touching base with a bar throughout the pandemic as they wade through the difficulties of diminished business, PPP applications, and unclear direction from the government). On Sundays, a guest reads an older, long form New York Times piece.\nThe Intelligence : The Intelligence, similarly to the Daily, is a daily weekday podcast covering important topics in the news, though typically has a more global focus than the Daily. Rather than following individuals, the Intelligence often brings in subject matter experts and local correspondents.\nChecks and Balance : This weekly podcast by the Economist takes a deep dive into one big topic shaping American politics each week. Approximately 45 minutes per podcast, the Checks and Balance hosts take care to thoroughly explore each topic.\nFiveThirtyEight’s Politics Podcast : Every week, the FiveThirtyEight team covers the latest news in politics, utilizing polling data to guide their discussion. During election years, they also host intermittent “Model Talks,” where Nate Silver talks about into some of the intricacies of the site’s forecast models based on questions from listeners.\n\nThe Rational Reminder : A non-politics podcast, the Rational Reminder is a weekly podcast discussing index investing and rational decision making. Although the podcast is made by and for Canadians, most of the content is widely applicable, and the podcast has garnered an international audience.\n\n\n\nYouTube\n\n3blue1brown : Many concepts in math can feel daunting and teaching methods are often unintuitive. Grant Sanderson’s channel attempts to introduce viewers to the beauty of math through intuitive visualizations and animations. As a fun fact, the animations are run via a python package, manim, developed on-the-fly by Grant himself!\nPhilip DeFranco : One of the original members of the YouTube community, Phil has grown from a weekly commentary on popular videos via a webcam in his bedroom to a daily rundown of the news backed by a full production staff. Phil does an excellent job of presenting the news whilst making it clear where the official reporting stops and his opinion starts.\nLegal Eagle : Dubbed “YouTube’s Lawyer,” Devin (DJ) Stone provides a perspective on the role the law plays in current events and controversies (as well as more fun videos, like reviewing a Spongebob episode for legal accuracy). I’m not sure how he manages to balance the two full time jobs of running channel with near-daily longform content and being a lawyer with active litigation, but I appreciate that he is able to find time for thoughtful (and often comedic) insight.\n\nCommon Sense Investing : Ben Felix’s Common Sense Investing investigates the academic research supporting passive, rather than active, portfolio management (in summary: the data shows that passive index investing is overwhelmingly a more effective long term investment strategy than trusting an active portfolio manager with your money).\nStandup Maths : Mathematician, comedian, and Excel-enthusiast Matt Parker shares the joy that can be found in math by exploring topics in a comedic setting. Matt’s book, Humble Pi, explores some of history’s most famous mathematical blunders, and is coming up soon on my reading list.\nNumberphile : The Numberphile channel is a collection of interviews of prominent mathematicians explaining interesting historical math problems on trademark brown parchment paper. Grant Sanderson and Matt Parker make appearances on the channel a number of times (the infamous Parker square first made its appearance on Numberphile).\nMinutePhysics : As Henry Reich, the channel owner, puts it, the channel is simply about “cool physics and other sweet science.” Henry’s videos explain concepts in physics via a whiteboard, expo markers, and a backdrop of jazzy standup bass.\n\n\n\nElection Twitter\n\nThere are quite a few, so I’ll just highlight a few & link ot the rest\n\n\nLakshya Jain : Lakshya is a software engineer and self-described amateur elections mapper/analyst. He’s a very vocal (and self-labeled) partisan democrat, so I take his non-analytical posts with a grain of salt, but his analytical posts are very insightful.\nJack Kersting : A relative rarity on Election Twitter, Jack is a conservative forecaster. He’s not an ardent twitter user, but developed one of the most complete and thorough election forecasts I’ve seen outside of professional work.\nMax : A self described mapmaker, shitposter, and ardent supporter of Long Nebraska, Max is known for his oddball posts and lukewarm political takes.\nSome other folks I follow for election maps/data: Nate Silver, G. Elliott Morris, U Mich Voter, and Sam\n\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {Some {Worthwhile} {Links}},\n  date = {2021-02-08},\n  url = {https://www.thedatadiary.net/posts/2021-02-08-some-worthwhile-links},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “Some Worthwhile Links.” February 8,\n2021. https://www.thedatadiary.net/posts/2021-02-08-some-worthwhile-links."
  },
  {
    "objectID": "posts/2021-02-14-scorecasting/index.html",
    "href": "posts/2021-02-14-scorecasting/index.html",
    "title": "Scorecasting",
    "section": "",
    "text": "A baseball forecast that correctly predicts the winner of the 15 opening day games could be a truly accurate model, or could just be getting lucky. Over the course of 6 months and 2,430 regular season games, however, a great forecast will continue to shine whereas an initially lucky one will falter. In data rich environments like sports, there are lots of events (games) over the course of a season to judge how well a model is performing.\nWhen there aren’t a lot of events to forecast, like presidential elections that occur once every four years (quadrennially?), it’s more difficult to tell how well a forecast performs (a handful of correct/incorrect predictions could just be good/bad luck!), but forecasters still have tools available to evaluate their model. This week, I took a look at a few different methods of comparing different models.\n\nScoring the Forecasters\nThe most common scoring method, the Brier score, is a measurement of a probabilistic forecast’s accuracy based on the confidence of the prediction. Each event has its own score, and the average of all predicted events is the model’s Brier score. Highly confident predictions are highly rewarded/punished for their accuracy/inaccuracy, whereas timid predictions don’t move the needle too much, regardless of the outcome. Scores can range from 0.00 to 1.00, with 0.00 being a perfect score (you can read more about Brier scores here, but the gist is that the lower the score, the better). In presidential forecasts, each state can be considered an event.\nOftentimes, presidential forecasters report Brier scores as weighted by each state’s number of electors. Correctly predicting Texas’ winner in this scoring system is far more important than correctly predicting Wyoming’s winner, given that Texas’ 38 electoral votes far overshadow Wyoming’s 3. I’m not quite convinced that this is the most meaningful way to evaluate models (California is pretty easy to predict and heavily weighted by its 55 electoral votes, but most of us were more concerned with Georgia’s outcome, despite being undervalued by this scoring method), but forecasters do use this scoring method.\nFinally, I came up with my own scoring method that makes a “bet” between $0 and $100 based on prediction confidence. 100% confidence would turn into a $100 bet, and 50% confidence (aka, a coin toss) would be a $0 bet. Each model’s average winnings (including losses) is reported as the score.\n\nI scored a few prominent presidential forecasts based on the above methodologies (you can read more about the JHK, Economist, FiveThirtyEight, and Bitecofer forecasts at the links here). While all the scoring methods are similar - rewarding confidence in correct predictions and penalizing meek or incorrect predictions - each model’s performance is all over the map (with perhaps the exception of Bitecofer, which scores in the lower half of all methods). But does that mean these methods are useless? No! If anything, it highlights the importance that each forecast method’s performance should be scored across a wide variety of scoring methodologies. While it might not make a huge difference at the margins, it may separate some models as clearly ahead or behind the curve.\nSo, how does each forecast model measure up? It depends on which yardstick you use!\nHere are some other things to consider with regards to each model:\n\nUnlike the Economist, FiveThirtyEight, and Bitecofer models, which simulate each state’s outcome thousands of times and reports the confidence as the percentage of simulations won, the JHK forecast reports the percentage of winning results that fall within the 80% confidence interval around the expected vote. Lower confidence intervals result in tighter bands, so this relatively low confidence interval means that the model is allowed to make quite a few 100% confident predictions (which, luckily for the model, all came true in this case).\nThe Bitecofer forecast is the only model that doesn’t utilize polls and instead uses an in-house developed negative-partisanship model.\nBased on conversations between Nate Silver of FiveThirtyEight and Elliott Morris of the Economist, it appears that FiveThirtyEight added a bit of uncertainty ad-hoc to account for COVID, whereas the Economist did not.\n\nAs always, source files and code can be found on github. Next week, I plan on making a relatively modest prediction of the election’s popular vote outcome based on polls from the final week.\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {Scorecasting},\n  date = {2021-02-14},\n  url = {https://www.thedatadiary.net/posts/2021-02-14-scorecasting},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “Scorecasting.” February 14, 2021. https://www.thedatadiary.net/posts/2021-02-14-scorecasting."
  },
  {
    "objectID": "posts/2021-02-21-kind-of-projecting-the-2020-election/index.html",
    "href": "posts/2021-02-21-kind-of-projecting-the-2020-election/index.html",
    "title": "(Kind of) Projecting the 2020 Election",
    "section": "",
    "text": "I’m about 4 months too late, but to practice forecasting, I made a modest projection for the 2020 national popular vote using polling data from the two weeks leading up to election day. In the weeks leading up to the election, aggregated polling was fairly stable, and the model projection was within 1.5% of the actual outcome (both the model and actual outcome are adjusted to exclude third parties and instead show the two-party vote share).\n\nThe projected outcome of Biden’s vote share, 53.6%, is only the most likely outcome in a distribution of possible outcomes predicted by the model. Of the possible outcomes, Biden wins the popular vote about 70% of the time, according to the November 2nd projection.\n\nLike the projected vote share, the probability of Biden winning the popular vote remained fairly constant in the two weeks leading up to the election.\n\nIf you were like me, obsessing over the prominent forecast models prior to election day, you may notice that this projection is substantially less confident in the outcome than the leading forecasters. FiveThirtyEight and the Economist, for example, both projected similar popular vote outcomes (within a percentage point of this forecast), but gave Biden at least a 97% of winning the popular vote! There are a couple reasons for this difference:\n\nThis model is one-dimensional : this is a pretty simple model built just to get practice with forecasting and some of the tools in R, so it only uses polls (and, at that, only a small subset of polls), whereas other forecast models used a wide variety of variables to inform the model (economic indicators, demographics, partisanship, etc.).\nThis model doesn’t weight polls : aside from the sample size of the poll, this model doesn’t apply any weights or corrections to the polling data. The polling method, date the poll was taken, and pollster house effect (i.e., how partisan the pollster tends to be relative to the average) can be used to inflate or deflate the weight of each poll in the model. This simple model ignores all of that and treats every poll as equal.\nThis model forces an uncertainty range : unlike other models, which are a set of linear regressions, this model is a relatively simple beta distribution of the vote, with the sum of parameters manually set to 50. This is a bit of technical mumbo-jumbo, but the gist is that a beta distribution allows you to control its “peaky-ness,” and I did this manually, whereas other forecasters had the model do it for them. Increasing the sum of parameters increases how peaky the distribution looks, and a sum of 50 was used based on Florian Muellerklein’s Georgia runoff model, which also used a sum of 50.\n\n\nSome Notes\nAs always, you can find source data and code on github. I’m pretty happy with how this turned out - I’ve been getting a bit more comfortable with R, and the tools used for this post were pretty intuitive to implement. I’m also happy with the color palette I selected (HTML Color Picker is a godsend). The only improvement is that I could/should have saved quite a bit of code by writing over one of the plot themes, rather than re-writing the theme for each plot. Something to remember going forward.\nNext week, I’ll dig into some of the county-level data from the election to see if there was any ticket splitting between the presidential election and the down-ballot races.\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {(Kind of) {Projecting} the 2020 {Election}},\n  date = {2021-02-21},\n  url = {https://www.thedatadiary.net/posts/2021-02-21-kind-of-projecting-the-2020-election},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “(Kind of) Projecting the 2020 Election.”\nFebruary 21, 2021. https://www.thedatadiary.net/posts/2021-02-21-kind-of-projecting-the-2020-election."
  },
  {
    "objectID": "posts/2021-02-28-doug-collins-saved-raphael-warnock-s-senate-bid/index.html",
    "href": "posts/2021-02-28-doug-collins-saved-raphael-warnock-s-senate-bid/index.html",
    "title": "Doug Collins Saved Raphael Warnock’s Senate Bid",
    "section": "",
    "text": "I’ll be honest - prior to writing this post, I had never heard of Doug Collins (R), the third major candidate in the race for Georgia’s special senate election after Raphael Warnock (D) and Kelly Loeffler (R). I wasn’t particularly tuned in to the Georgia senate elections prior to the runoff (by which time, the special election race had narrowed to just Warnock and Loeffler) and most of the coverage I had seen prior to Nov. 3rd pitted Warnock against Leoffler, without considering Collins. Even Google search trends show that people were more interested in Loeffler than Collins leading up to the election.\n\nDespite my ignorance, Collins was a major candidate in the special election, and ended up with a significant portion of the republican vote. It may be a bit obvious, but republicans splitting votes between Loeffler and Collins made Warnock a significantly more competitive candidate. However, the role of voter dropoff relative to the regular senate election is worth exploring in detail.\n\n\nComparing Results\nGeorgia’s regular senate election was a much more typical election than the special election - a tightly contested election between two candidates, Jon Ossoff (D) and David Perdue (R). The county result map comparing the voteshare of the two major candidates shows democratic strongholds in urban areas and a republican lean in rural areas. The county result map of the special election, if comparing Warnock to both Loeffler and Collins, is noticeably redder.\n\n\nIf we remove Collins, however, and just look at the county map comparing the top two candidates, the map shifts drastically in Warnock’s favor.\n\n\nAt face value, this explains how Warnock was able to advance to the runoff - split ticket votes aren’t usually good for the party with multiple major candidates. Voter dropoff between the regular and special election, however, shows just how much this pushed Warnock over the edge.\n\n\nVoter Retention: a Tale of Two Elections\nAbout 5 million Georgians voted in each of the senate elections in November. In the regular election, the major candidates, Ossoff and Perdue, accounted for about 98% of the votes, the rest going to Libertarian Shane Hazel or other write-in candidates. In the special election, on the other hand, a huge portion of Georgia voters didn’t vote for the major candidates - Warnock, Loeffler, and Collins only account for 79% of the votes! That’s about a 1 million voters who didn’t vote for their party’s major candidate (i.e., “dropped-off”).\nTo get a clear grasp of what voter retention means in this context, let’s consider a hypothetical county with 100 voters, of which 60 voted for Perdue and 40 voted for Ossoff in the regular election. Let’s also say these 100 voters split their special election votes in the following way: 35 votes for Loeffler, 15 votes for Collins, 30 votes for Warnock, and 20 write-in votes. In this scenario, Warnock retained 75% of regular election votes (30 / 40 = 75%), republicans, collectively, retained 83% ([35 + 15] / 60 = 83%), and Loeffler retained 58% (35 / 60 = 58%). In this hypothetical county, republicans collectively improved relative to the regular election, since they had a greater vote retention, but Loeffler on her own worsened, despite winning more votes than Warnock.\nIf we compare retentions for the entire state of Georgia, we can see that Loeffler and Collins were collectively better at retaining regular election votes than Warnock, retaining about 90% of Perdue votes compared to Warnock’s retention of about 68% of Ossoff votes. On her own, however, Loeffler was worse at retaining votes than Warnock, retaining a little over 50% of Perdue votes. This means that Warnock improved democratic performance relative to the regular election when compared against Loeffler directly.\n\nRepeating this comparison across every county in Georgia shows that Loeffler and Collins, collectively, improved republican performance relative to the regular senate election across every single county. When, however, Collins is omitted, and Warnock and Loeffler are compared directly, democratic performance improves across the majority of counties.\n\n\n\nIn Summary\nWhile it was obvious from the beginning that the split ticket hurt republicans in Georgia’s special senate election, it’s interesting to see just how much this split ticket helped Warnock. Collectively, republicans handily won the special election, but splitting votes between Loeffler and Collins meant that Warnock ended up winning a plurality of the votes.\nPolitical analysists are probably better than me at examining why multiple republicans ran as major contenders for the special senate election, but a quick take is that Kelly Loeffler wasn’t a particularly strong candidate. Although she was an incumbent, she wasn’t a senator any Georgian had ever voted for - she gained her senate seat through appointment. Loeffler came to office as a moderate, but quickly pivoted to a more Trumpy ideology, eventually defining herself as “more conservative than Atilla the Hun” in a campaign ad (I would normally just add a hyperlink, but this ad is just too weird not to link directly - I originally thought this was a bad parody).\n\nHer campaign was also plagued by a few scandals, like darkening Warnock’s skin in a campaign ad, taking photos with known KKK members, and potentially making stock trades based on COVID-19 information not-yet released to the public. It’s a bit of a conjecture, so take this with a grain of salt, but all of the above factors may have provided Collins an opportunity to run as a stronger & less divisive alternative to Loeffler.\n\n\nSome Final Notes\nChoropleth charts (maps) can be misleading in the wrong context - it’s easy to subconsciously associate area with population. It wasn’t particularly relevant to the post above, but in spirit of the animation “Land doesn’t vote, people do”, I added dot-plot maps where bubble size corresponds to total number of votes.\n\n\nAs always, source data and code can be found on github. I’m particularly happy with how this post turned out. I learned a lot of new things worth highlighting (some of this gets into technical mumbo jumbo):\n\nLearned how to use the maps, gganimate, and gifski packages;\nPicked up new method of piping objects into a ggplot object;\nWorked with geom_col and geom_poly for the first time;\nUsed forcats for the first time (just to reorder a factor, but I’ll still count it as a win);\nAdded new colors to the dd color palette;\nCreated a reusable theme var to reduce code;\nStarted using tibbles.\n\nFor this post, I had originally wanted to compare presidential vote to senate, governor, and house vote, but I had to pivot a bit to just the Georgia senate elections, for a few reasons. Firstly, the senate county dataset from Kaggle, for whatever reason, doesn’t include the winner of each county. Adding this by hand, just for Georgia’s 159 counties and two senate elections took hours. There’s probably a more efficient way to do this by scraping the data from online, but then I’d have to have learned a scraping package in addition to all of the other packages. I may have to hold on the original post idea for a bit while I pick up these skills.\nThat being said, I’ve already started working on next week’s post. It’s a bit of a departure from what I’ve been doing, so I’m excited to see how it’ll turn out.\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {Doug {Collins} {Saved} {Raphael} {Warnock’s} {Senate} {Bid}},\n  date = {2021-02-28},\n  url = {https://www.thedatadiary.net/posts/2021-02-28-doug-collins-saved-raphael-warnock-s-senate-bid},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “Doug Collins Saved Raphael Warnock’s Senate\nBid.” February 28, 2021. https://www.thedatadiary.net/posts/2021-02-28-doug-collins-saved-raphael-warnock-s-senate-bid."
  },
  {
    "objectID": "posts/2021-03-07-artwork/index.html",
    "href": "posts/2021-03-07-artwork/index.html",
    "title": "aRtwork!",
    "section": "",
    "text": "This week, I did something a bit different - the artwork above was made in R! The code to create the graph is actually pretty short, only taking up 28 lines. The bulk of the work was spent writing the csv containing all the polygon points and colors. I spent most of the past week doing some back end work that will pay off in the future. Namely, I set up a local clone of my repository on github and figured out how to embed Shiny applications on Squarespace.\nI’d like to address something that has been nagging me for a bit. Last week, I wrote about Georgia’s special senate race. I spent a lot of time learning new packages and methods, and am happy with how the plots and majority of the post turned out. Near the end of the post, however, I made the conjecture that Doug Collins may have seen an opportunity to run against Kelly Loeffler because she was a polarizing candidate with a string of controversies surrounding her campaign. While I shared sources that generally support this position, I didn’t back it up with any sort of data or in-depth analysis. I’m not a political scientist nor am I an expert in election strategy, so in hindsight I think it was pretty irresponsible to add my uninformed opinion to the post. I’ll leave the post up unedited, since it’s important to keep a true record of what I’ve written, but moving forward, I’ll be better about writing the story the data reveals, rather than one I’d like to believe.\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {aRtwork!},\n  date = {2021-03-07},\n  url = {https://www.thedatadiary.net/posts/2021-03-07-artwork},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “aRtwork!” March 7, 2021. https://www.thedatadiary.net/posts/2021-03-07-artwork."
  },
  {
    "objectID": "posts/2021-03-15-recent-works/index.html",
    "href": "posts/2021-03-15-recent-works/index.html",
    "title": "Recent Works",
    "section": "",
    "text": "One of the history teachers at my highschool was known for his prolific catchphrase, “give me the stuff, not the fluff,” often uttered to students attempting to submit history papers padded with superfluous words and sentences in order to meet a minimum word threshold. While I never took a class with this teacher, the phrase has stuck with me. With that in mind, I’ve reprioritized some of the things I’ve been working on. Here’s what I’ve been up to over the past week:\n\nTX COVID Tracker: Over the weekend, I put together and published my first Shiny application, a county-level interactive Texas COVID tracker. Using data from the New York Times, the tracker lets the user view county-level historical data. The goal is to fill in the gaps of the Times’ Texas tracker, which shows historical state data and live county data, but doesn’t offer historical county data.\n\nClasses: I had previously spent evenings working on projects in R and had pushed off some of the statistics classes to focus on these projects. This past week, I’ve scheduled an hour each day for dedicated class time, alternating daily between a Bayesian inference course and a machine learning course.\n\n\nMy plans for the upcoming week:\n\nTX COVID Tracker: While the interactive tracker is functional, it still leaves a lot to be desired. Being the first Shiny app I developed, I ran into a lot of learning curve issues but should be able to make updates more quickly. This upcoming week, I plan on converting to a bootstrap layout, adding in an interactive state map with leaflet, adding in statewide hospitalization and vaccination information, adding a state and county level log-log plot, making some formatting changes to help with mobile viewing, and updating themes/appearances. Eventually, I’d like to write a scheduleR script to automatically update the data, but that may need to be put off to focus on the laundry list above.\nClasses: I’ll still continue with the daily schedule for coursework and may be able to finish the Bayesian inference course this week.\n\nBetween the COVID tracker and classes, I don’t think I’ll have too much time to work on anything else. That being said, I’ve got some projects/articles in the backlog that I’m looking forward to working on:\n\nPolarization - top down or bottom up? It’s no secret that we are living in the most polarized political landscape since the Civil War, but I’m interested in exploring where this polarization originates. Do elected politicians split the public by pushing increasingly divisive policies? Or do voters lead the polarization effort by increasingly self-segregating into isolated camps?\nTwo Party PVI (Partisan Voter Index): The partisan voter index (PVI) is a measure of a state’s (or county’s) partisan preference relative to the national environment. There’s a story here about how the national environment is progressing - are our country voting preferences converging or diverging? What states are trending towards Democrats? What states are trending towards Republicans?\nThe Gas Price Fallacy: I’ve seen quite a few bad faith attempts to blame increasing gas prices on Joe Biden recently. Additionally, it’s particularly frustrating that I’ve seen a number of my colleagues in the oil & gas industry repeat this nonsense. As a working member of the oil & gas industry, I think I’m qualified and have a duty to explain why the administration change isn’t the cause of the rising gas prices (the TL;DR version is that we’re recovering from a global pandemic & oil oversupply, you can see that gas prices are just returning to pre-COVID Trump administration levelshere.)\n\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {Recent {Works}},\n  date = {2021-03-15},\n  url = {https://www.thedatadiary.net/posts/2021-03-15-recent-works},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “Recent Works.” March 15, 2021. https://www.thedatadiary.net/posts/2021-03-15-recent-works."
  },
  {
    "objectID": "posts/2021-04-05-seasonality-is-a-weak-predictor-of-border-crossings/index.html",
    "href": "posts/2021-04-05-seasonality-is-a-weak-predictor-of-border-crossings/index.html",
    "title": "Seasonality is a Weak Predictor of Border Crossings",
    "section": "",
    "text": "Last week, the Washington Post published an article postulating that the recent increase in crossings at the border cannot be attributed to Biden administration policies, but rather is a function of seasonal patterns and pent up demand due to restricted travel in 2020. While the article is well intentioned, a simple linear model that predicts crossings based on month shows seasonality is a poor predictor (or, at least is a poor predictor on its own).\n\nExamining the Data\nThe US Border Patrol (USBP) publishes monthly data summarizing the number of encounters/crossings at the Southern border. Since November of 2020, border crossings have increased monthly, from about 70,000 in November to a little over 100,000 in February.\n\nLooking at the long term monthly data, however, reveals two key aspects:\n\n2019 was an exceptionally high year for crossings, and 2021 is on track to have a similar number of crossings;\nThere seems to be a cyclical set of peaks and troughs in annual border crossing numbers.\n\n\nLooking at the chart above, it’s difficult to determine which months correspond to peaks and troughs. Aligning by month offers a clearer picture of how monthly border crossings in each year compare to other years.\n\nFrom this chart, it does appear that there is some semblance of a monthly pattern, albeit with extreme outliers of 2019, 2020, and 2021. Appearances, however, can be deceiving, and a mathematical model can help distinguish signal from noise. Using month as a predictive input, we can create a model that estimates the expected number of crossings in said month. If we look at the residual error (the actual number of crossings minus the predicted number of crossings) over time, we can get an idea of how well the model is performing. If seasonality is a good predictor of the number of border crossings, the model will accurately account for seasonal shifts, and the residual error plot would be expected to remove or reduce the “waviness” that appears in the crossing plot and only show the long term trends.\n\nPlotting the residual error, however, doesn’t filter out seasonal noise. In fact, apart from the y-axis shifting down, the residual plot is strikingly similar to the crossings plot. This means that the month offers little power on its own in predicting the number of border crossings.\n\n\nClosing Remarks\nThe Washington Post article makes good points that aren’t discussed fully here, but are worth mentioning:\n\nThose who planned on crossing the border in 2020 but couldn’t due to travel restrictions may have simply waited until now to travel.\nUnaccompanied migrants are arriving at the border in rates that exclude the possibility of seasonal patterns.\n\nAlso notable is that seasonal patterns - while not a good predictor of the number of border crossings on its own - may be more impactful in a more robust model. As a professor from the University of San Diego explains, conditions in the migrant’s country of origin and the countries they pass through are among a host of variables that play a significant role in determining if migrants make the trip to the US border. While a more robust model may be able to accurately incorporate season into its predictions, simply using month as a predictor, as the Washington Post does in their article, does not explain the surge at the border (evidenced by the fact that this equally simple model refutes their finding).\nFinally, it should be noted that, above all else, there is a humanitarian crisis at the border. If you’d like to find a way to help or donate, Global Giving is an excellent resource.\n\n\nSome Other Updates\nHere’s what I’ve been working on recently:\n\nTidyTuesday: I’ve made my first contribution to #tidytuesday! For those who are unaware, TidyTuesday is a weekly data project in R for the R4DS (R for data science) community. Each week, a public dataset is released on Monday, allowing users to explore and create interesting visualizations and analyses based on the dataset. This week, a dataset based on global deforestation was released, but it came along with interesting information on the production of vegetable oil. I plotted the global annual vegetable oil production by crop type in the steamgraph below.\n\n\n\nCourses: I’ve continued with the machine learning and Bayesian courses, though I’ve descoped to roughly two days each week, as my personal schedule has gotten quite hectic. The ML course is excellent for understanding the theory that goes into each algorithm, but in practice, I expect that I will likely make use of the tidymodels R package’s relatively simple interface.\nRmd: I’ve started writing my code in a R markdown document, which allows me to mix both code and prose. I mix in some off-the-rails/stream of consciousness commentary as I step through the process to get to a functional code. For regularly updated/critical code/projects, I’ll still stick to concise scripts.\n\nThat’s all for this week - as always, you can read through the code for this piece or for the tidytuesday piece on github. See you next week.\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {Seasonality Is a {Weak} {Predictor} of {Border} {Crossings}},\n  date = {2021-04-05},\n  url = {https://www.thedatadiary.net/posts/2021-04-05-seasonality-is-a-weak-predictor-of-border-crossings},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “Seasonality Is a Weak Predictor of Border\nCrossings.” April 5, 2021. https://www.thedatadiary.net/posts/2021-04-05-seasonality-is-a-weak-predictor-of-border-crossings."
  },
  {
    "objectID": "posts/2021-04-20-catching-up/index.html",
    "href": "posts/2021-04-20-catching-up/index.html",
    "title": "Catching Up",
    "section": "",
    "text": "The past few weeks have been a bit lite (read::absent) in terms of posting updates. When not taking the machine learning course, I’ve been spending quite a bit of time working on building out a database of demographic data (from 1980 to 2019). The goal is to be able to build this database out once, then reference it repeatedly for future projects. If you’re so inclined, you can read my unedited ad-hoc thoughts while working on the database here. There’s lots of code interspersed with prose, so it’s a bit of a slog to read through, but gives a really detailed account of how I worked through the different problems that arose.\nThat being said, I did have a bit of time to squeeze in a quick chart. YouGov recently conducted a poll on each state’s favorability rating according to Democrats and Republicans. Not surprisingly, state favorability is closely aligned with Biden’s voteshare in November.\n\nI’ll keep this short and sweet. I may continue to write sparsely over the next month or so, due to a few trips (now that I have the COVID vaccine), but will continue to work on the database in the background.\nAs always, you can find my work on Github.\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {Catching {Up}},\n  date = {2021-04-20},\n  url = {https://www.thedatadiary.net/posts/2021-04-20-catching-up},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “Catching Up.” April 20, 2021. https://www.thedatadiary.net/posts/2021-04-20-catching-up."
  },
  {
    "objectID": "posts/2021-04-27-a-lukewarm-case-for-dc-statehood/index.html",
    "href": "posts/2021-04-27-a-lukewarm-case-for-dc-statehood/index.html",
    "title": "A Lukewarm Case for DC Statehood",
    "section": "",
    "text": "Once again, I’ll be keeping this very short, as I’ve continued to primarily focus on building out the demographic/economic database. I, did, however, sneak in some time to explore the 2020 Census Apportionment Results that were released yesterday. Some surprises came out of the release: Texas and Florida underperformed expectations, only gaining two and one seat, respectively; Arizona didn’t gain a seat; and New York lost a seat but was apparently 89 people short of retaining its seats. Given that Texas and Florida each gained one less seat than was generally expected, there’s been some speculation online that Donald Trump’s attempt to undercount Hispanics was successful, but backfired. I’ll hold my judgments on this theory until I can read some more in-depth opinions (if they get written).\nWith the new apportionment and population data, each individual’s representative power in congress has shifted. Including both Senators and House Representatives, states with smaller populations continue to have outsized representation in congress: Wyoming and Vermont have significantly more representatives per million residents than California or Texas, for example. Residents of Washington D.C., however, receive no representation in congress.\n\n\n\nAs a side note, the reps-per-million will be greater than the total number of congressional representatives in states with less than one million residents. This chart is really meant to compare the uneven representative power of each state.\nIf D.C. were to become a state, it would have outsized representative power, similar to Wyoming and Vermont, due to its size. That, however, isn’t really a justification to deny ~700,000 Americans national representation, and the arguments against D.C. statehood have been weak at best and implicitly racist at worst, given D.C.’s majority-minority population.\nStill, it’s virtually impossible that D.C. statehood receive the filibuster-proof 60 votes in the Senate (I’d doubt that even a compromise of retrocession into Maryland would pass). For the time being, residents of D.C. will have to wait for representation until polarized ideological walls come down and Republicans vote for D.C. statehood, Democrats gain enough seats for a supermajority, or the filibuster is abolished.\nAs always, you can find my work on Github.\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {A {Lukewarm} {Case} for {DC} {Statehood}},\n  date = {2021-04-27},\n  url = {https://www.thedatadiary.net/posts/2021-04-27-a-lukewarm-case-for-dc-statehood},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “A Lukewarm Case for DC Statehood.” April\n27, 2021. https://www.thedatadiary.net/posts/2021-04-27-a-lukewarm-case-for-dc-statehood."
  },
  {
    "objectID": "posts/2021-05-05-president-of-the-polls/index.html",
    "href": "posts/2021-05-05-president-of-the-polls/index.html",
    "title": "President of the Polls",
    "section": "",
    "text": "Elliot Morris wrote in his newsletter last week about the increase in the public’s opinion that the government ought to be doing more. A summary of the last twenty years of polling by NBC shows that, while there seems to be a reactive effect based on the party of the president, support for increased government activity has generally been on the rise.\n\nThe combination of Biden’s approval across several categories, public support of his proposals (with the exception of his original cap on refugees, which he then raised after public outcry), and the public support of increased government activity gives Biden a strong argument in pressing congress to get his progressive policies pushed through to his desk. Despite the popularity, his $4 trillion infrastructure and families plan largely depends on what the senate parliamentarian will allow into a reconciliation bill (which only needs majority approval, rather than a filibuster-proof 60 votes) and the votes of a few moderate Democratic senators, since the bills will receive no Republican support in the senate.\n\nOutlook for the Coming Weeks\nI’ll be taking a break for the next few weeks, due to a flurry of weekend trips/weddings now that I/most of my friends are fully vaccinated. I’ll continue to work on the database in the background (notably, I need to dive deep into the census bureau demographic data, which needs a lot of fixing). I’ll likely write a short update in either late May or early June.\nAs always, you can find my work on Github.\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {President of the {Polls}},\n  date = {2021-05-05},\n  url = {https://www.thedatadiary.net/posts/2021-05-05-president-of-the-polls},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “President of the Polls.” May 5, 2021. https://www.thedatadiary.net/posts/2021-05-05-president-of-the-polls."
  },
  {
    "objectID": "posts/2021-06-03-covid-cases-improve-with-introduction-of-vaccines/index.html",
    "href": "posts/2021-06-03-covid-cases-improve-with-introduction-of-vaccines/index.html",
    "title": "COVID Cases Improve with Introduction of Vaccines",
    "section": "",
    "text": "As vaccines have become widely available in the US, new COVID cases and deaths have dropped significantly from winter peak, with the 7-day average for both cases and deaths nearing the averages reported during the initial lockdown in the summer of last year (note that, in the chart for the 7-day average of deaths below, the scale is restricted to 15 deaths per million; this cuts off some of the state surges, but shows the US average better than a chart with an unedited scale).\n\n\nAccording to the US Vaccine Tracker, 41% of Americans are fully vaccinated as of today. Even more heartening is the fact that the partisan divide regarding vaccine hesitancy, while still existent, is shrinking.\nWhile this is certainly good news about the country as a whole, individual states, counties, and cities may have varying levels of success in curbing the spread of the virus in the local community. The New York Times created a helpful dashboard that lets you look at the spread of the virus on a state, county, or metro area scale.\nAll this is to say that, while the pandemic is still not (and possibly will never be) a thing of the past, the introduction of vaccines certainly has appeared to help curb the spread.\n\nCatching Up\nIt’s been a while since I’ve written, due to a flurry of weddings and trips. While I had originally hoped to get back onto a regular weekly writing schedule, I will again have to push out the next post until late June, due to some exciting personal news: June 21st, I’ll be starting a new job! I’ll be transitioning from engineering in the oil & gas industry to healthcare analytics, which I’m really excited about! In the meantime, however, there’s a flurry of work to do in my current job to ensure that the person backfilling me is prepared to pick up the projects I’m working on. Hopefully after that, I can get back to a regular schedule.\nAs always, you can find my work on github.\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {COVID {Cases} {Improve} with {Introduction} of {Vaccines}},\n  date = {2021-06-03},\n  url = {https://www.thedatadiary.net/posts/2021-06-03-covid-cases-improve-with-introduction-of-vaccines},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “COVID Cases Improve with Introduction of\nVaccines.” June 3, 2021. https://www.thedatadiary.net/posts/2021-06-03-covid-cases-improve-with-introduction-of-vaccines."
  },
  {
    "objectID": "posts/2021-07-11-blexas/index.html",
    "href": "posts/2021-07-11-blexas/index.html",
    "title": "Blexas?",
    "section": "",
    "text": "Last year, Donald Trump won Texas’ 38 electoral votes handily, earning 52% of the vote compared to Biden’s 46% (R+6). Taken on its own, this is hardly surprising - Texas has gone to the Republican presidential candidate in every election since 1980. Looking at the relative gains made in Texas over recent election cycles, however, paints a much more hopeful picture for Democrats hoping to flip the state & garner the nickname, “blexas.”\nOn election day, FiveThirtyEight and the Economist gave Biden a 38% and 30% chance of winning in Texas, respectively. While still bearish on Biden, this is far more bullish than 2016, when FiveThirtyEight’s model gave Clinton a mere 5% chance of winning in Texas (in fact, both FiveThirtyEight and the Economist were more confident in Biden’s chance of winning Texas in 2020 than FiveThirtyEight was that Trump would win the presidency in 2016 (28%)). Part of this shift can be attributed to Democratic gains nationally, but Texas Democrats have been steadily gaining more ground than can be explained by national swing.\n\nThe Cook Political Report publishes its Partisan Voter Index (PVI) following each presidential election. PVI is a measure of how a state, district, or county votes relative to the national environment. For example, say a Democratic candidate wins 53% of the two-party voteshare nationally, but 51% in a given state. Despite the state going to the Democratic candidate, the state PVI would be R+2%, since the candidate performed 2% under the national vote (the actual PVI calculated in the Cook report is slightly more involved, but this basic understanding is sufficient for our purposes). Looking at Texas’ PVI over past elections, we see that Democrats have been making relative gains in every election since 2004, despite Texas still voting more Republican than the nation.\nMuch of this can likely be attributed to demographic shifts drive by major Texas cities - Dallas, Houston, Austin, and San Antonio have seen massive population increases, largely due to domestic and international migration (as opposed to natural changes - e.g., births). As Texas grows, it also continues to diversify. The non-hispanic white population in Texas dropped from 45% in 2010 to 41% in 2019, and non-white population groups have driven growth over the past 10 years.\n\nAll that being said, while I expect that Texas will continue to experience demographic shifts that are favorable in the eyes of Democratic politicians, in the absence of any real modeling work, I’m hesitant to say that Texas will turn blue in the near term. Despite gains in the presidential results, Texas Democrats didn’t outperform expectations in the house, nor did they even advance a candidate in the special election for TX-06 (two Republican candidates advanced to a runoff). Governor Abbott is also introducing a special legislative session with one goal (of many) of making Texas a state with some of the most restrictive voting laws with targeted partisan effects. Looking to other states as a reference, North Carolina had also seen similar PVI shifts in the past without resulting in Democratic victories (though, to be fair, North Carolina hasn’t experienced the same level of demographic shift that Texas is undergoing).\nPerhaps there is some modeling work I can do to produce a more definitive stance, but until then, I’ll hold on making any bold predictions.\nAs always, you can find my work on github.\n\nGeneral Updates\nIt’s been quite a while, but it’s good to get back into writing again. I wrote the scripts/made the charts for this post about two weeks ago, but haven’t had the time (or rather, haven’t made the time) to write the post itself. While I feel like I’ve been saying this for three months now, I do hope to get back to a more regular schedule soon - perhaps biweekly, to avoid rushing projects. We’ll see - in any regard, here are some updates that I’m excited about:\n\nI started my new job a few weeks ago! It’s both very rewarding (I get to use R & work with large polling datasets daily!) and very demanding. I enjoy the challenge & am excited for upcoming projects that I’ve been tasked with.\nI’ve been steadily making headway against the Machine Learning course I’ve been taking - at this point, I’m about halfway finished.\nI’ve begun to poke around in the tidymodels framework. I’m interested in the standardization the packages supply for generating models in R.\n\n\nSome reading/viewing material:\n\nI linked to it above, but it’s worth reading through this NYT article on the special legislative session in Texas. I only touched on voting rights, but session is primed as another hot-button issue session following spring’s ultra-conservative agenda.\nChecks and Balance, the Economist’s weekly podcast, focused on Critical Race Theory & the troubled history of race in American education in their most recent podcast, “History Test.” It’s well worth a listen, especially if you’re like me, and had never heard of CRT until recently.\nJust over 6 months ago on January 6th, violent Trump supporters stormed the U.S. capitol. With time and overt lies being propagated by Fox News, OANN, and Newsmaxx, it’s easy to forget exactly what happened and how it felt on that day. This NYT Visual Investigation does an excellent job of reconstructing the timeline of the events that day.\n\n\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {Blexas?},\n  date = {2021-07-11},\n  url = {https://www.thedatadiary.net/posts/2021-07-11-blexas},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “Blexas?” July 11, 2021. https://www.thedatadiary.net/posts/2021-07-11-blexas."
  },
  {
    "objectID": "posts/2021-07-18-the-gas-price-fallacy/index.html",
    "href": "posts/2021-07-18-the-gas-price-fallacy/index.html",
    "title": "The Gas Price Fallacy",
    "section": "",
    "text": "As America has vaccinated its population, life has seemingly begun to return to the pre-pandemic normal. Businesses have been opening to higher levels of capacity, schools are planning for higher levels of in-person learning in the fall, and, notably, Americans have returned to pre-pandemic levels of transit.\nConservative commentators and representatives have pointed out that prices of commodity goods - in particular, gasoline - have skyrocketed over the past year and attribute this increase to Biden’s presidency. Looking at retail gasoline prices, we can see a steady increase since Biden took office.\n\nWhat they fail to point out, however, is that the price of gasoline (and other commodities) significantly dropped during the pandemic, and that the increases are largely a return to pre-pandemic prices.\n\nWhile there are certainly opportunities to critique the current administration’s energy policy (banning new drilling leases on federal land/water and canceling the Keystone XL pipeline’s border-crossing permit, for example), attributing the recent return to pre-pandemic gasoline prices is a particularly weak and intentionally misleading avenue of attack.\nAs always, you can find my work on github.\n\nSome Reading/Viewing Material\n\nThe Economist wrote an interesting article finding a significant link between a county’s in-person voting rate and new COVID cases in November.\nElliot Morris summarized the work of several political scientists in a recent article describing why the GOP slid so far towards authoritarianism in the past decade. The article is behind a subscriber paywall, but is well worth the read. In short, however, “ethnically antagonistic” voters are much more likely to agree with statements traditionally viewed as authoritarian. This group hadn’t coalesced under a single party until Trump brought these anti-racial voters into the party (recall his role in the birther conspiracy and the racial antagonism surrounding his 2016 campaign), at which point democratic (note, small “d”) norms became a partisan issue.\nIn my machine learning class, I’ve started learning about the implementation of neural networks. The course’s instruction style is highly technical, but Grant Sanderson’s series on neural networks has helped me align a technical and intuitive understanding of the topic.\n\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {The {Gas} {Price} {Fallacy}},\n  date = {2021-07-18},\n  url = {https://www.thedatadiary.net/posts/2021-07-18-the-gas-price-fallacy},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “The Gas Price Fallacy.” July 18, 2021.\nhttps://www.thedatadiary.net/posts/2021-07-18-the-gas-price-fallacy."
  },
  {
    "objectID": "posts/2021-08-08-tidymodels-and-the-titanic/index.html",
    "href": "posts/2021-08-08-tidymodels-and-the-titanic/index.html",
    "title": "Tidymodels and the Titanic",
    "section": "",
    "text": "This week, I thought I’d do something a bit different. I’ve been working with & getting used to tidymodels, a suite of R packages for building machine learning models with tidyverse principles (you can thank Julia Silge’s blog for providing a whole host of examples and walkthroughs). Using tidymodels and Kaggle’s Titanic dataset, I created a few simple models to predict whether or not each passenger survived.\n\nExploratory Data Analysis\nFirst off, it’s important to get to know the dataset in a bit of detail before diving into model building. The Titanic dataset is relatively small, containing 12 columns of data on 891 passengers.\n\nPassengerId : ordered number assigned to each passenger (1, 2, 3… etc.).\nSurvived : indicates whether the passenger survived or not (imported as numeric, where a 1 means that the passenger survived and a 0 means that the passenger died.\nPClass : passenger class (1st, 2nd, or 3rd).\nName : passenger’s name.\nSex : passenger’s sex.\nAge : passenger’s age.\nSibSp : number of the passenger’s siblings and/or spouses aboard the ship.\nParch : number of the passenger’s parents and/or children aboard the ship.\nTicket : passenger’s ticket number.\nFare : price of the passenger’s ticket.\nCabin : passenger’s cabin number.\nEmbarked : port from which the passenger embarked.\n\nThe passenger’s name and ticket number are unique, and don’t offer any much predictive power (at least, not for the type we’ll be deploying), so I’ve removed those columns from the selection. The majority of passengers are missing Cabin information, so I’ve similarly removed that column. Let’s take a look at the age distribution by gender. Density plots are good for this sort of objective, but a quick note - density plots don’t show the counts (like a histogram), just the relative distribution of each group.\n\nBoth men and women have similar age distributions, with the majority being adults roughly in their twenties. It’d be interesting to look at which age groups survived.\n\nThere’s a spike survivorship at a young age (as expected, due to the prioritization of children). Young adults make up the majority of both survivors and those who died, simply because young adults made up the majority of passengers. It may be more interesting to see which male/female age groups survived.\n\nInterestingly, there’s a large portion of the women who died were children. To gain some more insight, however, it may be beneficial to bin the results. As mentioned above, density plots don’t show a count of each age, but the relative amounts instead. We can use a histogram to get a better idea of the actual amount in each group.\n\nFrom the histogram, we can see that there were far more male passengers than female passengers. Let’s setup age brackets and see how many of each age group survived.\n\nDespite there being more male passengers, there were more female survivors in just about every age bracket. I had expected that women would have a greater percentage of survivorship, but was mildly surprised that the absolute number of female survivors was greater than the number of male survivors.\nLets look at how survivorship is related to the number of family members each passenger was traveling with.\n\nWhile there’s not huge variation between survivorship percentages across each class, we can see that the majority of passengers were traveling alone.\nFinally, I looked at the distribution of the fare price by survivorship. As expected, passengers who didn’t survive skewed towards low cost fares while survivors were more evenly distributed across fare prices.\n\n\n\nModel Fitting and Evaluation\nNow that we’ve explored the data a bit, we can get started on some actual model fitting and evaluation. I split the dataset into a training set and test set, and built two simple models - a logistic regression and a random forest model. In very light detail, a logistic regression is the most basic classification model based on the generalized linear model and a random forest model is a decision tree-based algorithm. With no tuning applied, neither model performed particularly well - the logistic regression and random forest models had accuracies of 85.5% and 83.8% on the test set, respectively. While the logistic regression had a slightly higher accuracy on the test set, the random forest performed better on the metric of area under the ROC (receiver-operator characteristic) curve - 0.899 compared to 0.888 (generally, binary classification systems with ROC AUC values close to 1 perform better). A comparison between the two ROC curves is shown below.\nIt’s not perfect, but the purpose was to explore the tidymodels framework, and I’m pretty happy with the outcome/practice along the way.\n\n\n\nSome Things to Read/Watch/Listen to\n\nThere’s been a concerted effort by prominent conservatives to promote vaccination and shift the blame for vaccine hesitancy to the Biden administration, pointing in particular to one of Kamala Harris’ comments during the Vice Presidential debate. This criticism falls fairly short, given that commentators like Tucker Carlson and legislators like Marjorie Taylor Green continue to spread vaccine conspiracy theories. Further, a statistical model by the Economist found that the strongest predictor of whether or not someone has been vaccinated is who they voted for in the 2020 presidential election.\nJulia Silge was recently featured as a guest on an excellent episode of the Not So Standard Podcast. Well worth a listen for R dorks like myself.\nPerry Bacon Jr wrote an informative piece on the misplaced importance on swing voters in election cycles.\n\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {Tidymodels and the {Titanic}},\n  date = {2021-08-08},\n  url = {https://www.thedatadiary.net/posts/2021-08-08-tidymodels-and-the-titanic},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “Tidymodels and the Titanic.” August 8,\n2021. https://www.thedatadiary.net/posts/2021-08-08-tidymodels-and-the-titanic."
  },
  {
    "objectID": "posts/2021-09-23-catching-up-again/index.html",
    "href": "posts/2021-09-23-catching-up-again/index.html",
    "title": "Catching Up (again)",
    "section": "",
    "text": "Wow, it’s been quite a while! Once again, I’m catching up from a long hiatus (this time, there was about a month’s gap between posts). Although I haven’t been writing here, I have been keeping myself incredibly busy - for that reason, I think I’m going to switch up the format of this site. Here’s a few changes I’m planning on making, & also why I plan on making them.\n\nOpening up topic options\nUp until now, I’ve pretty much exclusively been looking at politically-related things from a data-centric point of view, partially because most news outlets are sensationalist and it annoys me, and partially because the 2020 election & aftermath was on my mind for most of early 2021 (also, wanting to understand the Economist’s Election Forecast methodology was the spark that led to taking stats classes, learning R, and [eventually] landing a new job). While I still want to/expect that I will touch on political-esque topics, I want to expand into things I’m more generally interested in. I’ve spent a lot of time this year working on projects that don’t touch politics and it was difficult to spur the enthusiasm to work on a political project just to fill the site after having spent hours coding something unrelated. I’ve imposed this political filter on myself, & expanding the horizon to include any/all topics that interest me will allow me to more consistently write without overextending myself.\n\n\nMoving away from Squarespace\nCurrently, this site is hosted by Squarespace. This was great for getting setup & used to working online, but after a year of working with it, I’ve realized that it isn’t the ideal platform for my use case. Early next year, hopefully, I’ll switch over to using rmarkdown, Hugo, and Netifly to build & deploy a site directly from R. This will make my life a lot easier, for a couple of reasons:\n\nI will have a lot more control over layouts and themes\nI’ll be able to more easily share code, images, and interactives(!)\nThe code & the website will be housed under one directory (rather than existing as two separate entities)\n\nAdditionally, the platform I’m planning on moving to is more blog/personal-professional focused, rather than retail focused, like Squarespace. I may also be able to move towards having posts appear in emails, rather than having to link to the post itself.\n\n\nTimeline\nI haven’t yet settled on this yet, but I may move from a weekly schedule to a bi-weekly schedule (not that I ever really stuck to the weekly schedule). The projects I’ve been working on typically take quite a bit of time, so trying to churn one out each week means that I may have to suffer quality for a schedule, which I don’t want to do. I’d rather have enough time to fully devote to completing something.\nAnyways, that’s a lot of updates - I’ll hopefully check in within a few weeks time!\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {Catching {Up} (Again)},\n  date = {2021-09-23},\n  url = {https://www.thedatadiary.net/posts/2021-09-23-catching-up-again},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “Catching Up (Again).” September 23,\n2021. https://www.thedatadiary.net/posts/2021-09-23-catching-up-again."
  },
  {
    "objectID": "posts/2021-10-05-rstudio-s-call-for-documentation/index.html",
    "href": "posts/2021-10-05-rstudio-s-call-for-documentation/index.html",
    "title": "RStudio’s Call for Documentation",
    "section": "",
    "text": "In my job, I spend a good amount of time working in the platform setup by our survey vendor, Qualtrics. There are some pre-formatted reports that we can send on a recurring basis through Qualtrics, but for one-off or custom reports, I can work with the raw data in R. A few weeks ago, however, I was asked to setup a recurring email to send a customized report to a group of hospital directors each week. While R makes generating the report simple, sending out each week was tedious, as each one needed to be sent separately. Since the reports contain patient information, I couldn’t automate via a third party server like GitHub Actions. I needed a way to localize the automation to my computer.\nLuckily enough, I was able to work out a solution! I wrote about it in a submission to RStudio’s 2021 Call for Documentation. You can read my article on Automated Email Reports with R, VBA, and the Task Scheduler for more detail.\n\nSome programming notes\n\nI’ve been named one of RStudio Community’s New Users of the Month!\nI’ve started working in Rmarkdown for these posts (you can actually view the native file for this post here). This allows me to mix in code and prose in one document & should make the process a bit easier on my end.\nI’ve scheduled out posts every two weeks for the rest of the year. I wanted to give myself enough time to work on some longer term projects, so be on the lookout for more in store!\n\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {RStudio’s {Call} for {Documentation}},\n  date = {2021-10-05},\n  url = {https://www.thedatadiary.net/posts/2021-10-05-rstudio-s-call-for-documentation},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “RStudio’s Call for Documentation.”\nOctober 5, 2021. https://www.thedatadiary.net/posts/2021-10-05-rstudio-s-call-for-documentation."
  },
  {
    "objectID": "posts/2021-10-19-polling-average-of-the-va-governor-s-race-using-purrr-map-functions/index.html",
    "href": "posts/2021-10-19-polling-average-of-the-va-governor-s-race-using-purrr-map-functions/index.html",
    "title": "Polling Average of the VA Governor’s Race using purrr::map functions",
    "section": "",
    "text": "Rolling poll averages can be misleading in the absence of errorbars or an expected distribution of outcomes. FiveThirtyEight is currently tracking polls of Virginia’s Governor race slated for early November, and has kindly made their polls available to the public. Their current polling average, however, looks to be a simple rolling average and doesn’t include a confidence interval. I’ve attempted to improve upon their tracker here by providing a weighted polling average and a 95% confidence interval.\n\n\nHow this works\nSince we’re only considering the top candidates from each party, we’ll look at each candidate’s two-party voteshare of each poll. To get the two-party voteshare, third party or other minor candidates are removed from each poll and each candidate’s percentage is recalculated as if they were the only two options on the ballot (in practice, this only removes a tiny amount of undecideds and third party voters). Then, the daily polling average is calculated by weighting each poll by sample size and recency. Using Bayes’ theorem and a weak uniform prior, we can use the same method recalculate the polling average and confidence interval for each day between today and the election. Because polls are weighted by recency, as we look further and further into the future, our confidence in the polls decreases and the confidence interval around the polling average fans out. Each candidate’s probability of winning is the portion of the portion of the projected election-day polling distribution in their favor, based on that day’s polling average.\n\n\nSome caveats worth noting\nThis is an inherently flawed method, and it’s worth pointing out a few of the flaws and shortcuts I used:\n\nThe functions used to weight polls are nowhere near perfect.\nThe original weighting functions (which I haven’t changed) were chosen somewhat arbitrarily. In hindsight, they’re probably placing too much emphasis on recency and the error bars ought to be larger. While I have received some advice on tuning arbitrary functions as a part of a larger model, I haven’t implemented here. It’s more prudent to think of this as an over-confident polling aggregate, rather than any sort of model. For a true projection model, I’d recommend looking at Jack Kersting’s website.\n\n\nThe weighting method ignores important weighting factors\nThis weighting method is super simple and ignores common weighting factors, like pollster and survey methodology. Other less-common poll weighting methods, like accounting for partisan non-response bias and and how the pollster weights their results (notably, whether or not the pollster weights by education) were similarly ignored. There is definitely a strong argument for including these weighting factors, but for me, this exercise was more about learning to use purrr::map() and other related functions when writing the script for this plot.\n\n\nSome pollsters are filtered out by design\nI debated this for quite some time, but decided to add a filter to remove polls conducted by Rasmussen and Trafalgar. Trafalgar is excluded from any of the Economist’s polling databases for opaque yet clearly shoddy methodology and Rasmussen is clearly partisan. Removing these from the average follows the general consensus on ET (though, to be transparent, ET does tend to slant far to the left). In future polling projects, I’d hope to develop some more robust methodology to programatically downweight problematic pollsters (how’s that for a tongue twister?), but for now I’m just going to exclude.\n\n\n\nFinal thoughts on polling\nThis methodology certainly has its flaws, but it is transparent. I’ll continuously update this plot up until election day on my github.\n\n\nSome programming notes\nI’ve finished migrating my site to Netlify! I had originally planned to make the switch from Squarespace sometime early in 2022, but motivation struck me during a relatively light work week and I was able to rebuild the site using blogdown. This allows for a lot more customization and control than was available with Squarespace, but the biggest upside is definitely the ease of sharing in-line code, here’s a quick example:\n\n\nCode\nlibrary(tidyverse)\n\n# let's put together a plot from the diamonds dataset\ndiamonds %>%\n  ggplot(aes(x = carat,\n             y = price,\n             color = cut)) +\n  geom_point(alpha = 0.1) +\n  theme_minimal() +\n  viridis::scale_color_viridis(discrete = TRUE)\n\n\n\n\n\nThis ease of use and visibility will make things more seamless for me and allow me to dig into more technical content in more detail in the future!\nI’ll be taking a (much needed) vacation next week, spending some time off the grid in the Grand Canyon and surrounding area with my family. I’ve got a short post lined up for early November when I return - see you then!\nAs always, you can find the source files for the script to generate the polling average and for this site on my github.\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {Polling {Average} of the {VA} {Governor’s} {Race} Using\n    Purrr::map Functions},\n  date = {2021-10-19},\n  url = {https://www.thedatadiary.net/posts/2021-10-19-polling-average-of-the-va-governor-s-race-using-purrr-map-functions},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “Polling Average of the VA Governor’s Race Using\nPurrr::map Functions.” October 19, 2021. https://www.thedatadiary.net/posts/2021-10-19-polling-average-of-the-va-governor-s-race-using-purrr-map-functions."
  },
  {
    "objectID": "posts/2021-11-02-election-night-some-closing-thoughts-on-the-va-governor-race/index.html",
    "href": "posts/2021-11-02-election-night-some-closing-thoughts-on-the-va-governor-race/index.html",
    "title": "Election Night: Some Closing Thoughts on the VA Governor Race",
    "section": "",
    "text": "A few weeks ago I had written about the VA Governor’s race before going on vacation - in that time it seems as though Terry McAuliffe’s campaign had lost a lot of steam and Youngkin made up a lot of ground in the final weeks of the campaign. At the time of this writing, it seems overwhelmingly likely that Glenn Youngkin will become the next governor of Virginia. To avoid some of the galaxy-brain takes that will inevitabely wind up twitter, I thought I’d distract myself by following up on my previous post.\nFirstly, I should share the updated polling average. A few weeks ago, McAullife appeared to have a sizeable lead in the polls:\n\nHowever, as of election day, the race had significantly tightened to effectively a coin-toss:\n\nAs I mention in the above tweet, the win probability isn’t a true forecast, just the portion of each candidate’s election day distribution above 50%. That being said, actual forecasts similarly had the race down to a near 50-50 split as of this morning:\n\n\nEven the model most confident in McAuliffe built by Lakshya Jain and Thorongil had dropped McAuliffe’s win probability from ~85% to 67% over the course of a few weeks:\n\nWhile I definitely plan on utilizing a more scientific poll-weighting methodology in the future, I do find it interesting that even a simple averaging method can produce relatively accurate results in line with the majority of other forecasters.\nRegarding post-hoc analysis of why McAuliffe lost, I won’t dredge up any of my own (partially because it’d be irresponsible & pundit-y to do so without referencing any data and partially because it’s getting late & I’m a bit tired), but I’ll point out a few tweets from Nate Cohn that show that the results appear to show a near uniform shift across precincts and different voting groups. This would suggest that McAuliffe’s loss is tied more closely to the national environment, rather than shifts amongst specific groups/counties.\n\n\nThis won’t stop the networks from ascribing the win/loss to very specific campaign issues (I’ve already seen quite a few folks ascribe Youngkin’s win to education, race, suburban-reversion, etc., without any evidence to back up such claims). Until there are deep dives into data regarding the election, I’d treat any comments from pundits with a hefty grain of salt.\n\nSome closing thoughts\nThat’s all for me today! I’ll be back in a few weeks with some non-political content, looking at a machine learning model predicting the price of a diamond in the diamonds dataset.\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {Election {Night:} {Some} {Closing} {Thoughts} on the {VA}\n    {Governor} {Race}},\n  date = {2021-11-02},\n  url = {https://www.thedatadiary.net/posts/2021-11-02-election-night-some-closing-thoughts-on-the-va-governor-race},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “Election Night: Some Closing Thoughts on the VA\nGovernor Race.” November 2, 2021. https://www.thedatadiary.net/posts/2021-11-02-election-night-some-closing-thoughts-on-the-va-governor-race."
  },
  {
    "objectID": "posts/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index.html",
    "href": "posts/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset/index.html",
    "title": "Diamonds are Forever",
    "section": "",
    "text": "Are y’all ready for some charts?? This week, I did a bit of machine learning practice with the diamonds dataset. This dataset is interesting and good for practice for a few reasons:\n\nthere are lots of observations (50,000+);\nit includes a mix of numeric and categorical variables;\nthere are some data oddities to deal with (log scales, interactions, non-linear relations)\n\nI’ll be doing a bit of feature engineering prior to fitting an tuning a linear model that predicts the each diamond’s price with the glmnet package. This will give a good end-to-end glimpse into the data exploration and model fitting process! Before we get into that, let’s load some packages and get a preview of the dataset.\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(vip)\n\ntheme_set(theme_minimal())\n\ndiamonds %>%\n  slice_head(n = 10)\n\n\n# A tibble: 10 × 10\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n\n\nSince we’re predicting price, let’s look at its distribution first.\n\n\nCode\ndiamonds %>%\n  ggplot(aes(x = price)) +\n  geom_histogram()\n\n\n\n\n\nWe’re definitely gonna want to apply a transformation to the price when modeling - let’s look at the distribution on a log-10 scale.\n\n\nCode\ndiamonds %>%\n  ggplot(aes(x = price)) +\n  geom_histogram() +\n  scale_x_log10()\n\n\n\n\n\nThat’s a lot more evenly distributed, if not perfect. That’s a fine starting point, so now we’ll look through the rest of the data.\n\n\nCode\ndiamonds %>%\n  ggplot(aes(x = carat)) +\n  geom_histogram()\n\n\n\n\n\nCode\ndiamonds %>%\n  ggplot(aes(x = cut,\n             y = price)) +\n  geom_boxplot()\n\n\n\n\n\nCode\ndiamonds %>%\n  count(cut) %>%\n  ggplot(aes(x = cut,\n             y = n)) +\n  geom_col()\n\n\n\n\n\nCode\ndiamonds %>%\n  ggplot(aes(x = color,\n             y = price)) +\n  geom_boxplot()\n\n\n\n\n\nCode\ndiamonds %>%\n  count(color) %>%\n  ggplot(aes(x = color,\n             y = n)) +\n  geom_col()\n\n\n\n\n\nCode\ndiamonds %>%\n  ggplot(aes(x = clarity,\n             y = price)) +\n  geom_boxplot()\n\n\n\n\n\nCode\ndiamonds %>%\n  count(clarity) %>%\n  ggplot(aes(x = clarity,\n             y = n)) +\n  geom_col()\n\n\n\n\n\nCode\ndiamonds %>%\n  ggplot(aes(x = depth)) +\n  geom_histogram()\n\n\n\n\n\nCode\ndiamonds %>%\n  ggplot(aes(x = table)) +\n  geom_histogram() \n\n\n\n\n\nCode\ndiamonds %>%\n  ggplot(aes(x = x)) +\n  geom_histogram()\n\n\n\n\n\nCode\ndiamonds %>%\n  ggplot(aes(x = y)) +\n  geom_histogram() \n\n\n\n\n\nCode\ndiamonds %>% \n  ggplot(aes(x = z)) +\n  geom_histogram()\n\n\n\n\n\nIt looks like there may be a good opportunity to try out a few normalization and resampling techniques, but before we get into any of that, let’s build a baseline linear model.\n\n\nCode\n# splits\ndiamonds_split <- initial_split(diamonds)\ndiamonds_train <- training(diamonds_split)\ndiamonds_test <- testing(diamonds_split)\n\n# resamples (don't want to use testing data!)\ndiamonds_folds <- vfold_cv(diamonds_train)\n\n# model spec\nmod01 <-\n  linear_reg() %>%\n  set_engine(\"lm\")\n\n# recipe\nrec01 <-\n  recipe(price ~ ., data = diamonds_train) %>%\n  step_dummy(all_nominal_predictors())\n\n# controls\nctrl_preds <- \n  control_resamples(save_pred = TRUE)\n\n# create a wf\nwf01 <-\n  workflow() %>%\n  add_model(mod01) %>%\n  add_recipe(rec01)\n\n# parallel processing\ndoParallel::registerDoParallel()\n\n# fit\nrs01 <- \n  fit_resamples(\n    wf01,\n    diamonds_folds,\n    control = ctrl_preds\n  )\n\n# metrics!\ncollect_metrics(rs01)\n\n\n# A tibble: 2 × 6\n  .metric .estimator     mean     n std_err .config             \n  <chr>   <chr>         <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   1135.       10 8.10    Preprocessor1_Model1\n2 rsq     standard      0.919    10 0.00129 Preprocessor1_Model1\n\n\nAnd right off the bat, we can see a fairly high value for rsq! However, rsq doesn’t tell the whole story, so we should check our predictions and residuals plots.\n\n\nCode\naugment(rs01) %>%\n  ggplot(aes(x = price,\n             y = .pred)) +\n  geom_point(alpha = 0.01) +\n  geom_abline(linetype = \"dashed\",\n              size = 0.1,\n              alpha = 0.5)\n\n\n\n\n\nThis is definitely not what we want to see! It looks like there’s an odd curve/structure to the graph and we’re actually predicting quite a few negative values. The residuals plot doesn’t look too great either.\n\n\nCode\naugment(rs01) %>%\n  ggplot(aes(x = price,\n             y = .resid)) +\n  geom_point(alpha = 0.01) +\n  geom_hline(yintercept = 0,\n             linetype = \"dashed\",\n             alpha = 0.5,\n             size = 0.1)\n\n\n\n\n\nWhat we’d like to see is a 0-correlation plot with errors normally distributed; what we’re seeing instead, however, is a ton of structure.\nThat being said, that’s okay! we expected this first pass to be pretty rough! And the price is clearly on a log-10 scale. To make apples-apples comparisons with models going forward, I’ll retrain this basic linear model to predict the log10(price). This’ll involve a bit of data re-manipulation!\n\n\nCode\n# log transform price\ndiamonds_model <-\n  diamonds %>%\n  mutate(price = log10(price),\n         across(cut:clarity, as.character))\n\n# bad practice copy + paste lol\n\n# splits\nset.seed(999)\ndiamonds_split <- initial_split(diamonds_model)\ndiamonds_train <- training(diamonds_split)\ndiamonds_test <- testing(diamonds_split)\n\n# resamples (don't want to use testing data!)\nset.seed(888)\ndiamonds_folds <- vfold_cv(diamonds_train)\n\n# model spec\nmod01 <-\n  linear_reg() %>%\n  set_engine(\"lm\")\n\n# recipe\nrec01 <-\n  recipe(price ~ ., data = diamonds_train) %>%\n  step_dummy(all_nominal_predictors())\n\n# controls\nctrl_preds <- \n  control_resamples(save_pred = TRUE)\n\n# create a wf\nwf01 <-\n  workflow() %>%\n  add_model(mod01) %>%\n  add_recipe(rec01)\n\n# parallel processing\ndoParallel::registerDoParallel()\n\n# fit\nset.seed(777)\nrs01 <- \n  fit_resamples(\n    wf01,\n    diamonds_folds,\n    control = ctrl_preds\n  )\n\n# metrics!\ncollect_metrics(rs01)\n\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.0793    10 0.00557 Preprocessor1_Model1\n2 rsq     standard   0.966     10 0.00494 Preprocessor1_Model1\n\n\nAnd wow, that one transformation increased our rsq to 0.96! Again, that’s not the whole story, and we’re going to be evaluating models based on the rmse. Let’s look at how our prediction map has updated:\n\n\nCode\nrs01 %>%\n  augment() %>%\n  ggplot(aes(x = price,\n             y = .pred)) +\n  geom_point(alpha = 0.01) +\n  geom_abline(linetype = \"dashed\",\n              size = 0.1,\n              alpha = 0.5) \n\n\n\n\n\nNow that is a much better starting place to be at! Let’s look at our coefficients\n\n\nCode\nset.seed(666) # :thedevilisalive:\nwf01 %>%\n  fit(diamonds_train) %>%\n  pull_workflow_fit() %>%\n  vip::vi() %>%\n  mutate(Variable = fct_reorder(Variable, Importance)) %>%\n  ggplot(aes(x = Variable,\n             y = Importance,\n             fill = Sign)) +\n  geom_col() +\n  coord_flip() + \n  theme(plot.title.position = \"plot\") +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Diamonds are forever\",\n       subtitle = \"Variable importance plot of a basic linear regression predicting diamond price\")\n\n\n\n\n\nAnother way of looking at it:\n\n\nCode\nset.seed(666)\nwf01 %>%\n  fit(diamonds_train) %>%\n  pull_workflow_fit() %>%\n  vip::vi() %>%\n  mutate(Importance = if_else(Sign == \"NEG\", Importance * -1, Importance),\n         Variable = fct_reorder(Variable, Importance)) %>%\n  ggplot(aes(x = Variable,\n             y = Importance,\n             fill = Sign)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Diamonds are forever\",\n       subtitle = \"Variable importance plot of a basic linear regression predicting diamond price\",\n       x = NULL,\n       y = NULL) +\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\nThis is a good, but definitely improvable, starting point. We can likely decrease our overall error with a bit of feature engineering and drop unimportant features by tuning a regularized model. There are some oddities in this initial model that will need to be improved upon; for one, we can definitively say that the carat feature ought to be positively associated with price\n\n\nCode\ndiamonds_train %>%\n  ggplot(aes(x = carat,\n             y = price)) +\n  geom_point(alpha = 0.01) +\n  labs(title = \"A clear positive (albeit nonlinear) relationship between `carat` and `price`\") +\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\nAnother few things that are interesting to note in this plot! It looks like there are clusterings of carat ratings around round-ish numbers. My hypothesis here is that carat ratings tend to get rounded up to the next size. There’s also a clear abscence of diamonds priced at $1,500 (~3.17 on the log10 scale). I suppose there is some industry-specific reason to avoid a diamond price of $,1500?\n\n\nCode\ndiamonds_train %>%\n  ggplot(aes(x = carat,\n             y = price)) +\n  geom_point(alpha = 0.01) +\n  labs(title = \"A clear positive (albeit nonlinear) relationship between `carat` and `price`\") +\n  theme(plot.title.position = \"plot\") +\n  geom_hline(yintercept = log10(1500),\n             linetype = \"dashed\",\n             size = 0.9,\n             alpha = 0.5)\n\n\n\n\n\nHow to address all these things? With some feature engineering! Firstly, let’s add some recipe steps to balance classes & normalize continuous variables.\nBut before I get into that, I’ll save the resample metrics so that we can compare models!\n\n\nCode\nmetrics <- collect_metrics(rs01) %>% mutate(model = \"model01\")\n\nmetrics\n\n\n# A tibble: 2 × 7\n  .metric .estimator   mean     n std_err .config              model  \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>                <chr>  \n1 rmse    standard   0.0793    10 0.00557 Preprocessor1_Model1 model01\n2 rsq     standard   0.966     10 0.00494 Preprocessor1_Model1 model01\n\n\n\n\nCode\n# spec will be the same as model01\nmod02 <- mod01\n\n# recipe!\nrec02 <- \n  recipe(price ~ ., data = diamonds_train) %>%\n  step_other(cut, color, clarity) %>% \n  step_dummy(all_nominal_predictors(), -cut) %>%\n  \n  # use smote resampling to balance classes\n  themis::step_smote(cut) %>% \n    \n  # normalize continuous vars\n  bestNormalize::step_best_normalize(carat, depth, table, x, y, z)\n\n\nLet’s bake our recipe to verify that everything looks up-to-snuff in the preprocessed dataset.\n\n\nCode\nbaked_rec02 <- \n  rec02 %>%\n  prep() %>%\n  bake(new_data = NULL)\n\nbaked_rec02\n\n\n# A tibble: 80,495 × 20\n     carat cut        depth  table       x       y      z price color_E color_F\n     <dbl> <fct>      <dbl>  <dbl>   <dbl>   <dbl>  <dbl> <dbl>   <dbl>   <dbl>\n 1 -0.706  Premium    0.138 -0.760 -0.709  -0.738  -0.695  3.01       0       1\n 2  0.356  Very Good  0.570  0.835  0.342   0.251   0.344  3.63       0       0\n 3  0.214  Premium   -0.308  0.835  0.293   0.263   0.166  3.58       0       0\n 4 -1.08   other      1.04  -0.310 -1.30   -1.40   -0.995  2.70       0       0\n 5 -0.641  Ideal     -0.602 -0.760 -0.595  -0.560  -0.622  2.97       0       0\n 6 -0.0759 Premium   -0.602  0.494 -0.0349 -0.0460 -0.114  3.38       0       0\n 7 -0.149  Premium   -1.16   0.103 -0.0565 -0.0842 -0.246  3.44       1       0\n 8  0.170  Very Good -0.371  0.494  0.178   0.313   0.130  3.56       0       1\n 9 -0.736  Ideal     -0.110 -0.760 -0.709  -0.738  -0.723  3.09       0       0\n10  0.782  Ideal     -0.602 -0.310  0.819   0.846   0.732  4.02       0       0\n# … with 80,485 more rows, and 10 more variables: color_G <dbl>, color_H <dbl>,\n#   color_I <dbl>, color_J <dbl>, clarity_SI2 <dbl>, clarity_VS1 <dbl>,\n#   clarity_VS2 <dbl>, clarity_VVS1 <dbl>, clarity_VVS2 <dbl>,\n#   clarity_other <dbl>\n\n\n\n\nCode\nbaked_rec02 %>%\n  count(cut) %>%\n  ggplot(aes(x = cut,\n             y = n)) +\n  geom_col()\n\n\n\n\n\nCode\nbaked_rec02 %>%\n  ggplot(aes(x = carat)) +\n  geom_histogram()\n\n\n\n\n\nCode\nbaked_rec02 %>%\n  ggplot(aes(x = depth)) +\n  geom_histogram()\n\n\n\n\n\nCode\nbaked_rec02 %>%\n  ggplot(aes(x = table)) +\n  geom_histogram()\n\n\n\n\n\nCode\nbaked_rec02 %>%\n  ggplot(aes(x = x)) +\n  geom_histogram()\n\n\n\n\n\nCode\nbaked_rec02 %>%\n  ggplot(aes(x = y)) +\n  geom_histogram() \n\n\n\n\n\nCode\nbaked_rec02 %>%\n  ggplot(aes(x = z)) +\n  geom_histogram()\n\n\n\n\n\nEverything looks alright with the exception of the table predictor. I wonder if there are a lot of repeated values in the table variable - that may be why we’re seeing a “chunky” histogram. Let’s check\n\n\nCode\nbaked_rec02 %>%\n  count(table) %>%\n  arrange(desc(n))\n\n\n# A tibble: 10,406 × 2\n    table     n\n    <dbl> <int>\n 1  0.103 12167\n 2 -0.310 11408\n 3 -0.760 11031\n 4  0.494  9406\n 5 -1.28   6726\n 6  0.835  6165\n 7  1.15   3810\n 8 -1.85   2789\n 9  1.42   2182\n10  1.64    972\n# … with 10,396 more rows\n\n\nOoh - okay yeah that’s definitely the issue! I’m not quite sure how to deal with it, so we’re just going to ignore for now! Let’s add a new model & see how it compares against the baseline transformed model.\n\n\nCode\nwf02 <-\n  workflow() %>%\n  add_model(mod02) %>%\n  add_recipe(rec02)\n\n# stop parallel to avoid error!\n# need to replace with PSOCK clusters\n# see github issue here: https://github.com/tidymodels/recipes/issues/847\nforeach::registerDoSEQ()\n\nset.seed(666) # spoopy\nrs02 <-\n  fit_resamples(\n    wf02,\n    diamonds_folds,\n    control = ctrl_preds\n  )\n\ncollect_metrics(rs02)\n\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.115    10 0.00143 Preprocessor1_Model1\n2 rsq     standard   0.932    10 0.00161 Preprocessor1_Model1\n\n\nOof - that’s actually slightly worse than our baseline model!\n\n\nCode\nrs02 %>%\n  augment() %>%\n  ggplot(aes(x = price,\n             y = .pred)) +\n  geom_point(alpha = 0.01) +\n  geom_abline(linetype = \"dashed\",\n              size = 0.1,\n              alpha = 0.5) \n\n\n\n\n\nIt looks like we’ve introduced structure into the residual plot!\n\n\nCode\nrs02 %>%\n  augment() %>%\n  ggplot(aes(x = price,\n             y = .resid)) +\n  geom_point(alpha = 0.01) +\n  geom_hline(yintercept = 0,\n             linetype = \"dashed\",\n             size = 0.1,\n             alpha = 0.5)\n\n\n\n\n\nYeah that’s fairly wonky! I’m wondering if it’s due to the SMOTE upsampling method we introduced? To counteract, I’ll build & train new models after each set of recipe steps (e.g., resampling, normalizing, interactions) to buil up a better performing model one step at a time.\n\n\nCode\nmetrics <- \n  metrics %>%\n  bind_rows(collect_metrics(rs02) %>% mutate(model = \"model02\"))\n\n\n\n\nCode\n# same model spec\nmod03 <- mod02\n\n# rebuild rec+wf & retrain\nrec03 <- \n  recipe(price ~ ., data = diamonds_train) %>%\n  step_other(cut, color, clarity) %>%\n  step_dummy(all_nominal_predictors(), -cut) %>%\n  themis::step_smote(cut)\n\nwf03 <- \n  workflow() %>%\n  add_model(mod03) %>%\n  add_recipe(rec03)\n\n# do paralllel\ndoParallel::registerDoParallel()\n\n# refit!\nset.seed(123)\nrs03 <-\n  fit_resamples(\n    wf03,\n    diamonds_folds,\n    control = ctrl_preds\n  )\n\ncollect_metrics(rs03)\n\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.0918    10 0.00502 Preprocessor1_Model1\n2 rsq     standard   0.956     10 0.00502 Preprocessor1_Model1\n\n\nInteresting! Improved relative to rs02, but still not as good as our first model! Let’s try using step_downsample() to balance classes & see how we fare.\n\n\nCode\n# cleanup some large-ish items eating up memory\nrm(mod01, mod02, rec01, rec02, wf01, wf02, rs01, rs02)\n\n# save metrics\nmetrics <- \n  metrics %>%\n  bind_rows(collect_metrics(rs03) %>% mutate(model = \"model03\"))\n\n# new mod\nmod04 <- mod03\n\n# new rec\nrec04 <-\n  recipe(price ~ ., data = diamonds_train) %>%\n  step_other(cut, color, clarity) %>%\n  step_dummy(all_nominal_predictors(), -cut) %>%\n  themis::step_downsample(cut)\n\nwf04 <-\n  workflow() %>%\n  add_model(mod04) %>%\n  add_recipe(rec04) \n\nset.seed(456) \nrs04 <-\n  fit_resamples(\n    wf04,\n    diamonds_folds,\n    control = ctrl_preds\n  )\n\ncollect_metrics(rs04)\n\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.113    10  0.0120 Preprocessor1_Model1\n2 rsq     standard   0.930    10  0.0153 Preprocessor1_Model1\n\n\nWow - still a bit worse! I’ll try upsampling & if there is no improvement, we’ll move on without resampling!\n\n\nCode\nmetrics <-\n  metrics %>%\n  bind_rows(collect_metrics(rs04) %>% mutate(model = \"model04\"))\n\nmod05 <- mod04\n\nrec05 <-\n  recipe(price ~ ., data = diamonds_train) %>%\n  step_other(cut, color, clarity) %>%\n  step_dummy(all_nominal_predictors(), -cut) %>%\n  themis::step_upsample(cut)\n\nwf05 <- \n  workflow() %>%\n  add_model(mod05) %>%\n  add_recipe(rec05) \n\nset.seed(789)\nrs05 <-\n  fit_resamples(\n    wf05,\n    diamonds_folds,\n    control = ctrl_preds\n  )\n\ncollect_metrics(rs05)\n\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.101    10 0.00486 Preprocessor1_Model1\n2 rsq     standard   0.946    10 0.00527 Preprocessor1_Model1\n\n\nOkay - resampling gets stricken off our list of recipe steps! Let’s look at how the models compare so far\n\n\nCode\nmetrics <-\n  metrics %>%\n  bind_rows(collect_metrics(rs05) %>% mutate(model = \"model05\"))\n\nmetrics %>%\n  ggplot(aes(x = model)) +\n  geom_point(aes(y = mean)) +\n  geom_errorbar(aes(ymin = mean - std_err,\n                    ymax = mean + std_err)) +\n  facet_wrap(~.metric, scales = \"free_y\")\n\n\n\n\n\nThe first simple linear model was the best as measured by both metrics! Let’s see if we can improve with some normalization of the continuous vars.\n\n\nCode\nrm(mod03, mod04, rec03, rec04, rs03, rs04, wf03, wf04)\n\nmod06 <- mod05\n\nrec06 <-\n  recipe(price ~ ., data = diamonds_train) %>%\n  step_other(cut, color, clarity) %>%\n  bestNormalize::step_best_normalize(all_numeric_predictors()) %>%\n  step_dummy(all_nominal_predictors())\n\nwf06 <-\n  workflow() %>%\n  add_model(mod06) %>%\n  add_recipe(rec06)\n\nforeach::registerDoSEQ()\nset.seed(101112)\nrs06 <-\n  fit_resamples(\n    wf06,\n    diamonds_folds,\n    control = ctrl_preds\n  )\n\ncollect_metrics(rs06)\n\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.127    10 0.00115 Preprocessor1_Model1\n2 rsq     standard   0.916    10 0.00136 Preprocessor1_Model1\n\n\nWell - that was quite a bit for no improvement! I guess that normalizing the continuous vars in this case isn’t helping. Moving on to adding some interactions - first let’s explore potential interactions a bit.\n\n\nCode\nmetrics <-\n  metrics %>% \n  bind_rows(collect_metrics(rs06) %>% mutate(model = \"model06\"))\n\ndiamonds_train %>%\n  ggplot(aes(x = carat,\n             y = price,\n             color = cut)) +\n  geom_point(alpha = 0.05) + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\nCode\nlibrary(splines)\ndiamonds_train %>%\n  ggplot(aes(x = carat,\n             y = price,\n             color = cut)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(method = lm,\n              formula = y ~ ns(x, df = 5),\n              se = FALSE) +\n  facet_wrap(~cut, scales = \"free\")\n\n\n\n\n\n5 spline terms might not be sufficient here - capturing the lower bound well but really not doing well with the higher carat diamonds.\n\n\nCode\ndiamonds_train %>%\n  ggplot(aes(x = carat,\n             y = price,\n             color = cut)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(method = lm,\n              formula = y ~ ns(x, df = 10),\n              se = FALSE) +\n  facet_wrap(~cut, scales = \"free\")\n\n\n\n\n\nHmmmm, 10 might be too many. It looks lie we’ll just lose a bit of confidence for the Premium & Very Good diamonds at higher carats. Relative to the total number, I’m not too concerned.\n\n\nCode\ndiamonds_train %>%\n  ggplot(aes(x = carat,\n             y = price,\n             color = cut)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(method = lm,\n              formula = y ~ ns(x, df = 7),\n              se = FALSE) +\n  facet_wrap(~cut, scales = \"free\")\n\n\n\n\n\n7 terms feels like the best we’re going to do here - I think this is tuneable, but we’ll leave as is (now & in the final model).\nNext, we’ll look at creating interactions between the color and carat variables:\n\n\nCode\ndiamonds_train %>%\n  ggplot(aes(x = carat, \n             y = price,\n             color = color)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(method = lm, \n              formula = y ~ ns(x, df = 15),\n              se = FALSE) +\n  facet_wrap(~color)\n\n\n\n\n\nAdding interactive spline terms with df of 15 seems to add some useful information!\nWe have three shape parameters, x, y, and z - I wonder if creating a stand-in for volume by multiplying them all together will provide any useful information?\n\n\nCode\ndiamonds_train %>%\n  mutate(volume_param = x * y * z) %>%\n  ggplot(aes(x = volume_param,\n             y = price)) +\n  geom_point(alpha = 0.05)\n\n\n\n\n\nOoh, looks like we’re getting some good info here, but we may want to use log10 to scale this back.\n\n\nCode\ndiamonds_train %>%\n  mutate(volume_param = log10(x * y * z)) %>%\n  ggplot(aes(x = volume_param, \n             y = price)) +\n  geom_point(alpha = 0.05)\n\n\n\n\n\nLet’s see if this ought to interact with any other paramaters:\n\n\nCode\ndiamonds_train %>%\n  mutate(volume_param = log10(x * y * z)) %>%\n  ggplot(aes(x = volume_param, \n             y = price,\n             color = cut)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\nCode\ndiamonds_train %>%\n  mutate(volume_param = log10(x * y * z)) %>%\n  ggplot(aes(x = volume_param, \n             y = price,\n             color = color)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\nCode\ndiamonds_train %>%\n  mutate(volume_param = log10(x * y * z)) %>%\n  ggplot(aes(x = volume_param, \n             y = price,\n             color = clarity)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\nHmm, it doesn’t really look like we’re capturing too great of interactions, so I’ll leave out for now. It looks like the size of the rock is more important than anything else! I could continue to dig further, but I’ll stop there. I’m likely getting diminishing returns, & I’d like to get back into modeling!\n\n\nCode\nmod07 <- mod06\n\nrec07 <-\n  recipe(price ~ ., data = diamonds_train) %>%\n  step_other(cut, color, clarity) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_interact(~carat:starts_with(\"cut_\")) %>%\n  step_interact(~carat:starts_with(\"color_\")) %>%\n  step_mutate_at(c(x, y, z),\n                 fn = ~if_else(.x == 0, mean(.x), .x)) %>%\n  step_mutate(volume_param = log10(x * y * z)) %>%\n  step_ns(starts_with(\"carat_x_cut\"), deg_free = 7) %>%\n  step_ns(starts_with(\"carat_x_color\"), deg_free = 15) \n\nrec07\n\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          9\n\nOperations:\n\nCollapsing factor levels for cut, color, clarity\nDummy variables from all_nominal_predictors()\nInteractions with carat:starts_with(\"cut_\")\nInteractions with carat:starts_with(\"color_\")\nVariable mutation for c(x, y, z)\nVariable mutation for log10(x * y * z)\nNatural splines on starts_with(\"carat_x_cut\")\nNatural splines on starts_with(\"carat_x_color\")\n\n\nCode\nwf07 <-\n  workflow() %>%\n  add_model(mod07) %>%\n  add_recipe(rec07)\n\ndoParallel::registerDoParallel()\nset.seed(9876)\nrs07 <-\n  fit_resamples(\n    wf07,\n    diamonds_folds,\n    control = ctrl_preds\n  )\n\n\nThis is definitely going to way overfit our data:\n\n\nCode\nrs07 %>%\n  collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.0750    10 0.00377 Preprocessor1_Model1\n2 rsq     standard   0.970     10 0.00342 Preprocessor1_Model1\n\n\nWell we (finally) made a modes improvement! Let’s see how the predictions/residuals plot:\n\n\nCode\nrs07 %>%\n  augment() %>%\n  ggplot(aes(x = price,\n             y = .pred)) +\n  geom_point(alpha = 0.05) +\n  geom_abline(linetype = \"dashed\",\n              alpha = 0.5,\n              size = 0.5)\n\n\n\n\n\nThat’s pretty good! We do have one value that’s way off, so let’s see if regulization can help. This will require setting a new baseline model, and we’ll tune our way to the best regularizaion parameters.\n\n\nCode\nmetrics <- \n  rs07 %>%\n  collect_metrics() %>%\n  mutate(model = \"model07\") %>%\n  bind_rows(metrics)\n\n# add normalization step\nrec08 <- \n  rec07 %>% \n  step_zv(all_numeric_predictors()) %>%\n  step_normalize(all_numeric_predictors(),\n                 -cut_Ideal, -cut_Premium, -cut_Very.Good, -cut_other,\n                 -color_E, -color_F, -color_G, -color_H, -color_I, -color_J,\n                 -clarity_SI2, -clarity_VS1, -clarity_VS2, -clarity_VVS1, -clarity_VVS2, -clarity_other)\n\nrm(mod05, mod06, mod07, rec05, rec06, rec07, wf05, wf06, wf07, rs05, rs06, rs07)\n\nmod08 <-\n  linear_reg(penalty = tune(), mixture = tune()) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"regression\") \n\nwf08 <-\n  workflow() %>%\n  add_model(mod08) %>%\n  add_recipe(rec08)\n\ndiamonds_grid <- \n  grid_regular(penalty(), mixture(), levels = 20)\n\ndoParallel::registerDoParallel()\nset.seed(5831)\nrs08 <-\n  tune_grid(\n    wf08,\n    resamples = diamonds_folds,\n    control = ctrl_preds,\n    grid = diamonds_grid\n  )\n\n\nSome notes but let’s explore our results…\n\n\nCode\nrs08 %>%\n  collect_metrics() %>%\n  ggplot(aes(x = penalty,\n             y = mean,\n             color = as.character(mixture))) +\n  geom_point() +\n  geom_line(alpha = 0.75) +\n  facet_wrap(~.metric, scales = \"free\") +\n  scale_x_log10()\n\n\n\n\n\nLooks like we were performing pretty well with the unregularized model, oddly enough! Let’s select the best and finalize our workflow.\n\n\nCode\nbest_metrics <- \n  rs08 %>%\n  select_best(\"rmse\")\n\nwf_final <- \n  finalize_workflow(wf08, best_metrics)\n\nrm(mod08, rec07, rec08, rs08, wf08)\n\nset.seed(333)\nfinal_fit <- \n  wf_final %>%\n  fit(diamonds_train)\n\nfinal_fit %>%\n  predict(diamonds_test) %>%\n  bind_cols(diamonds_test) %>%\n  select(price, .pred) %>%\n  ggplot(aes(x = price, \n             y = .pred)) +\n  geom_point(alpha = 0.05) + \n  geom_abline(alpha = 0.5,\n              linetype = \"dashed\",\n              size = 0.5)\n\n\n\n\n\nWhat are the most important variables in this regularized model?\n\n\nCode\nfinal_fit %>%\n  pull_workflow_fit() %>%\n  vi(lambda = best_metrics$penalty) %>%\n  mutate(Variable = fct_reorder(Variable, Importance)) %>%\n  ggplot(aes(x = Variable,\n             y = Importance, \n             fill = Sign)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\n\nAs expected, most of our terms get regularized away, which is what we want! Our chart is a little unreadable; let’s plot just the most important variables in a few ways:\n\n\nCode\nfinal_fit %>%\n  pull_workflow_fit() %>%\n  vi(lambda = best_metrics$penalty) %>%\n  arrange(desc(Importance)) %>%\n  slice_head(n = 10) %>%\n  mutate(Variable = fct_reorder(Variable, Importance)) %>%\n  ggplot(aes(x = Variable,\n             y = Importance,\n             fill = Sign)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\n\n\n\nCode\nfinal_fit %>%\n  pull_workflow_fit() %>%\n  vi(lambda = best_metrics$penalty) %>%\n  arrange(desc(Importance)) %>% \n  slice_head(n = 10) %>%\n  mutate(Importance = if_else(Sign == \"NEG\", -Importance, Importance),\n         Variable = fct_reorder(Variable, Importance)) %>%\n  ggplot(aes(x = Variable,\n             y = Importance,\n             fill = Sign)) + \n  geom_col() +\n  coord_flip()\n\n\n\n\n\nAnd look at that! Our most important variable was one that came from feature engineering! The size of the rock had the biggest impact on price.\nWe’ve gone through a lot of steps, so it may be good to look back on what was done:\n\nExplored our dataset via some simple exploratory data analysis;\nFit a simple linear model to predict the log-transform of price;\nAttempted (and failed) to improve upon the simple model with fancier normalization and resampling techniques;\nExplored the dataset further to find meaningful interactions and potential new features;\nFit a new model with feature engineering;\nTuned regularization parameters on our model with feature engineering to arrive at the final model.\n\nOur models’ performances, ranked from best to worst, show that the final tuned model did indeed perform the best on the test dataset!\n\n\nCode\nfinal_preds <-\n  final_fit %>%\n  predict(diamonds_train) %>%\n  bind_cols(diamonds_train) %>%\n  select(price, .pred)\n\nbind_rows(final_preds %>% rmse(price, .pred),\n          final_preds %>% rsq(price, .pred)) %>%\n  rename(mean = .estimate) %>%\n  select(-.estimator) %>%\n  mutate(model = \"model_final\") %>%\n  bind_rows(metrics %>% select(.metric, mean, model)) %>%\n  pivot_wider(names_from = .metric,\n              values_from = mean) %>%\n  mutate(model = fct_reorder(model, desc(rmse))) %>%\n  pivot_longer(rmse:rsq,\n               names_to = \"metric\",\n               values_to = \"value\") %>%\n  ggplot(aes(x = model,\n             y = value)) +\n  geom_point() +\n  facet_wrap(~metric, scales = \"free\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{rieke2021,\n  author = {Mark Rieke},\n  title = {Diamonds Are {Forever}},\n  date = {2021-11-14},\n  url = {https://www.thedatadiary.net/posts/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMark Rieke. 2021. “Diamonds Are Forever.” November 14,\n2021. https://www.thedatadiary.net/posts/2021-11-14-diamonds-are-forever-feature-engineering-with-the-diamonds-dataset."
  }
]